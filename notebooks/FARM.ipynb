{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWSR3a5vxSGL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/ColabNotebooks\")\n",
    "# For Colab: Install FARM\n",
    "!pip install torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install farm==0.5.0\n",
    "!pip install -U -q emoji soynlp\n",
    "!git clone https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iM4H-pbDL_XC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "def read_data(path:str, header=None):\n",
    "    return pd.read_csv(path, sep='\\t', header=header)\n",
    "\n",
    "def clean(x):\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "    url_pattern = re.compile(\n",
    "        r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    \n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x\n",
    "\n",
    "def preprocess_dataframe(df:pd.DataFrame):\n",
    "    r\"\"\"\n",
    "    Changed the code\n",
    "    source from: https://colab.research.google.com/drive/1IPkZo1Wd-DghIOK6gJpcb0Dv4_Gv2kXB\n",
    "    \"\"\"\n",
    "\n",
    "    label_dict = {0:\"bad\", 1:\"good\"}\n",
    "    df['document'] = df['document'].apply(lambda x: clean(str(x)))\n",
    "    df['label'] = df['label'].apply(label_dict.get)\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_dataframe(read_data(\"./nsmc/ratings_train.txt\", header=0))\n",
    "df_test = preprocess_dataframe(read_data(\"./nsmc/ratings_test.txt\", header=0))\n",
    "df_train.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/train.tsv\", sep=\"\\t\", index=False)\n",
    "df_test.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/test.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-PcIhBaB3-E",
    "outputId": "158cb1c2-8b2e-42b3-9c83-257c607fa318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\t\t  ratings_train.txt  raw\tsynopses.json  train.tsv\n",
      "ratings_test.txt  ratings.txt\t     README.md\ttest.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeTtCfRkyxVJ",
    "outputId": "8fb8c67c-7c93-45ec-85f9-e7e60ace7a90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 09:45:11 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __          __  _                            _        \n",
      " \\ \\        / / | |                          | |       \n",
      "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
      "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
      "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
      "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
      "  ______      _____  __  __  \n",
      " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
      " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
      " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
      " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
      " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
      "                                     |    |==|==|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      " \n",
      "Devices available: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import TextClassificationHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.train import Trainer\n",
    "from farm.utils import MLFlowLogger\n",
    "\n",
    "repo_path = Path() # Path().absolute().parent\n",
    "sys.path.append(str(repo_path))\n",
    "\n",
    "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
    "ml_logger.init_experiment(experiment_name=\"FARM_tutorial\", run_name=\"NSMC\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Devices available: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS3WaV8YMJmh"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1hbtUClFoXg45IbViZoFRLnnDGVlr9Dlb\" alt=\"Fine-tuning\" width=\"30%\" height=\"30%\"></center>\n",
    "\n",
    "# FARM\n",
    "\n",
    "> Framework for Adapting Representation Models\n",
    "\n",
    "이 패키지를 한 마디로 요약하면 Fine-tuning에 최적화된 도구다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PkdHkjZoR4A"
   },
   "source": [
    "## Core Features\n",
    "\n",
    "- **Easy fine-tuning of language models** to your task and domain language\n",
    "- **Speed**: AMP(Automatic Mixed Precision) optimizers (~35% faster) and parallel preprocessing (16 CPU cores => ~16x faster)\n",
    "- **Modular design** of language models and prediction heads\n",
    "- Switch between heads or combine them for **multitask learning**\n",
    "- **Full Compatibility** with HuggingFace Transformers' models and model hub\n",
    "- **Smooth upgrading** to newer language models\n",
    "- Integration of **custom datasets** via Processor class\n",
    "- Powerful **experiment tracking** & execution\n",
    "- **Checkpointing & Caching** to resume training and reduce costs with spot instances\n",
    "- Simple **deployment** and **visualization** to showcase your model\n",
    "\n",
    "<details>\n",
    "<summary> AMP </summary>\n",
    "\n",
    "**Reference**\n",
    "- https://github.com/NVIDIA/apex\n",
    "- https://forums.fast.ai/t/mixed-precision-training/20720\n",
    "\n",
    "**mixed precision training이란**\n",
    "- 처리 속도를 높이기 위한 FP16(16bit floating point)연산과 정확도 유지를 위한 FP32 연산을 섞어 학습하는 방법\n",
    "- Tensor Core를 활용한 FP16연산을 이용하면 FP32연산 대비 절반의 메모리 사용량과 8배의 연산 처리량 & 2배의 메모리 처리량 효과가 있다\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjzUlWjDobxD"
   },
   "source": [
    "# NSMC 데이터 세트로 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP4UcaeKoVM8"
   },
   "source": [
    "## Fine-tuning Process\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1j9pn8Lpg7sy6S8Ubvq3E7JLWf28KvRt4\" alt=\"Fine-tuning\" width=\"50%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Fine-tuning Processing 그림과 같이 진행된다.\n",
    "\n",
    "* Load Data: 데이터를 알맞는 형식(json, csv 등)으로 불러온다.\n",
    "* Create Dataset: 데이터세트(Dataset) 만들기\n",
    "    * Tokenization: 텍스트를 토큰으로 나누고, 단어장(vocab)을 생성한다.\n",
    "    * ToTensor: vocab에 해당하는 단어를 수치화하는 과정 (`input_ids` in transformers)\n",
    "    * Attention Mask: 패딩계산을 피하기 위해 Attention 해야할 토큰만 masking(`attention_mask` in transformers)\n",
    "* Create Dataloader: 훈련, 평가시 배치크기 단위로 데이터를 불러오는 객체\n",
    "* Create Model:\n",
    "    * Pretrained Language Model: 대량의 텍스트 데이터로 사전에 훈련된 모델 \n",
    "$$P(x_t \\vert x_{1:t-1})$$\n",
    "    * Fine-tuninig Layer: Downstream Task에 맞춰서 학습        \n",
    "$$P(y\\vert x_{1:t})$$\n",
    "* Train Model\n",
    "* Eval Model\n",
    "* Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "7hSnf1TDMJEB",
    "outputId": "733e7c1f-b6ca-4e0e-ae82-d6097394d8f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bad</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bad</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           document\n",
       "0   bad                                아 더빙.. 진짜 짜증나네요 목소리\n",
       "1  good                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
       "2   bad                                  너무재밓었다그래서보는것을추천한다\n",
       "3   bad                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
       "4  good  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ..."
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from src import read_data\n",
    "\n",
    "DATA_PATH = repo_path / \"nsmc\"\n",
    "df = read_data(DATA_PATH / \"train.tsv\", header=0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsQKHSPlMovO"
   },
   "source": [
    "## Processor & Data Silo\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1XCc0AJpPBMFcC81NW0A6w0mpswZ2KU7h\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **Processor**는 file 혹은 request를 PyTorch Datset로 만들어 주는 역할\n",
    "* **Data Silo**는 train, dev, test sets를 관리하고, Processor의 function들 이용해 각 set를 DataLoader로 변환한다.\n",
    "    * **Samples**, **SampleBasket**은 raw document를 관리하는 객체이며 tokenized, features등 데이터를 저장하고 있다. 이렇게 하는 이유는 하나의 소스 텍스트(raw text)에서 여러개의 샘플을 생성할 수도 있기 때문이다(e.g. QA task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuQEtIvNTI5V",
    "outputId": "57da6862-931f-4490-b86a-e7700da2b698"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 09:45:12 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/29/2021 09:45:14 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "03/29/2021 09:45:14 - INFO - farm.data_handler.data_silo -   Loading train set from: nsmc/train.tsv \n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 149539 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   /|\\\n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   /'\\\n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/train.tsv:   0%|          | 0/149539 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/29/2021 09:45:17 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/29/2021 09:45:17 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1213-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: 그림체는 좋다. 그렇다 그뿐이다 내용 전개도 억지스럽고 설명 부족에 뿌린 떡밥은 많으나 치워지지 않는 떡밥 사실 이 평점 1점도 이 글을 쓰기위함이지 사실 점수가 된다면 -무한대를 주고 싶을만큼 졸작이자 쓰래기이다. 개인적인 의견이지만..\n",
      "Tokenized: \n",
      " \ttokens: ['그림', '##체', '##는', '좋다', '.', '그렇다', '그', '##뿐이다', '내용', '전', '##개도', '억지', '##스럽고', '설명', '부족', '##에', '뿌린', '떡', '##밥', '##은', '많으', '##나', '치워', '##지지', '않는', '떡', '##밥', '사실', '이', '평', '##점', '1', '##점도', '이', '글을', '쓰기', '##위', '##함이', '##지', '사실', '점수', '##가', '된다면', '-', '무한', '##대를', '주고', '싶을', '##만큼', '졸', '##작', '##이자', '쓰래기', '##이다', '.', '개인적인', '의견이', '##지만', '.', '.']\n",
      " \toffsets: [0, 2, 3, 5, 7, 9, 13, 14, 18, 21, 22, 25, 27, 31, 34, 36, 38, 41, 42, 43, 45, 47, 49, 51, 54, 57, 58, 60, 63, 65, 66, 68, 69, 72, 74, 77, 79, 80, 82, 84, 87, 89, 91, 95, 96, 98, 101, 104, 106, 109, 110, 111, 114, 117, 119, 121, 126, 129, 131, 132]\n",
      " \tstart_of_word: [True, False, False, True, False, True, True, False, True, True, False, True, False, True, True, False, True, True, False, False, True, False, True, False, True, True, False, True, True, True, False, True, False, True, True, True, False, False, False, True, True, False, True, True, False, False, True, True, False, True, False, False, True, False, False, True, True, False, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 13447, 4230, 4008, 10350, 17, 12120, 391, 12678, 9132, 2525, 25289, 12676, 13937, 10415, 9633, 4113, 19542, 1023, 4578, 4057, 20847, 4136, 19129, 8840, 8743, 1023, 4578, 8220, 2451, 3288, 4213, 20, 24896, 2451, 12342, 25912, 4069, 11467, 4102, 8220, 25899, 4009, 17524, 16, 14244, 10633, 9568, 22519, 8801, 2579, 4240, 10575, 15893, 7976, 17, 20485, 21100, 8015, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/29/2021 09:45:17 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 214-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: good\n",
      " \ttext: 강부자 여동생 대사와 연기에 넋을 잃고 봤네\n",
      "Tokenized: \n",
      " \ttokens: ['강', '##부자', '여', '##동생', '대사', '##와', '연기', '##에', '넋', '##을', '잃고', '봤', '##네']\n",
      " \toffsets: [0, 1, 4, 5, 8, 10, 12, 14, 16, 17, 19, 22, 23]\n",
      " \tstart_of_word: [True, False, True, False, True, False, True, False, True, False, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 213, 27729, 2269, 16247, 23805, 4196, 11219, 4113, 636, 4027, 18629, 1594, 4011, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/train.tsv:   5%|▌         | 8000/149539 [00:07<02:20, 1009.24 Dicts/s]03/29/2021 09:45:23 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  11%|█         | 16000/149539 [00:15<02:07, 1048.69 Dicts/s]03/29/2021 09:45:32 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:45:32 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  17%|█▋        | 26000/149539 [00:25<01:59, 1035.47 Dicts/s]03/29/2021 09:45:41 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:45:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:45:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  19%|█▊        | 28000/149539 [00:27<01:58, 1026.38 Dicts/s]03/29/2021 09:45:44 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  21%|██▏       | 32000/149539 [00:31<01:54, 1030.38 Dicts/s]03/29/2021 09:45:47 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  28%|██▊       | 42000/149539 [00:40<01:45, 1019.58 Dicts/s]03/29/2021 09:45:57 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  32%|███▏      | 48000/149539 [00:46<01:36, 1051.39 Dicts/s]03/29/2021 09:46:04 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  35%|███▍      | 52000/149539 [00:50<01:33, 1046.30 Dicts/s]03/29/2021 09:46:06 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  45%|████▌     | 68000/149539 [01:05<01:19, 1029.91 Dicts/s]03/29/2021 09:46:21 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  47%|████▋     | 70000/149539 [01:07<01:16, 1035.14 Dicts/s]03/29/2021 09:46:24 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  49%|████▉     | 74000/149539 [01:11<01:13, 1031.48 Dicts/s]03/29/2021 09:46:27 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:46:28 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:46:28 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  51%|█████     | 76000/149539 [01:13<01:10, 1045.74 Dicts/s]03/29/2021 09:46:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  55%|█████▍    | 82000/149539 [01:19<01:05, 1035.73 Dicts/s]03/29/2021 09:46:35 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  68%|██████▊   | 102000/149539 [01:38<00:44, 1058.87 Dicts/s]03/29/2021 09:46:54 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:46:54 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  70%|██████▉   | 104000/149539 [01:40<00:43, 1051.16 Dicts/s]03/29/2021 09:46:56 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  75%|███████▍  | 112000/149539 [01:47<00:35, 1053.11 Dicts/s]03/29/2021 09:47:04 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:47:05 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  86%|████████▌ | 128000/149539 [02:03<00:20, 1034.29 Dicts/s]03/29/2021 09:47:20 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  91%|█████████ | 136000/149539 [02:11<00:13, 1036.30 Dicts/s]03/29/2021 09:47:27 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  92%|█████████▏| 138000/149539 [02:13<00:11, 1013.23 Dicts/s]03/29/2021 09:47:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  96%|█████████▋| 144000/149539 [02:18<00:05, 1027.34 Dicts/s]03/29/2021 09:47:36 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv: 100%|██████████| 149539/149539 [02:23<00:00, 1039.25 Dicts/s]\n",
      "03/29/2021 09:47:39 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
      "03/29/2021 09:47:39 - INFO - farm.data_handler.data_silo -   Took 15536 samples out of train set to create dev set (dev split is roughly 0.1)\n",
      "03/29/2021 09:47:39 - INFO - farm.data_handler.data_silo -   Loading test set from: nsmc/test.tsv\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 50000 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   /w\\\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   / \\\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/test.tsv:   0%|          | 0/50000 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/29/2021 09:47:42 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/29/2021 09:47:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 71-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: 미친놈들 집합소네 연출력 빵점, 스토리 빵점. 구성빵점\n",
      "Tokenized: \n",
      " \ttokens: ['미친', '##놈들', '집합소', '##네', '연출', '##력', '빵', '##점', ',', '스토', '##리', '빵', '##점', '.', '구성', '##빵', '##점']\n",
      " \toffsets: [0, 2, 5, 8, 10, 12, 14, 15, 16, 18, 20, 22, 23, 24, 26, 28, 29]\n",
      " \tstart_of_word: [True, False, True, False, True, False, True, False, False, True, False, True, False, False, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 8257, 8423, 20933, 4011, 23889, 4286, 1692, 4213, 15, 17319, 4038, 1692, 4213, 17, 17474, 4555, 4213, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/29/2021 09:47:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 767-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: 그냥 답답하다. 영화보는 내내 우울한 분위기 ...\n",
      "Tokenized: \n",
      " \ttokens: ['그냥', '답답하다', '.', '영화', '##보는', '내내', '우울', '##한', '분위기', '.', '.', '.']\n",
      " \toffsets: [0, 3, 7, 9, 11, 14, 17, 19, 21, 25, 26, 27]\n",
      " \tstart_of_word: [True, True, False, True, False, True, True, False, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 8019, 13234, 17, 9376, 9136, 16714, 15343, 4047, 12655, 17, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/test.tsv:   8%|▊         | 4000/50000 [00:04<00:47, 969.59 Dicts/s]03/29/2021 09:47:45 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  12%|█▏        | 6000/50000 [00:05<00:44, 994.10 Dicts/s]03/29/2021 09:47:46 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:47:47 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:47:48 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  24%|██▍       | 12000/50000 [00:11<00:36, 1044.20 Dicts/s]03/29/2021 09:47:52 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  48%|████▊     | 24000/50000 [00:23<00:25, 1029.36 Dicts/s]03/29/2021 09:48:03 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:48:04 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  52%|█████▏    | 26000/50000 [00:25<00:23, 1026.97 Dicts/s]03/29/2021 09:48:06 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  64%|██████▍   | 32000/50000 [00:30<00:17, 1030.19 Dicts/s]03/29/2021 09:48:12 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  80%|████████  | 40000/50000 [00:38<00:09, 1049.44 Dicts/s]03/29/2021 09:48:19 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  92%|█████████▏| 46000/50000 [00:44<00:03, 1020.58 Dicts/s]03/29/2021 09:48:25 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv: 100%|██████████| 50000/50000 [00:48<00:00, 1031.86 Dicts/s]\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Cached the datasets at cache/data_silo/ac8b9ede46e0a9423d4adc9c41a611b6\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Examples in train: 133976\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Examples in dev  : 15536\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Examples in test : 49989\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   \n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     150\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 19.57721532214725\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Proportion clipped:      4.478414044306443e-05\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME_OR_PATH = \"beomi/kcbert-base\"  # Reference: https://github.com/Beomi/KcBERT\n",
    "MAX_LENGTH = 150\n",
    "LABEL_LIST = [\"bad\", \"good\"]\n",
    "TRAIN_FILE = \"train.tsv\"\n",
    "TEST_FILE = \"test.tsv\"\n",
    "TASK_TYPE = \"text_classification\"\n",
    "\n",
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "\n",
    "processor = TextClassificationProcessor(\n",
    "    tokenizer=tokenizer,\n",
    "    train_filename=TRAIN_FILE,\n",
    "    test_filename=TEST_FILE,\n",
    "    dev_split=0.1,\n",
    "    header=0,\n",
    "    max_seq_len=MAX_LENGTH,\n",
    "    data_dir=str(DATA_PATH),\n",
    "    label_list=LABEL_LIST,\n",
    "    metric=\"acc\",\n",
    "    label_column_name=\"label\",\n",
    "    text_column_name=\"document\",\n",
    "    delimiter=\"\\t\"\n",
    ")\n",
    "\n",
    "data_silo = DataSilo(\n",
    "    processor=processor,\n",
    "    batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBqy3S68OIX9"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1DVPT_Rjv_SI4ggJZzqfPh0MgsMa1Q9El\" alt=\"Fine-tuning\" width=\"100%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "```plaintext\n",
    "03/28/2021 22:12:15 - INFO - farm.data_handler.processor -   \n",
    "\n",
    "      .--.        _____                       _      \n",
    "    .'_\\/_'.     / ____|                     | |     \n",
    "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
    "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
    "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
    "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
    "   (/\\||/                             |_|           \n",
    "______\\||/___________________________________________                     \n",
    "\n",
    "ID: 437-0\n",
    "Clear Text: \n",
    " \ttext_classification_label: good\n",
    " \ttext: 이 영화를 보고 두통이 나았습니다. ㅠ ㅠ\n",
    "Tokenized: \n",
    " \ttokens: ['이', '영화를', '보고', '두', '##통이', '나', '##았습니다', '.', '[UNK]', '[UNK]']\n",
    " \toffsets: [0, 2, 6, 9, 10, 13, 14, 18, 20, 22]\n",
    " \tstart_of_word: [True, True, True, True, False, True, False, False, True, True]\n",
    "Features: \n",
    " \tinput_ids: [2, 2451, 25833, 8198, 917, 11765, 587, 21809, 17, 1,\n",
    "      1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \ttext_classification_label_ids: [1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ_tuLHRTgGb"
   },
   "source": [
    "## Modeling Layers: AdaptiveModel = LanguageModel + PredictionHead\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1OLWdr8rh7ucpF9t55gzVeMawMBJbRiEC\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **LanguageModel**은 pretrained language models(BERT, XLNet ...)의 표준 클래스 \n",
    "* **PredictionHead**는 모든 down-stream tasks(NER, Text classification, QA ...)를 표준 클래스\n",
    "* **AdaptiveModel**은 위 두 가지 모들의 결합, 하나의 LanguageModel과 여러 개의 PredictionHead를 결합할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60M4IEtWTitY",
    "outputId": "f56fa874-aff8-45f2-de57-a1605444b9bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 10:22:19 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "03/29/2021 10:22:19 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.9967563 1.0032649]\n"
     ]
    }
   ],
   "source": [
    "# LanguageModel: Build pretrained language model\n",
    "EMBEDS_DROPOUT_PROB = 0.1\n",
    "TASK_NAME = \"text_classification\"\n",
    "\n",
    "language_model = LanguageModel.load(PRETRAINED_MODEL_NAME_OR_PATH, language=\"korean\")\n",
    "# PredictionHead: Build predictor layer\n",
    "prediction_head = TextClassificationHead(\n",
    "    num_labels=len(LABEL_LIST), \n",
    "    class_weights=data_silo.calculate_class_weights(\n",
    "        task_name=TASK_NAME\n",
    "    )\n",
    ")\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[prediction_head],\n",
    "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
    "    lm_output_types=[\"per_sequence\"],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhVOJTGugWlT",
    "outputId": "ead457e2-251a-4768-c9d4-a37d719acdaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <class 'farm.modeling.adaptive_model.AdaptiveModel'>\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: embeddings\n",
      "--------------------------------------------------------\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(300, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: encoder\n",
      "--------------------------------------------------------\n",
      "Showing last layer\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: feed_forward\n",
      "--------------------------------------------------------\n",
      "FeedForwardBlock(\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: loss_fct\n",
      "--------------------------------------------------------\n",
      "CrossEntropyLoss()\n",
      "Last Dropout Layer\n",
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {type(model)}\")\n",
    "for k, v in model.named_children():\n",
    "    for k1, v1 in v.named_children():\n",
    "        \n",
    "        for k2, v2 in v1.named_children():\n",
    "            print(\"----------------------------\"*2)\n",
    "            print(f\"Module: {k} | Layer: {k2}\")\n",
    "            print(\"----------------------------\"*2)\n",
    "            if k2 == \"encoder\":\n",
    "                print(\"Showing last layer\")\n",
    "                print(list(v2.children())[0][-1])\n",
    "                break\n",
    "            else:\n",
    "                print(v2)\n",
    "\n",
    "print(\"Last Dropout Layer\")\n",
    "print(model.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmMLVGnpTqxB",
    "outputId": "e1f6a8c7-3010-4cbf-e8f9-5f3c960f25a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Module: classifer\n",
      "--------------------------------------------------------\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "--------------------------------------------------------\n",
      "Module: dropout\n",
      "--------------------------------------------------------\n",
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "bert = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: classifer\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.classifier)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: dropout\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AMG9kkTmeoO"
   },
   "source": [
    "## Train & Eval & Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbvskPXaTlGp"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1bD54igqAn7T96gDCFZ2uxzFHpZIL5GOh\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4JEzzR4nZej"
   },
   "source": [
    "### Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "id": "IuqGruw3mzm2",
    "outputId": "2762b7d5-54c1-49b0-cb69-39b531b52cfa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 10:52:16 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 2e-05}'\n",
      "03/29/2021 10:52:16 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "03/29/2021 10:52:16 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 1674.7, 'num_training_steps': 16747}'\n",
      "03/29/2021 10:52:16 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.6181):   0%|          | 28/16747 [00:14<2:15:31,  2.06it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-aa4abf55ebca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# now train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mper_sample_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_to_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_sample_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;31m# Perform  evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/train.py\u001b[0m in \u001b[0;36mbackward_propagate\u001b[0;34m(self, loss, step)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_acc_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "N_EPOCHS = 1\n",
    "N_GPU = 1\n",
    "# Initialize Optimizer\n",
    "model, optimizer, lr_schedule = initialize_optimizer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_batches=len(data_silo.loaders[\"train\"]),\n",
    "    n_epochs=N_EPOCHS\n",
    ")\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_schedule=lr_schedule,\n",
    "    data_silo=data_silo,\n",
    "    epochs=N_EPOCHS,\n",
    "    n_gpu=N_GPU,\n",
    "    device=device, \n",
    ")\n",
    "# now train!\n",
    "model = trainer.train()\n",
    "model.save(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vr574Y5nWEu"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mhWTc4FstDU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/ColabNotebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nyAQNWsnlUK",
    "outputId": "22a8b301-fe24-4877-e7e1-32bac5e9ec57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 11:39:09 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "03/29/2021 11:39:11 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.9966925978660583, 1.0033293962478638]\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.prediction_head -   Loading prediction head from ckpt/NSMC/prediction_head_0.bin\n",
      "03/29/2021 11:39:12 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "03/29/2021 11:39:12 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   Got ya 1 parallel workers to do inference ...\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -    0 \n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   /w\\\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   /'\\\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   \n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1-0\n",
      "Clear Text: \n",
      " \ttext: 황정민 나오는 영화는 다 볼만한듯.\n",
      "Tokenized: \n",
      " \ttokens: ['황', '##정', '##민', '나오는', '영화', '##는', '다', '볼만', '##한듯', '.']\n",
      " \toffsets: [0, 1, 2, 4, 8, 10, 12, 14, 16, 18]\n",
      " \tstart_of_word: [True, False, False, True, True, False, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 3461, 4036, 4141, 9592, 9376, 4008, 786, 20421, 15510, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: 기생충,,, 이 영화 정말 재밌네요.\n",
      "Tokenized: \n",
      " \ttokens: ['기생충', ',', ',', ',', '이', '영화', '정말', '재밌', '##네요', '.']\n",
      " \toffsets: [0, 3, 4, 5, 7, 9, 12, 15, 17, 19]\n",
      " \tstart_of_word: [True, False, False, False, True, True, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 9409, 15, 15, 15, 2451, 9376, 8050, 13365, 8025, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.30s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from farm.infer import Inferencer\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "basic_texts = [\n",
    "    {\"text\": \"기생충,,, 이 영화 정말 재밌네요.\"},\n",
    "    {\"text\": \"황정민 나오는 영화는 다 볼만한듯.\"},\n",
    "]\n",
    "\n",
    "infer_model = Inferencer.load(\n",
    "    model_name_or_path=\"./ckpt/NSMC\",\n",
    "    task_type=\"text_classification\"\n",
    ")\n",
    "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-mk4CwptABQ",
    "outputId": "5d33ee9d-cdd1-4da8-c638-5a398e875728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'predictions': [{'context': '기생충,,, 이 영화 정말 재밌네요.',\n",
      "                   'end': None,\n",
      "                   'label': 'good',\n",
      "                   'probability': 0.83329886,\n",
      "                   'start': None},\n",
      "                  {'context': '황정민 나오는 영화는 다 볼만한듯.',\n",
      "                   'end': None,\n",
      "                   'label': 'good',\n",
      "                   'probability': 0.7448745,\n",
      "                   'start': None}],\n",
      "  'task': 'text_classification'}]\n"
     ]
    }
   ],
   "source": [
    "PrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WlcEEWGyVEZ"
   },
   "source": [
    "# MLflow\n",
    "\n",
    "public mlflow: https://public-mlflow.deepset.ai/#/experiments/313/runs/f7f5999c30194f1d964d0693e683be62\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1ip_--tgyo0M3V0mpGnONw0jTP2fv8sOE\" alt=\"Fine-tuning\" width=\"30%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=18PmyeDB4xsgrFVOmttsq0eiyFPzDm9IA\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA3QoKnvTz17"
   },
   "source": [
    "# TASK Supported\n",
    "\n",
    "|Task|BERT|RoBERTa*|XLNet|ALBERT|DistilBERT|XLMRoBERTa|ELECTRA|MiniLM|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|Text classification|x|x|x|x|x|x|x|x|\n",
    "|NER|x|x|x|x|x|x|x|x|\n",
    "|Question Answering|x|x|x|x|x|x|x|x|\n",
    "|Language Model Fine-tuning|x||||||||\n",
    "|Text Regression|x|x|x|x|x|x|x|x|\n",
    "|Multilabel Text classif.|x|x|x|x|x|x|x|x|\n",
    "|Extracting embeddings|x|x|x|x|x|x|x|x|\n",
    "|LM from scratch|x||||||||\n",
    "|Text Pair Classification|x|x|x|x|x|x|x|x|\n",
    "|Passage Ranking|x|x|x|x|x|x|x|x|\n",
    "|Document retrieval (DPR)|x|x||x|x|x|x|x|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhTAYSY1t4Sr"
   },
   "source": [
    "# Compare to others\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1TZoRpza8-o4wSTr0s16f8hHQRroLQg30\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "## FARM 장단점\n",
    "\n",
    "장점:\n",
    "\n",
    "* 데이터 세트만 준비되어 있으면, 다른 패키지에 비해 상대적으로 설정 할 것이 적음\n",
    "* 훈련 속도가 빠르고, 실험 기록 및 관리이 편리해서 빠르게 실험해 볼 수 있다.\n",
    "\n",
    "단점: \n",
    "\n",
    "* customization이 상대적으로 힘듦"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FARM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
