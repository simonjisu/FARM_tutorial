{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "innovative-search",
   "metadata": {},
   "source": [
    "# FARM\n",
    "\n",
    "\n",
    "> Framework for Adapting Representation Models\n",
    "\n",
    "## Core Features\n",
    "\n",
    "- **Easy fine-tuning of language models** to your task and domain language\n",
    "- **Speed**: AMP optimizers (~35% faster) and parallel preprocessing (16 CPU cores => ~16x faster)\n",
    "- **Modular design** of language models and prediction heads\n",
    "- Switch between heads or combine them for **multitask learning**\n",
    "- **Full Compatibility** with HuggingFace Transformers' models and model hub\n",
    "- **Smooth upgrading** to newer language models\n",
    "- Integration of **custom datasets** via Processor class\n",
    "- Powerful **experiment tracking** & execution\n",
    "- **Checkpointing & Caching** to resume training and reduce costs with spot instances\n",
    "- Simple **deployment** and **visualization** to showcase your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "engaging-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor, SquadProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import TextClassificationHead, QuestionAnsweringHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.train import Trainer\n",
    "from farm.utils import MLFlowLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indonesian-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"run farm\")\n",
    "parser.add_argument(\"--tracking_uri\", type=str, default=\"https://public-mlflow.deepset.ai/\",\n",
    "    help=\"MLFlow - tracking uri \")\n",
    "parser.add_argument(\"--experiment_name\", type=str, default=\"FARM_tutorial\",\n",
    "    help=\"MLFlow - experiment name\")\n",
    "parser.add_argument(\"--run_name\", type=str, default=\"NSMC\",\n",
    "    help=\"MLFlow - run name\")\n",
    "parser.add_argument(\"--pretrained_model_name_or_path\", type=str, default=\"beomi/kcbert-base\",\n",
    "    help=\"Tokenizer, LanguageModel - pretrained model name\")\n",
    "\n",
    "parser.add_argument(\"--train_filename\", type=str, default=\"train.tsv\",\n",
    "    help=\"Processor - train file name\")\n",
    "parser.add_argument(\"--test_filename\", type=str, default=\"test.tsv\",\n",
    "    help=\"Processor - test file name\")\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=150,\n",
    "    help=\"Processor - max sequence lenght of tokens\")\n",
    "parser.add_argument(\"--data_dir\",  type=str, default=\"./nsmc/\",\n",
    "    help=\"Processor - data directory\")\n",
    "parser.add_argument(\"--label_list\", nargs=\"*\", default=[\"bad\", \"good\"],\n",
    "    help=\"Processor - label list with string\")\n",
    "parser.add_argument(\"--metric\",  type=str, default=\"acc\",\n",
    "    help=\"Processor - acc or f1_macro\")\n",
    "parser.add_argument(\"--label_column_name\",  type=str, default=\"label\",\n",
    "    help=\"Processor - label column name\")\n",
    "parser.add_argument(\"--text_column_name\",  type=str, default=\"document\",\n",
    "    help=\"Processor - text column name\")\n",
    "parser.add_argument(\"--ckpt_path\",  type=str, default=\"./ckpt\",\n",
    "    help=\"Processor - checkpoint to save processor\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256,\n",
    "    help=\"DataSilo - train batch size\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=256,\n",
    "    help=\"DataSilo - eval batch size\")\n",
    "parser.add_argument(\"--embeds_dropout_prob\", type=float, default=0.1,\n",
    "    help=\"AdaptiveModel - The probability that a value in the embeddings returned by the language model will be zeroed.\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=2e-5,\n",
    "    help=\"initialize_optimizer - learning rate\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=1,\n",
    "    help=\"initialize_optimizer - number of epochs\")\n",
    "parser.add_argument(\"--n_gpu\", type=int, default=4,\n",
    "    help=\"Trainer - number of gpus\")\n",
    "parser.add_argument(\"--checkpoint_root_dir\", type=str, default=\"./ckpt\",\n",
    "    help=\"Trainer - checkpoint root directory\")\n",
    "parser.add_argument(\"--checkpoints_to_keep\", type=int, default=3,\n",
    "    help=\"Trainer - number of checkpoint to keep\")\n",
    "parser.add_argument(\"--checkpoint_every\", type=int, default=200,\n",
    "    help=\"Trainer - checkpoint every\")\n",
    "parser.add_argument(\"--evaluate_every\", type=int, default=200,\n",
    "    help=\"Trainer - evaluate steps\")\n",
    "args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-montana",
   "metadata": {},
   "source": [
    "# Total Process\n",
    "\n",
    "```\n",
    "Processor > DataSilo > Build Model(LanguageModel, TextClassificationHead, AdaptiveModel) > Build Optimizer & Trainer(initialize_optimizer, Trainer) > Train!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=args.pretrained_model_name_or_path,\n",
    "    do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-defeat",
   "metadata": {},
   "source": [
    "# Comparing to transformers with pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-calculator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
