{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need to run this cell\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/ColabNotebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Colab User \n",
    "\n",
    "* set `runtime=GPU` then start to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWSR3a5vxSGL"
   },
   "outputs": [],
   "source": [
    "# For Colab: Install FARM\n",
    "!pip install torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install farm==0.5.0\n",
    "!pip install -U -q emoji soynlp\n",
    "!git clone https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²˜ë¦¬í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iM4H-pbDL_XC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "def read_data(path:str, header=None):\n",
    "    return pd.read_csv(path, sep='\\t', header=header)\n",
    "\n",
    "def clean(x):\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\\x00-\\x7Fã„±-í£{emojis}]+')\n",
    "    url_pattern = re.compile(\n",
    "        r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    \n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x\n",
    "\n",
    "def preprocess_dataframe(df:pd.DataFrame):\n",
    "    r\"\"\"\n",
    "    Changed the code\n",
    "    source from: https://colab.research.google.com/drive/1IPkZo1Wd-DghIOK6gJpcb0Dv4_Gv2kXB\n",
    "    \"\"\"\n",
    "\n",
    "    label_dict = {0:\"bad\", 1:\"good\"}\n",
    "    df['document'] = df['document'].apply(lambda x: clean(str(x)))\n",
    "    df['label'] = df['label'].apply(label_dict.get)\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_dataframe(read_data(\"./nsmc/ratings_train.txt\", header=0))\n",
    "df_test = preprocess_dataframe(read_data(\"./nsmc/ratings_test.txt\", header=0))\n",
    "df_train.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/train.tsv\", sep=\"\\t\", index=False)\n",
    "df_test.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/test.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-PcIhBaB3-E",
    "outputId": "158cb1c2-8b2e-42b3-9c83-257c607fa318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\t\t  ratings_train.txt  raw\tsynopses.json  train.tsv\n",
      "ratings_test.txt  ratings.txt\t     README.md\ttest.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeTtCfRkyxVJ",
    "outputId": "8fb8c67c-7c93-45ec-85f9-e7e60ace7a90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 09:45:11 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __          __  _                            _        \n",
      " \\ \\        / / | |                          | |       \n",
      "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
      "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
      "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
      "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
      "  ______      _____  __  __  \n",
      " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
      " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
      " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
      " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
      " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
      "                                     |    |==|==|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      " \n",
      "Devices available: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import TextClassificationHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.train import Trainer\n",
    "from farm.utils import MLFlowLogger\n",
    "\n",
    "repo_path = Path() # Path().absolute().parent\n",
    "sys.path.append(str(repo_path))\n",
    "\n",
    "# Change to your experiment name and run name\n",
    "EXP_NAME = \"FARM_tutorial\"\n",
    "RUN_NAME = \"NSMC_colab\"\n",
    "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
    "ml_logger.init_experiment(experiment_name=EXP_NAME, run_name=RUN_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Devices available: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS3WaV8YMJmh"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1hbtUClFoXg45IbViZoFRLnnDGVlr9Dlb\" alt=\"Fine-tuning\" width=\"30%\" height=\"30%\"></center>\n",
    "\n",
    "# FARM\n",
    "\n",
    "> Framework for Adapting Representation Models\n",
    "\n",
    "ì´ íŒ¨í‚¤ì§€ë¥¼ í•œ ë§ˆë””ë¡œ ìš”ì•½í•˜ë©´ Fine-tuningì— ìµœì í™”ëœ ë„êµ¬ë‹¤.\n",
    "\n",
    "ìµœê·¼ì˜ ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ëŠ” Transformerì™€ ê·¸ ë³€í˜•ì˜ ë“±ì¥ìœ¼ë¡œ ì¸í•´, ë³´í†µ 2ë‹¨ê³„ë¡œ ë‚˜ëˆ ì„œ í•™ìŠµì´ ì§„í–‰ëœë‹¤. \n",
    "1. **Pretrained Language Modeling**\n",
    "\n",
    "   ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë¹„ì§€ë„í•™ìŠµ(unsupervised learning)ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ë§ì€ ì§„í–‰í•œë‹¤. ì–¸ì–´ ëª¨ë¸ë§ì´ë€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì»´í“¨í„°ë¡œ ëª¨ë¸ë§í•˜ëŠ” ê³¼ì •ì´ë‹¤. ì‰½ê²Œ ë§í•˜ë©´, ëª¨ë¸ì—ê²Œ ë‹¨ì–´ë“¤ì„ ì…ë ¥í–ˆì„ ë•Œ, ì œì¼ ë§ì´ ë˜ëŠ” ë‹¨ì–´(í† í°)ì„ ë±‰ì–´ë‚´ê²Œ í•˜ëŠ” ê²ƒì´ë‹¤. ê³¼ê±°ì—ëŠ” ë‹¨ì–´(í† í°)ì˜ ìˆœì„œê°€ ì¤‘ìš”í–ˆì—ˆë‹¤. ì¦‰, ì¼ì • ë‹¨ì–´ë“¤ì˜ ì‹œí€€ìŠ¤ $x_{1:t-1}$ê°€ ì£¼ì–´ì§€ë©´, $t$ë²ˆì§¸ ë‹¨ì–´ì¸ $x_t$ë¥¼ ì˜ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ì—ˆë‹¤. ì´ë¥¼ Auto Regressive Modelingì´ë¼ê³ ë„ í•œë‹¤. ê·¸ëŸ¬ë‚˜, Masked Language Modeling ë°©ë²•ì´ ë“±ì¥í–ˆëŠ”ë°, ì´ëŠ” ëœë¤ìœ¼ë¡œ ë§ì¶°ì•¼í•  ë‹¨ì–´ë¥¼ ê°€ë¦° ë‹¤ìŒì— ê°€ë ¤ì§„ ë‹¨ì–´ $x_{mask}$ê°€ í¬í•¨ëœ ì‹œí€€ìŠ¤ $x_{1:t}$ ë¥¼ ëª¨ë¸ì—ê²Œ ì…ë ¥í•˜ì—¬ ë§ì¶”ëŠ” í•™ìŠµ ë°©ë²•ì´ë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì´ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘ë©´ì„œ, ìµœê·¼ì—ëŠ” ëª¨ë“  \n",
    "2. **Fine-tuning**\n",
    "\n",
    "    PLM(Pretrained Language Model)ì„ ë§Œë“¤ê³  ë‚˜ë©´, ê°ê¸° ë‹¤ë¥¸ downstream taskì— ë”°ë¼ì„œ fine-tuningì„ í•˜ê²Œ ëœë‹¤. Downstream taskì€ êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œë¥¼ ë§í•˜ë©°, ì£¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë“¤ì´ë‹¤.\n",
    "    * í…ìŠ¤íŠ¸ ë¶„ë¥˜ Text Classification - ì˜ˆì‹œ: ì˜í™” ëŒ“ê¸€ ê¸ì •/ë¶€ì • ë¶„ë¥˜í•˜ê¸°\n",
    "    * ê°œì²´ëª…ì¸ì‹ NER(Named Entity Recognition) - ì˜ˆì‹œ: íŠ¹ì • ê¸°ê´€ëª…, ì¸ëª… ë° ì‹œê°„ ë‚ ì§œ ë“± í† í°ì— ì•Œë§ëŠ” íƒœê·¸ë¡œ ë¶„ë¥˜í•˜ê¸°\n",
    "    * ì§ˆì˜ì‘ë‹µ Question and Answering - ì˜ˆì‹œ: íŠ¹ì • ì§€ë¬¸ê³¼ ì§ˆì˜(query)ê°€ ì£¼ì–´ì§€ë©´ ëŒ€ë‹µí•˜ê¸°\n",
    "\n",
    "ì˜¤ëŠ˜ ì†Œê°œí•  FARM íŒ¨í‚¤ì§€ëŠ” 2ë²ˆì§¸ ë‹¨ê³„ì¸ Fine-tuningì„ ë³´ë‹¤ ì†ì‰½ê²Œ ë§Œë“¤ì–´ë†“ì€ íŒ¨í‚¤ì§€ë‹¤. Githubì—ì„œ Colab tutorialê³¼ í•¨ê»˜ ë³´ë©´ ì¢‹ë‹¤.\n",
    "\n",
    "- **Tutorial github:** https://github.com/simonjisu/FARM_tutorial\n",
    "- **Colab Tutorial:** [ë§í¬](https://colab.research.google.com/github/simonjisu/FARM_tutorial/blob/main/notebooks/FARM_colab.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PkdHkjZoR4A"
   },
   "source": [
    "## Core Features\n",
    "\n",
    "- **Easy fine-tuning of language models**Â to your task and domain language\n",
    "- **Speed**: AMP(Automatic Mixed Precision) optimizers (~35% faster) and parallel preprocessing (16 CPU cores => ~16x faster)\n",
    "- **Modular design**Â of language models and prediction heads\n",
    "- Switch between heads or combine them forÂ **multitask learning**\n",
    "- **Full Compatibility**Â with HuggingFace Transformers' models and model hub\n",
    "- **Smooth upgrading**Â to newer language models\n",
    "- Integration ofÂ **custom datasets**Â via Processor class\n",
    "- PowerfulÂ **experiment tracking**Â & execution\n",
    "- **Checkpointing & Caching**Â to resume training and reduce costs with spot instances\n",
    "- SimpleÂ **deployment**Â andÂ **visualization**Â to showcase your model\n",
    "\n",
    "<details>\n",
    "<summary> ğŸ‘‰ what is AMP? </summary>\n",
    "\n",
    "**Reference**\n",
    "- https://github.com/NVIDIA/apex\n",
    "- https://forums.fast.ai/t/mixed-precision-training/20720\n",
    "\n",
    "**mixed precision trainingì´ë€**\n",
    "- ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ FP16(16bit floating point)ì—°ì‚°ê³¼ ì •í™•ë„ ìœ ì§€ë¥¼ ìœ„í•œ FP32 ì—°ì‚°ì„ ì„ì–´ í•™ìŠµí•˜ëŠ” ë°©ë²•\n",
    "- Tensor Coreë¥¼ í™œìš©í•œ FP16ì—°ì‚°ì„ ì´ìš©í•˜ë©´ FP32ì—°ì‚° ëŒ€ë¹„Â ì ˆë°˜ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ 8ë°°ì˜ ì—°ì‚° ì²˜ë¦¬ëŸ‰ & 2ë°°ì˜ ë©”ëª¨ë¦¬ ì²˜ë¦¬ëŸ‰ íš¨ê³¼ê°€ ìˆë‹¤\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjzUlWjDobxD"
   },
   "source": [
    "# NSMC ë°ì´í„°ë¡œ FARM ì•Œì•„ë³´ê¸°\n",
    "\n",
    "## NSMC ë°ì´í„°\n",
    "\n",
    "NSMC(Naver Sentiment Movie Corpus)ëŠ” í•œêµ­ì–´ë¡œ ëœ ì˜í™” ëŒ“ê¸€ ë°ì´í„° ì„¸íŠ¸ë‹¤. í•´ë‹¹ TaskëŠ” íƒ€ê²Ÿ ê°’ì´ ê¸ì •(1)/ë¶€ì •(0)ì´ ë˜ëŠ” Binary Text Classification ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆë‹¤. https://github.com/e9t/nsmc ì—ì„œ ë°›ì„ ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1FIGIBtZxtuKD5Prps5vPOPldBb0xHwzH\" alt=\"Fine-tuning\" width=\"50%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP4UcaeKoVM8"
   },
   "source": [
    "## Fine-tuning Process\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1j9pn8Lpg7sy6S8Ubvq3E7JLWf28KvRt4\" alt=\"Fine-tuning\" width=\"50%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Fine-tuning ProcessëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.\n",
    "\n",
    "* Load Data: ë°ì´í„°ë¥¼ ì•Œë§ëŠ” í˜•ì‹(json, csv ë“±)ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
    "* Create Dataset: ë°ì´í„°ì„¸íŠ¸(Dataset) ë§Œë“¤ê¸°\n",
    "    * Tokenization: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ë‹¨ì–´ì¥(vocab)ì„ ìƒì„±í•œë‹¤.\n",
    "    * ToTensor: vocabì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ìˆ˜ì¹˜í™”í•˜ëŠ” ê³¼ì • (`input_ids` in transformers)\n",
    "    * Attention Mask: íŒ¨ë”©ê³„ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´ Attention í•´ì•¼í•  í† í°ë§Œ masking(`attention_mask` in transformers)\n",
    "* Create Dataloader: í›ˆë ¨, í‰ê°€ì‹œ ë°°ì¹˜í¬ê¸° ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê°ì²´\n",
    "* Create Model:\n",
    "    * Pretrained Language Model: ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ì „ì— í›ˆë ¨ëœ ëª¨ë¸ \n",
    "$$\\underset{\\theta}{\\arg \\max} P(x_{mask} \\vert x_{1:t})$$\n",
    "    * Fine-tuninig Layer: Downstream Taskì— ë§ì¶°ì„œ í•™ìŠµí•œë‹¤.       \n",
    "$$\\underset{\\theta}{\\arg \\max}P(y\\vert x_{1:t})$$\n",
    "        ì˜ˆë¥¼ ë“¤ì–´, ì˜í™” ê¸ì •/ë¶€ì • ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš°\n",
    "$$\\underset{\\theta}{\\arg \\max} P(y=\\text{ê¸ì •/ë¶€ì •} \\vert x_{1:t})$$\n",
    "* Train Model: ëª¨ë¸ í›ˆë ¨\n",
    "* Eval Model: ëª¨ë¸ í‰ê°€\n",
    "* Inference: ëª¨ë¸ ì„œë¹„ìŠ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsQKHSPlMovO"
   },
   "source": [
    "## Processor & Data Silo\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1XCc0AJpPBMFcC81NW0A6w0mpswZ2KU7h\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **Processor**ëŠ” file í˜¹ì€ requestë¥¼ PyTorch Datsetë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” ì—­í• ì´ë‹¤. ìì„¸í•œ ì¸ìê°’ì€ ë‹¤ìŒ ì½”ë“œ ë¸”ë¡ì—ì„œ ì„¤ëª…í•œë‹¤.\n",
    "* **Data Silo**ëŠ” train, dev, test setsë¥¼ ê´€ë¦¬í•˜ê³ , Processorì˜ functionë“¤ ì´ìš©í•´ ê° setë¥¼ DataLoaderë¡œ ë³€í™˜í•œë‹¤.\n",
    "* **Processor**ëŠ” ê° ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ, **Samples**, **SampleBasket**ì— ë‹´ê²Œ ë˜ëŠ”ë°, ì´ë“¤ì€ raw documentë¥¼ ê´€ë¦¬í•˜ëŠ” ê°ì²´ì´ë©° tokenized, featuresë“± ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ìˆë‹¤. ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” í•˜ë‚˜ì˜ ì†ŒìŠ¤ í…ìŠ¤íŠ¸(raw text)ì—ì„œ ì—¬ëŸ¬ê°œì˜ ìƒ˜í”Œì„ ìƒì„±í•  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ë‹¤(e.g. QA task)\n",
    "    ```python\n",
    "    def dataset_from_dicts(self, ...)\n",
    "        # ...\n",
    "        for dictionary, input_ids, segment_ids, padding_mask, tokens in zip(\n",
    "                dicts, input_ids_batch, segment_ids_batch, padding_masks_batch, tokens_batch\n",
    "        ):\n",
    "            # ...\n",
    "            # Add Basket to self.baskets\n",
    "            curr_sample = Sample(\n",
    "                id=None,\n",
    "                clear_text=dictionary,\n",
    "                tokenized=tokenized,\n",
    "                features=[feat_dict]\n",
    "            )\n",
    "            curr_basket = SampleBasket(\n",
    "                id_internal=None,\n",
    "                raw=dictionary,\n",
    "                id_external=None,\n",
    "                samples=[curr_sample]\n",
    "               )\n",
    "            self.baskets.append(curr_basket)\n",
    "\n",
    "        # ...\n",
    "    ```\n",
    "\n",
    "ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuQEtIvNTI5V",
    "outputId": "57da6862-931f-4490-b86a-e7700da2b698"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 09:45:12 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/29/2021 09:45:14 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "03/29/2021 09:45:14 - INFO - farm.data_handler.data_silo -   Loading train set from: nsmc/train.tsv \n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 149539 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   /|\\\n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   /'\\\n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/train.tsv:   0%|          | 0/149539 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/29/2021 09:45:17 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/29/2021 09:45:17 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1213-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: ê·¸ë¦¼ì²´ëŠ” ì¢‹ë‹¤. ê·¸ë ‡ë‹¤ ê·¸ë¿ì´ë‹¤ ë‚´ìš© ì „ê°œë„ ì–µì§€ìŠ¤ëŸ½ê³  ì„¤ëª… ë¶€ì¡±ì— ë¿Œë¦° ë–¡ë°¥ì€ ë§ìœ¼ë‚˜ ì¹˜ì›Œì§€ì§€ ì•ŠëŠ” ë–¡ë°¥ ì‚¬ì‹¤ ì´ í‰ì  1ì ë„ ì´ ê¸€ì„ ì“°ê¸°ìœ„í•¨ì´ì§€ ì‚¬ì‹¤ ì ìˆ˜ê°€ ëœë‹¤ë©´ -ë¬´í•œëŒ€ë¥¼ ì£¼ê³  ì‹¶ì„ë§Œí¼ ì¡¸ì‘ì´ì ì“°ë˜ê¸°ì´ë‹¤. ê°œì¸ì ì¸ ì˜ê²¬ì´ì§€ë§Œ..\n",
      "Tokenized: \n",
      " \ttokens: ['ê·¸ë¦¼', '##ì²´', '##ëŠ”', 'ì¢‹ë‹¤', '.', 'ê·¸ë ‡ë‹¤', 'ê·¸', '##ë¿ì´ë‹¤', 'ë‚´ìš©', 'ì „', '##ê°œë„', 'ì–µì§€', '##ìŠ¤ëŸ½ê³ ', 'ì„¤ëª…', 'ë¶€ì¡±', '##ì—', 'ë¿Œë¦°', 'ë–¡', '##ë°¥', '##ì€', 'ë§ìœ¼', '##ë‚˜', 'ì¹˜ì›Œ', '##ì§€ì§€', 'ì•ŠëŠ”', 'ë–¡', '##ë°¥', 'ì‚¬ì‹¤', 'ì´', 'í‰', '##ì ', '1', '##ì ë„', 'ì´', 'ê¸€ì„', 'ì“°ê¸°', '##ìœ„', '##í•¨ì´', '##ì§€', 'ì‚¬ì‹¤', 'ì ìˆ˜', '##ê°€', 'ëœë‹¤ë©´', '-', 'ë¬´í•œ', '##ëŒ€ë¥¼', 'ì£¼ê³ ', 'ì‹¶ì„', '##ë§Œí¼', 'ì¡¸', '##ì‘', '##ì´ì', 'ì“°ë˜ê¸°', '##ì´ë‹¤', '.', 'ê°œì¸ì ì¸', 'ì˜ê²¬ì´', '##ì§€ë§Œ', '.', '.']\n",
      " \toffsets: [0, 2, 3, 5, 7, 9, 13, 14, 18, 21, 22, 25, 27, 31, 34, 36, 38, 41, 42, 43, 45, 47, 49, 51, 54, 57, 58, 60, 63, 65, 66, 68, 69, 72, 74, 77, 79, 80, 82, 84, 87, 89, 91, 95, 96, 98, 101, 104, 106, 109, 110, 111, 114, 117, 119, 121, 126, 129, 131, 132]\n",
      " \tstart_of_word: [True, False, False, True, False, True, True, False, True, True, False, True, False, True, True, False, True, True, False, False, True, False, True, False, True, True, False, True, True, True, False, True, False, True, True, True, False, False, False, True, True, False, True, True, False, False, True, True, False, True, False, False, True, False, False, True, True, False, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 13447, 4230, 4008, 10350, 17, 12120, 391, 12678, 9132, 2525, 25289, 12676, 13937, 10415, 9633, 4113, 19542, 1023, 4578, 4057, 20847, 4136, 19129, 8840, 8743, 1023, 4578, 8220, 2451, 3288, 4213, 20, 24896, 2451, 12342, 25912, 4069, 11467, 4102, 8220, 25899, 4009, 17524, 16, 14244, 10633, 9568, 22519, 8801, 2579, 4240, 10575, 15893, 7976, 17, 20485, 21100, 8015, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/29/2021 09:45:17 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 214-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: good\n",
      " \ttext: ê°•ë¶€ì ì—¬ë™ìƒ ëŒ€ì‚¬ì™€ ì—°ê¸°ì— ë„‹ì„ ìƒê³  ë´¤ë„¤\n",
      "Tokenized: \n",
      " \ttokens: ['ê°•', '##ë¶€ì', 'ì—¬', '##ë™ìƒ', 'ëŒ€ì‚¬', '##ì™€', 'ì—°ê¸°', '##ì—', 'ë„‹', '##ì„', 'ìƒê³ ', 'ë´¤', '##ë„¤']\n",
      " \toffsets: [0, 1, 4, 5, 8, 10, 12, 14, 16, 17, 19, 22, 23]\n",
      " \tstart_of_word: [True, False, True, False, True, False, True, False, True, False, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 213, 27729, 2269, 16247, 23805, 4196, 11219, 4113, 636, 4027, 18629, 1594, 4011, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/train.tsv:   5%|â–Œ         | 8000/149539 [00:07<02:20, 1009.24 Dicts/s]03/29/2021 09:45:23 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  11%|â–ˆ         | 16000/149539 [00:15<02:07, 1048.69 Dicts/s]03/29/2021 09:45:32 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:45:32 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  17%|â–ˆâ–‹        | 26000/149539 [00:25<01:59, 1035.47 Dicts/s]03/29/2021 09:45:41 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:45:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:45:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  19%|â–ˆâ–Š        | 28000/149539 [00:27<01:58, 1026.38 Dicts/s]03/29/2021 09:45:44 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  21%|â–ˆâ–ˆâ–       | 32000/149539 [00:31<01:54, 1030.38 Dicts/s]03/29/2021 09:45:47 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  28%|â–ˆâ–ˆâ–Š       | 42000/149539 [00:40<01:45, 1019.58 Dicts/s]03/29/2021 09:45:57 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  32%|â–ˆâ–ˆâ–ˆâ–      | 48000/149539 [00:46<01:36, 1051.39 Dicts/s]03/29/2021 09:46:04 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  35%|â–ˆâ–ˆâ–ˆâ–      | 52000/149539 [00:50<01:33, 1046.30 Dicts/s]03/29/2021 09:46:06 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68000/149539 [01:05<01:19, 1029.91 Dicts/s]03/29/2021 09:46:21 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70000/149539 [01:07<01:16, 1035.14 Dicts/s]03/29/2021 09:46:24 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74000/149539 [01:11<01:13, 1031.48 Dicts/s]03/29/2021 09:46:27 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:46:28 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:46:28 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76000/149539 [01:13<01:10, 1045.74 Dicts/s]03/29/2021 09:46:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82000/149539 [01:19<01:05, 1035.73 Dicts/s]03/29/2021 09:46:35 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102000/149539 [01:38<00:44, 1058.87 Dicts/s]03/29/2021 09:46:54 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:46:54 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104000/149539 [01:40<00:43, 1051.16 Dicts/s]03/29/2021 09:46:56 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112000/149539 [01:47<00:35, 1053.11 Dicts/s]03/29/2021 09:47:04 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:47:05 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 128000/149539 [02:03<00:20, 1034.29 Dicts/s]03/29/2021 09:47:20 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136000/149539 [02:11<00:13, 1036.30 Dicts/s]03/29/2021 09:47:27 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138000/149539 [02:13<00:11, 1013.23 Dicts/s]03/29/2021 09:47:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 144000/149539 [02:18<00:05, 1027.34 Dicts/s]03/29/2021 09:47:36 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149539/149539 [02:23<00:00, 1039.25 Dicts/s]\n",
      "03/29/2021 09:47:39 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
      "03/29/2021 09:47:39 - INFO - farm.data_handler.data_silo -   Took 15536 samples out of train set to create dev set (dev split is roughly 0.1)\n",
      "03/29/2021 09:47:39 - INFO - farm.data_handler.data_silo -   Loading test set from: nsmc/test.tsv\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 50000 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   /w\\\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   / \\\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/test.tsv:   0%|          | 0/50000 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/29/2021 09:47:42 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/29/2021 09:47:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 71-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: ë¯¸ì¹œë†ˆë“¤ ì§‘í•©ì†Œë„¤ ì—°ì¶œë ¥ ë¹µì , ìŠ¤í† ë¦¬ ë¹µì . êµ¬ì„±ë¹µì \n",
      "Tokenized: \n",
      " \ttokens: ['ë¯¸ì¹œ', '##ë†ˆë“¤', 'ì§‘í•©ì†Œ', '##ë„¤', 'ì—°ì¶œ', '##ë ¥', 'ë¹µ', '##ì ', ',', 'ìŠ¤í† ', '##ë¦¬', 'ë¹µ', '##ì ', '.', 'êµ¬ì„±', '##ë¹µ', '##ì ']\n",
      " \toffsets: [0, 2, 5, 8, 10, 12, 14, 15, 16, 18, 20, 22, 23, 24, 26, 28, 29]\n",
      " \tstart_of_word: [True, False, True, False, True, False, True, False, False, True, False, True, False, False, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 8257, 8423, 20933, 4011, 23889, 4286, 1692, 4213, 15, 17319, 4038, 1692, 4213, 17, 17474, 4555, 4213, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/29/2021 09:47:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 767-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: ê·¸ëƒ¥ ë‹µë‹µí•˜ë‹¤. ì˜í™”ë³´ëŠ” ë‚´ë‚´ ìš°ìš¸í•œ ë¶„ìœ„ê¸° ...\n",
      "Tokenized: \n",
      " \ttokens: ['ê·¸ëƒ¥', 'ë‹µë‹µí•˜ë‹¤', '.', 'ì˜í™”', '##ë³´ëŠ”', 'ë‚´ë‚´', 'ìš°ìš¸', '##í•œ', 'ë¶„ìœ„ê¸°', '.', '.', '.']\n",
      " \toffsets: [0, 3, 7, 9, 11, 14, 17, 19, 21, 25, 26, 27]\n",
      " \tstart_of_word: [True, True, False, True, False, True, True, False, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 8019, 13234, 17, 9376, 9136, 16714, 15343, 4047, 12655, 17, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/test.tsv:   8%|â–Š         | 4000/50000 [00:04<00:47, 969.59 Dicts/s]03/29/2021 09:47:45 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  12%|â–ˆâ–        | 6000/50000 [00:05<00:44, 994.10 Dicts/s]03/29/2021 09:47:46 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:47:47 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:47:48 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  24%|â–ˆâ–ˆâ–       | 12000/50000 [00:11<00:36, 1044.20 Dicts/s]03/29/2021 09:47:52 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24000/50000 [00:23<00:25, 1029.36 Dicts/s]03/29/2021 09:48:03 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:48:04 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [00:25<00:23, 1026.97 Dicts/s]03/29/2021 09:48:06 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [00:30<00:17, 1030.19 Dicts/s]03/29/2021 09:48:12 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40000/50000 [00:38<00:09, 1049.44 Dicts/s]03/29/2021 09:48:19 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46000/50000 [00:44<00:03, 1020.58 Dicts/s]03/29/2021 09:48:25 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:48<00:00, 1031.86 Dicts/s]\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Cached the datasets at cache/data_silo/ac8b9ede46e0a9423d4adc9c41a611b6\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Examples in train: 133976\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Examples in dev  : 15536\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Examples in test : 49989\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   \n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     150\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 19.57721532214725\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Proportion clipped:      4.478414044306443e-05\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = repo_path / \"nsmc\"\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"beomi/kcbert-base\"  # Reference: https://github.com/Beomi/KcBERT\n",
    "MAX_LENGTH = 150\n",
    "LABEL_LIST = [\"bad\", \"good\"]\n",
    "TRAIN_FILE = \"train.tsv\"\n",
    "TEST_FILE = \"test.tsv\"\n",
    "TASK_TYPE = \"text_classification\"\n",
    "\n",
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "\n",
    "processor = TextClassificationProcessor(\n",
    "    tokenizer=tokenizer,  # tokenizer \n",
    "    train_filename=TRAIN_FILE,  # training data íŒŒì¼ëª…\n",
    "    dev_filename=None,  # development data íŒŒì¼ëª…, ì—†ìœ¼ë©´, dev_split ë¹„ìœ¨ë§Œí¼ training dataì—ì„œ ìë¥¸ë‹¤ \n",
    "    test_filename=TEST_FILE,  # test data íŒŒì¼ëª…\n",
    "    dev_split=0.1,  # development dataë¡œ ì„¤ì •í•  ë¹„ìœ¨\n",
    "    header=0,  # csv, tsv, excel ë“± tabularí˜•íƒœ ë°ì´í„°ì—ì„œ ì²«í–‰(ë³´í†µì€ ì»¬ëŸ¼ëª…)ì˜ ìœ„ì¹˜\n",
    "    max_seq_len=MAX_LENGTH,  # ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´\n",
    "    data_dir=str(DATA_PATH),  # ë°ì´í„°ì˜ ë””ë ‰í† ë¦¬\n",
    "    label_list=LABEL_LIST,  # ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸(string í•„ìš”)\n",
    "    metric=\"acc\",  # í‰ê°€ì§€í‘œ\n",
    "    label_column_name=\"label\",  # tabularí˜•íƒœ ë°ì´í„°ì—ì„œ ë ˆì´ë¸”ì˜ ì»¬ëŸ¼ëª…\n",
    "    text_column_name=\"document\",  # tabularí˜•íƒœ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ì˜ ì»¬ëŸ¼ëª…\n",
    "    delimiter=\"\\t\"\n",
    ")\n",
    "\n",
    "\n",
    "data_silo = DataSilo(\n",
    "    processor=processor,\n",
    "    batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBqy3S68OIX9"
   },
   "source": [
    "ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ tokenization ë˜ë©°, sample ê°ì²´ì— ì €ì¥ëœë‹¤. \n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1DVPT_Rjv_SI4ggJZzqfPh0MgsMa1Q9El\" alt=\"Fine-tuning\" width=\"100%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "```plaintext\n",
    "03/28/2021 22:12:15 - INFO - farm.data_handler.processor -   \n",
    "\n",
    "      .--.        _____                       _      \n",
    "    .'_\\/_'.     / ____|                     | |     \n",
    "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
    "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
    "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
    "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
    "   (/\\||/                             |_|           \n",
    "______\\||/___________________________________________                     \n",
    "\n",
    "ID: 437-0\n",
    "Clear Text: \n",
    " \ttext_classification_label: good\n",
    " \ttext: ì´ ì˜í™”ë¥¼ ë³´ê³  ë‘í†µì´ ë‚˜ì•˜ìŠµë‹ˆë‹¤. ã…  ã… \n",
    "Tokenized: \n",
    " \ttokens: ['ì´', 'ì˜í™”ë¥¼', 'ë³´ê³ ', 'ë‘', '##í†µì´', 'ë‚˜', '##ì•˜ìŠµë‹ˆë‹¤', '.', '[UNK]', '[UNK]']\n",
    " \toffsets: [0, 2, 6, 9, 10, 13, 14, 18, 20, 22]\n",
    " \tstart_of_word: [True, True, True, True, False, True, False, False, True, True]\n",
    "Features: \n",
    " \tinput_ids: [2, 2451, 25833, 8198, 917, 11765, 587, 21809, 17, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \ttext_classification_label_ids: [1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ_tuLHRTgGb"
   },
   "source": [
    "## Modeling Layers: AdaptiveModel = LanguageModel + PredictionHead\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1OLWdr8rh7ucpF9t55gzVeMawMBJbRiEC\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **LanguageModel**ì€ pretrained language models(BERT, XLNet ...)ì˜ í‘œì¤€ í´ë˜ìŠ¤ \n",
    "* **PredictionHead**ëŠ” ëª¨ë“  down-stream tasks(NER, Text classification, QA ...)ë¥¼ í‘œì¤€ í´ë˜ìŠ¤\n",
    "* **AdaptiveModel**ì€ ìœ„ ë‘ ê°€ì§€ ëª¨ë“¤ì˜ ê²°í•©, í•˜ë‚˜ì˜ LanguageModelê³¼ ì—¬ëŸ¬ ê°œì˜ PredictionHeadë¥¼ ê²°í•©í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60M4IEtWTitY",
    "outputId": "f56fa874-aff8-45f2-de57-a1605444b9bf"
   },
   "outputs": [],
   "source": [
    "# LanguageModel: Build pretrained language model\n",
    "EMBEDS_DROPOUT_PROB = 0.1\n",
    "TASK_NAME = \"text_classification\"\n",
    "\n",
    "language_model = LanguageModel.load(PRETRAINED_MODEL_NAME_OR_PATH, language=\"korean\")\n",
    "# PredictionHead: Build predictor layer\n",
    "prediction_head = TextClassificationHead(\n",
    "    num_labels=len(LABEL_LIST), \n",
    "    class_weights=data_silo.calculate_class_weights(\n",
    "        task_name=TASK_NAME\n",
    "    )\n",
    ")\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[prediction_head],\n",
    "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
    "    lm_output_types=[\"per_sequence\"],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhVOJTGugWlT",
    "outputId": "ead457e2-251a-4768-c9d4-a37d719acdaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <class 'farm.modeling.adaptive_model.AdaptiveModel'>\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: embeddings\n",
      "--------------------------------------------------------\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(300, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: encoder\n",
      "--------------------------------------------------------\n",
      "Showing last layer\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: feed_forward\n",
      "--------------------------------------------------------\n",
      "FeedForwardBlock(\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: loss_fct\n",
      "--------------------------------------------------------\n",
      "CrossEntropyLoss()\n",
      "Last Dropout Layer\n",
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {type(model)}\")\n",
    "for k, v in model.named_children():\n",
    "    for k1, v1 in v.named_children():\n",
    "        \n",
    "        for k2, v2 in v1.named_children():\n",
    "            print(\"----------------------------\"*2)\n",
    "            print(f\"Module: {k} | Layer: {k2}\")\n",
    "            print(\"----------------------------\"*2)\n",
    "            if k2 == \"encoder\":\n",
    "                print(\"Showing last layer\")\n",
    "                print(list(v2.children())[0][-1])\n",
    "                break\n",
    "            else:\n",
    "                print(v2)\n",
    "\n",
    "print(\"Last Dropout Layer\")\n",
    "print(model.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹¤ì œë¡œ transformers íŒ¨í‚¤ì§€ì™€ ë¹„êµí•´ë³´ë©´ ë¹„ìŠ·í•˜ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmMLVGnpTqxB",
    "outputId": "e1f6a8c7-3010-4cbf-e8f9-5f3c960f25a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Module: classifer\n",
      "--------------------------------------------------------\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "--------------------------------------------------------\n",
      "Module: dropout\n",
      "--------------------------------------------------------\n",
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "bert = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: classifer\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.classifier)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: dropout\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbvskPXaTlGp"
   },
   "source": [
    "## Train & Eval & Inference\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1bD54igqAn7T96gDCFZ2uxzFHpZIL5GOh\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4JEzzR4nZej"
   },
   "source": [
    "### Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "id": "IuqGruw3mzm2",
    "outputId": "2762b7d5-54c1-49b0-cb69-39b531b52cfa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 10:52:16 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 2e-05}'\n",
      "03/29/2021 10:52:16 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "03/29/2021 10:52:16 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 1674.7, 'num_training_steps': 16747}'\n",
      "03/29/2021 10:52:16 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.6181):   0%|          | 28/16747 [00:14<2:15:31,  2.06it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-aa4abf55ebca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# now train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mper_sample_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_to_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_sample_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;31m# Perform  evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/train.py\u001b[0m in \u001b[0;36mbackward_propagate\u001b[0;34m(self, loss, step)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_acc_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "N_EPOCHS = 1\n",
    "N_GPU = 1\n",
    "checkpoint_path = \"./ckpt/NSMC\"\n",
    "\n",
    "# Initialize Optimizer\n",
    "model, optimizer, lr_schedule = initialize_optimizer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_batches=len(data_silo.loaders[\"train\"]),\n",
    "    n_epochs=N_EPOCHS\n",
    ")\n",
    "# EarlyStopping\n",
    "earlymetric = \"f1\" if args.task_name == \"question_answering\" else \"acc\" \n",
    "mode = \"max\" if args.task_name in [\"text_classification\", \"question_answering\"] else \"min\"\n",
    "earlystop = EarlyStopping(\n",
    "    save_dir=checkpoint_path,\n",
    "    metric=earlymetric,\n",
    "    mode=mode,\n",
    "    patience=3,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_schedule=lr_schedule,\n",
    "    data_silo=data_silo,\n",
    "    early_stopping=earlystop,\n",
    "    evaluate_every=20,\n",
    "    checkpoints_to_keep=3,\n",
    "    checkpoint_root_dir=checkpoint_path,\n",
    "    checkpoint_every=200,\n",
    "    epochs=N_EPOCHS,\n",
    "    n_gpu=N_GPU,\n",
    "    device=device, \n",
    ")\n",
    "# now train!\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í›ˆë ¨ ê³¼ì •ì— ê³„ì† Logê°€ ì°íˆê³ , Processorë‹¨ê³„ì—ì„œ ì…ë ¥í•´ë‘” `test_filename`ë¡œ í‰ê°€ë„ í•´ì¤€ë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì€ 430ê°œì˜ ë°°ì¹˜ ë°ì´í„°(batch_size=256)ë¥¼ ëŒë ¸ì„ ë•Œ earlystoppingí•œ ê²°ê³¼ë‹¤.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1m1K9CjBNulC4dzSxC1vKjLb94p9BQu26\" alt=\"Fine-tuning\" width=\"80%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vr574Y5nWEu"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mhWTc4FstDU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/ColabNotebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nyAQNWsnlUK",
    "outputId": "22a8b301-fe24-4877-e7e1-32bac5e9ec57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 11:39:09 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "03/29/2021 11:39:11 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.9966925978660583, 1.0033293962478638]\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.prediction_head -   Loading prediction head from ckpt/NSMC/prediction_head_0.bin\n",
      "03/29/2021 11:39:12 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "03/29/2021 11:39:12 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   Got ya 1 parallel workers to do inference ...\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -    0 \n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   /w\\\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   /'\\\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   \n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1-0\n",
      "Clear Text: \n",
      " \ttext: í™©ì •ë¯¼ ë‚˜ì˜¤ëŠ” ì˜í™”ëŠ” ë‹¤ ë³¼ë§Œí•œë“¯.\n",
      "Tokenized: \n",
      " \ttokens: ['í™©', '##ì •', '##ë¯¼', 'ë‚˜ì˜¤ëŠ”', 'ì˜í™”', '##ëŠ”', 'ë‹¤', 'ë³¼ë§Œ', '##í•œë“¯', '.']\n",
      " \toffsets: [0, 1, 2, 4, 8, 10, 12, 14, 16, 18]\n",
      " \tstart_of_word: [True, False, False, True, True, False, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 3461, 4036, 4141, 9592, 9376, 4008, 786, 20421, 15510, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: ê¸°ìƒì¶©,,, ì´ ì˜í™” ì •ë§ ì¬ë°Œë„¤ìš”.\n",
      "Tokenized: \n",
      " \ttokens: ['ê¸°ìƒì¶©', ',', ',', ',', 'ì´', 'ì˜í™”', 'ì •ë§', 'ì¬ë°Œ', '##ë„¤ìš”', '.']\n",
      " \toffsets: [0, 3, 4, 5, 7, 9, 12, 15, 17, 19]\n",
      " \tstart_of_word: [True, False, False, False, True, True, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 9409, 15, 15, 15, 2451, 9376, 8050, 13365, 8025, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "Inferencing Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.30s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from farm.infer import Inferencer\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "basic_texts = [\n",
    "    {\"text\": \"ê¸°ìƒì¶©... ì´ ì˜í™” ì •ë§ ì˜¬í•´ì˜ ëŒ€ì‘ì„.\"},\n",
    "    {\"text\": \"í™©ì •ë¯¼ ë‚˜ì˜¤ëŠ” ì˜í™”ëŠ” ë‹¤ ë³¼ë§Œí•œë“¯.\"},\n",
    "]\n",
    "\n",
    "infer_model = Inferencer.load(\n",
    "    model_name_or_path=\"./ckpt/best_nsmc\",\n",
    "    task_type=\"text_classification\"\n",
    ")\n",
    "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-mk4CwptABQ",
    "outputId": "5d33ee9d-cdd1-4da8-c638-5a398e875728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'predictions': [{'context': 'ê¸°ìƒì¶©,,, ì´ ì˜í™” ì •ë§ ì¬ë°Œë„¤ìš”.',\n",
      "                   'end': None,\n",
      "                   'label': 'good',\n",
      "                   'probability': 0.83329886,\n",
      "                   'start': None},\n",
      "                  {'context': 'í™©ì •ë¯¼ ë‚˜ì˜¤ëŠ” ì˜í™”ëŠ” ë‹¤ ë³¼ë§Œí•œë“¯.',\n",
      "                   'end': None,\n",
      "                   'label': 'good',\n",
      "                   'probability': 0.7448745,\n",
      "                   'start': None}],\n",
      "  'task': 'text_classification'}]\n"
     ]
    }
   ],
   "source": [
    "PrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WlcEEWGyVEZ"
   },
   "source": [
    "# MLflow\n",
    "\n",
    "MLflowë¥¼ ì´ìš©í•˜ ë¹ ë¥´ê³  ì‰½ê²Œ ì‹¤í—˜ì„ ê´€ë¦¬í•˜ê³ , ê´€ë ¨ í‰ê°€ì§€í‘œë„ í•¨ê»˜ ë³¼ ìˆ˜ ìˆë‹¤. public mlflow([ë§í¬](https://public-mlflow.deepset.ai/#/experiments/313/runs/05e7e3d4945642f9ab3e296637d57c26))ì—ì„œ í™•ì¸í•˜ê¸°\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=13Cg8eziHBgA3JLwZJ3Bo8YzeySWPRmiP\" alt=\"Fine-tuning\" width=\"30%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Trainê³¼ Dev ì„¸íŠ¸ì˜ lossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1cpFWVvjkSqshvN0hS_CuPk4RjyEyM0AV\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Dev ì„¸íŠ¸ì˜ ì •í™•ë„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1VPso9Gx60V8_dgE4as054n7kymCoQ9w5\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA3QoKnvTz17"
   },
   "source": [
    "# TASK Supported\n",
    "\n",
    "|Task|BERT|RoBERTa*|XLNet|ALBERT|DistilBERT|XLMRoBERTa|ELECTRA|MiniLM|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|Text classification|x|x|x|x|x|x|x|x|\n",
    "|NER|x|x|x|x|x|x|x|x|\n",
    "|Question Answering|x|x|x|x|x|x|x|x|\n",
    "|Language Model Fine-tuning|x||||||||\n",
    "|Text Regression|x|x|x|x|x|x|x|x|\n",
    "|Multilabel Text classif.|x|x|x|x|x|x|x|x|\n",
    "|Extracting embeddings|x|x|x|x|x|x|x|x|\n",
    "|LM from scratch|x||||||||\n",
    "|Text Pair Classification|x|x|x|x|x|x|x|x|\n",
    "|Passage Ranking|x|x|x|x|x|x|x|x|\n",
    "|Document retrieval (DPR)|x|x||x|x|x|x|x|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhTAYSY1t4Sr"
   },
   "source": [
    "# Compare to others\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1TZoRpza8-o4wSTr0s16f8hHQRroLQg30\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµí•´ë³´ë©´ FARMì€ ì¡°ê¸ˆ ë” huggingfaceì™€ pytorch-lightningì˜ í•©ë³¸ ì¶•ì•½ ë²„ì „ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. ë§ˆì¹˜ Tensorflow v1ê³¼ kerasì˜ ì°¨ì´ ëŠë‚Œì´ë‹¤.\n",
    "\n",
    "## FARM ì¥ë‹¨ì \n",
    "\n",
    "ì¥ì :\n",
    "\n",
    "* ë°ì´í„° ì„¸íŠ¸ë§Œ ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë©´, ë‹¤ë¥¸ íŒ¨í‚¤ì§€ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì„¤ì • í•  ê²ƒì´ ì ìŒ\n",
    "* í›ˆë ¨ ì†ë„ê°€ ë¹ ë¥´ê³ , ì‹¤í—˜ ê¸°ë¡ ë° ê´€ë¦¬ì´ í¸ë¦¬í•´ì„œ ë¹ ë¥´ê²Œ ì‹¤í—˜í•´ ë³¼ ìˆ˜ ìˆë‹¤.\n",
    "* í…ì„œë³´ë“œ ëŒ€ì‹  mlflow ì‚¬ìš© ê°€ëŠ¥\n",
    "\n",
    "ë‹¨ì : \n",
    "\n",
    "* customizationì´ ìƒëŒ€ì ìœ¼ë¡œ í˜ë“¦\n",
    "* ì•„ì§ ë°œì „ ì¤‘ì´ë¼ ë¶ˆì•ˆì •í•˜ê³  documentatonì´ ì˜ ì•ˆë˜ì–´ ìˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FARM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
