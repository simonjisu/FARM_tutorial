{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4dmCNpOc64i"
   },
   "source": [
    "# For Colab User \n",
    "\n",
    "* set `runtime=GPU` then start to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWSR3a5vxSGL",
    "outputId": "e77e8eb2-2cdf-4ab0-cbe1-645dc90f8ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.6.0+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
      "Requirement already satisfied: farm==0.5.0 in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.3.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (2.23.0)\n",
      "Requirement already satisfied: torch<1.7,>1.5 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.6.0+cu101)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (54.1.2)\n",
      "Requirement already satisfied: flask-cors in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (3.0.10)\n",
      "Requirement already satisfied: flask-restplus in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.13.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.17.40)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.36.2)\n",
      "Requirement already satisfied: dotmap==1.3.0 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.3.0)\n",
      "Requirement already satisfied: Werkzeug==0.16.1 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.16.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.4.1)\n",
      "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.0.12)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (4.41.1)\n",
      "Requirement already satisfied: transformers==3.3.1 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (3.3.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (5.4.8)\n",
      "Requirement already satisfied: mlflow==1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (1.24.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<1.7,>1.5->farm==0.5.0) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<1.7,>1.5->farm==0.5.0) (1.19.5)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (7.1.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (2.11.3)\n",
      "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors->farm==0.5.0) (1.15.0)\n",
      "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (9.0.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (2018.9)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->farm==0.5.0) (0.22.2.post1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (0.3.6)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.40 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (1.20.40)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->farm==0.5.0) (2.4.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.0.43)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.8.1rc2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (2019.12.20)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.1.95)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (20.9)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.12.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.1.5)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.1.14)\n",
      "Requirement already satisfied: databricks-cli>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.14.3)\n",
      "Requirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.4.1)\n",
      "Requirement already satisfied: gunicorn in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (20.1.0)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.3.23)\n",
      "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.2.4)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.5.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.13)\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.17.2)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.3.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.3)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (2.8.1)\n",
      "Requirement already satisfied: docker>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (4.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->farm==0.5.0) (1.1.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->farm==0.5.0) (1.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->farm==0.5.0) (2.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1->farm==0.5.0) (2.4.7)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (4.0.7)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.5.0) (0.8.9)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (1.1.4)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (1.0.4)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from docker>=3.6.0->mlflow==1.0.0->farm==0.5.0) (0.58.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (4.0.0)\n",
      "Cloning into 'nsmc'...\n",
      "remote: Enumerating objects: 14763, done.\u001b[K\n",
      "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
      "Receiving objects: 100% (14763/14763), 56.19 MiB | 21.48 MiB/s, done.\n",
      "Resolving deltas: 100% (1749/1749), done.\n",
      "Checking out files: 100% (14737/14737), done.\n"
     ]
    }
   ],
   "source": [
    "# For Colab: Install FARM\n",
    "!pip install torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install farm==0.5.0\n",
    "!pip install -U -q emoji soynlp\n",
    "!git clone https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxcBddyhc64j"
   },
   "source": [
    "ì „ì²˜ë¦¬í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iM4H-pbDL_XC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "def read_data(path:str, header=None):\n",
    "    return pd.read_csv(path, sep='\\t', header=header)\n",
    "\n",
    "def clean(x):\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\\x00-\\x7Fã„±-í£{emojis}]+')\n",
    "    url_pattern = re.compile(\n",
    "        r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    \n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x\n",
    "\n",
    "def preprocess_dataframe(df:pd.DataFrame):\n",
    "    r\"\"\"\n",
    "    Changed the code\n",
    "    source from: https://colab.research.google.com/drive/1IPkZo1Wd-DghIOK6gJpcb0Dv4_Gv2kXB\n",
    "    \"\"\"\n",
    "\n",
    "    label_dict = {0:\"bad\", 1:\"good\"}\n",
    "    df['document'] = df['document'].apply(lambda x: clean(str(x)))\n",
    "    df['label'] = df['label'].apply(label_dict.get)\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_dataframe(read_data(\"./nsmc/ratings_train.txt\", header=0))\n",
    "df_test = preprocess_dataframe(read_data(\"./nsmc/ratings_test.txt\", header=0))\n",
    "df_train.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/train.tsv\", sep=\"\\t\", index=False)\n",
    "df_test.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/test.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-PcIhBaB3-E",
    "outputId": "d85d0f31-16e2-41c2-e783-214bc035b744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\t\t  ratings_train.txt  raw\tsynopses.json  train.tsv\n",
      "ratings_test.txt  ratings.txt\t     README.md\ttest.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeTtCfRkyxVJ",
    "outputId": "6d8a2731-f944-420f-cd3b-277508cfdfb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:29:20 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __          __  _                            _        \n",
      " \\ \\        / / | |                          | |       \n",
      "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
      "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
      "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
      "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
      "  ______      _____  __  __  \n",
      " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
      " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
      " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
      " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
      " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
      "                                     |    |==|==|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      " \n",
      "Devices available: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import TextClassificationHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.train import Trainer, EarlyStopping\n",
    "from farm.utils import MLFlowLogger\n",
    "\n",
    "repo_path = Path() # Path().absolute().parent\n",
    "sys.path.append(str(repo_path))\n",
    "\n",
    "# Change to your experiment name and run name\n",
    "EXP_NAME = \"FARM_tutorial\"\n",
    "RUN_NAME = \"NSMC_colab\"\n",
    "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
    "ml_logger.init_experiment(experiment_name=EXP_NAME, run_name=RUN_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Devices available: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS3WaV8YMJmh"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1hbtUClFoXg45IbViZoFRLnnDGVlr9Dlb\" alt=\"Fine-tuning\" width=\"30%\" height=\"30%\"></center>\n",
    "\n",
    "# FARM\n",
    "\n",
    "> Framework for Adapting Representation Models\n",
    "\n",
    "ì´ íŒ¨í‚¤ì§€ë¥¼ í•œ ë§ˆë””ë¡œ ìš”ì•½í•˜ë©´ Fine-tuningì— ìµœì í™”ëœ ë„êµ¬ë‹¤.\n",
    "\n",
    "ìµœê·¼ì˜ ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ëŠ” Transformerì™€ ê·¸ ë³€í˜•ì˜ ë“±ì¥ìœ¼ë¡œ ì¸í•´, ë³´í†µ 2ë‹¨ê³„ë¡œ ë‚˜ëˆ ì„œ í•™ìŠµì´ ì§„í–‰ëœë‹¤. \n",
    "1. **Pretrained Language Modeling**\n",
    "\n",
    "   ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë¹„ì§€ë„í•™ìŠµ(unsupervised learning)ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ë§ì€ ì§„í–‰í•œë‹¤. ì–¸ì–´ ëª¨ë¸ë§ì´ë€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì»´í“¨í„°ë¡œ ëª¨ë¸ë§í•˜ëŠ” ê³¼ì •ì´ë‹¤. ì‰½ê²Œ ë§í•˜ë©´, ëª¨ë¸ì—ê²Œ ë‹¨ì–´ë“¤ì„ ì…ë ¥í–ˆì„ ë•Œ, ì œì¼ ë§ì´ ë˜ëŠ” ë‹¨ì–´(í† í°)ì„ ë±‰ì–´ë‚´ê²Œ í•˜ëŠ” ê²ƒì´ë‹¤. ê³¼ê±°ì—ëŠ” ë‹¨ì–´(í† í°)ì˜ ìˆœì„œê°€ ì¤‘ìš”í–ˆì—ˆë‹¤. ì¦‰, ì¼ì • ë‹¨ì–´ë“¤ì˜ ì‹œí€€ìŠ¤ $x_{1:t-1}$ê°€ ì£¼ì–´ì§€ë©´, $t$ë²ˆì§¸ ë‹¨ì–´ì¸ $x_t$ë¥¼ ì˜ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ì—ˆë‹¤. ì´ë¥¼ Auto Regressive Modelingì´ë¼ê³ ë„ í•œë‹¤. ê·¸ëŸ¬ë‚˜, Masked Language Modeling ë°©ë²•ì´ ë“±ì¥í–ˆëŠ”ë°, ì´ëŠ” ëœë¤ìœ¼ë¡œ ë§ì¶°ì•¼í•  ë‹¨ì–´ë¥¼ ê°€ë¦° ë‹¤ìŒì— ê°€ë ¤ì§„ ë‹¨ì–´ $x_{mask}$ê°€ í¬í•¨ëœ ì‹œí€€ìŠ¤ $x_{1:t}$ ë¥¼ ëª¨ë¸ì—ê²Œ ì…ë ¥í•˜ì—¬ ë§ì¶”ëŠ” í•™ìŠµ ë°©ë²•ì´ë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì´ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘ë©´ì„œ, ìµœê·¼ì—ëŠ” ëª¨ë“  ì–¸ì–´ëª¨ë¸ë§ ê¸°ë²•ë“¤ì´ MLMì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆë‹¤.\n",
    "2. **Fine-tuning**\n",
    "\n",
    "    PLM(Pretrained Language Model)ì„ ë§Œë“¤ê³  ë‚˜ë©´, ê°ê¸° ë‹¤ë¥¸ downstream taskì— ë”°ë¼ì„œ fine-tuningì„ í•˜ê²Œ ëœë‹¤. Downstream taskì€ êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œë¥¼ ë§í•˜ë©°, ì£¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë“¤ì´ë‹¤.\n",
    "    * í…ìŠ¤íŠ¸ ë¶„ë¥˜ Text Classification - ì˜ˆì‹œ: ì˜í™” ëŒ“ê¸€ ê¸ì •/ë¶€ì • ë¶„ë¥˜í•˜ê¸°\n",
    "    * ê°œì²´ëª…ì¸ì‹ NER(Named Entity Recognition) - ì˜ˆì‹œ: íŠ¹ì • ê¸°ê´€ëª…, ì¸ëª… ë° ì‹œê°„ ë‚ ì§œ ë“± í† í°ì— ì•Œë§ëŠ” íƒœê·¸ë¡œ ë¶„ë¥˜í•˜ê¸°\n",
    "    * ì§ˆì˜ì‘ë‹µ Question and Answering - ì˜ˆì‹œ: íŠ¹ì • ì§€ë¬¸ê³¼ ì§ˆì˜(query)ê°€ ì£¼ì–´ì§€ë©´ ëŒ€ë‹µí•˜ê¸°\n",
    "\n",
    "ì˜¤ëŠ˜ ì†Œê°œí•  FARM íŒ¨í‚¤ì§€ëŠ” 2ë²ˆì§¸ ë‹¨ê³„ì¸ Fine-tuningì„ ë³´ë‹¤ ì†ì‰½ê²Œ ë§Œë“¤ì–´ë†“ì€ íŒ¨í‚¤ì§€ë‹¤. Githubì—ì„œ Colab tutorialê³¼ í•¨ê»˜ ë³´ë©´ ì¢‹ë‹¤.\n",
    "\n",
    "- **ì½”ë“œ ì—†ëŠ” ë²„ì „ - Blog:** [ë§í¬](https://simonjisu.github.io/nlp/2021/03/30/farm.html)\n",
    "- **Tutorial github:** [ë§í¬](https://github.com/simonjisu/FARM_tutorial)\n",
    "- **Colab Tutorial:** [ë§í¬](https://colab.research.google.com/github/simonjisu/FARM_tutorial/blob/main/notebooks/FARM_colab.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PkdHkjZoR4A"
   },
   "source": [
    "## Core Features\n",
    "\n",
    "- **Easy fine-tuning of language models**Â to your task and domain language\n",
    "- **Speed**: AMP(Automatic Mixed Precision) optimizers (~35% faster) and parallel preprocessing (16 CPU cores => ~16x faster)\n",
    "- **Modular design**Â of language models and prediction heads\n",
    "- Switch between heads or combine them forÂ **multitask learning**\n",
    "- **Full Compatibility**Â with HuggingFace Transformers' models and model hub\n",
    "- **Smooth upgrading**Â to newer language models\n",
    "- Integration ofÂ **custom datasets**Â via Processor class\n",
    "- PowerfulÂ **experiment tracking**Â & execution\n",
    "- **Checkpointing & Caching**Â to resume training and reduce costs with spot instances\n",
    "- SimpleÂ **deployment**Â andÂ **visualization**Â to showcase your model\n",
    "\n",
    "<details>\n",
    "<summary> ğŸ‘‰ what is AMP? </summary>\n",
    "\n",
    "**Reference**\n",
    "- https://github.com/NVIDIA/apex\n",
    "- https://forums.fast.ai/t/mixed-precision-training/20720\n",
    "\n",
    "**mixed precision trainingì´ë€**\n",
    "- ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ FP16(16bit floating point)ì—°ì‚°ê³¼ ì •í™•ë„ ìœ ì§€ë¥¼ ìœ„í•œ FP32 ì—°ì‚°ì„ ì„ì–´ í•™ìŠµí•˜ëŠ” ë°©ë²•\n",
    "- Tensor Coreë¥¼ í™œìš©í•œ FP16ì—°ì‚°ì„ ì´ìš©í•˜ë©´ FP32ì—°ì‚° ëŒ€ë¹„Â ì ˆë°˜ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ 8ë°°ì˜ ì—°ì‚° ì²˜ë¦¬ëŸ‰ & 2ë°°ì˜ ë©”ëª¨ë¦¬ ì²˜ë¦¬ëŸ‰ íš¨ê³¼ê°€ ìˆë‹¤\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enUPk-Nac64m"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjzUlWjDobxD"
   },
   "source": [
    "# NSMC ë°ì´í„°ë¡œ FARM ì•Œì•„ë³´ê¸°\n",
    "\n",
    "## NSMC ë°ì´í„°\n",
    "\n",
    "NSMC(Naver Sentiment Movie Corpus)ëŠ” í•œêµ­ì–´ë¡œ ëœ ì˜í™” ëŒ“ê¸€ ë°ì´í„° ì„¸íŠ¸ë‹¤. í•´ë‹¹ TaskëŠ” íƒ€ê²Ÿ ê°’ì´ ê¸ì •(1)/ë¶€ì •(0)ì´ ë˜ëŠ” Binary Text Classification ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆë‹¤. https://github.com/e9t/nsmc ì—ì„œ ë°›ì„ ìˆ˜ ìˆë‹¤(ì•„ë˜ ê·¸ë¦¼ì€ labelì„ badì™€ goodìœ¼ë¡œ ì²˜ë¦¬í•´ë†“ì€ ìƒíƒœ).\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1FIGIBtZxtuKD5Prps5vPOPldBb0xHwzH\" alt=\"Fine-tuning\" width=\"50%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP4UcaeKoVM8"
   },
   "source": [
    "## Fine-tuning Process\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1j9pn8Lpg7sy6S8Ubvq3E7JLWf28KvRt4\" alt=\"Fine-tuning\" width=\"50%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Fine-tuning ProcessëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.\n",
    "\n",
    "* Load Data: ë°ì´í„°ë¥¼ ì•Œë§ëŠ” í˜•ì‹(json, csv ë“±)ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
    "* Create Dataset: ë°ì´í„°ì„¸íŠ¸(Dataset) ë§Œë“¤ê¸°\n",
    "    * Tokenization: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ë‹¨ì–´ì¥(vocab)ì„ ìƒì„±í•œë‹¤.\n",
    "    * ToTensor: vocabì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ìˆ˜ì¹˜í™”í•˜ëŠ” ê³¼ì • (`input_ids` in transformers)\n",
    "    * Attention Mask: íŒ¨ë”©ê³„ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´ Attention í•´ì•¼í•  í† í°ë§Œ masking(`attention_mask` in transformers)\n",
    "* Create Dataloader: í›ˆë ¨, í‰ê°€ì‹œ ë°°ì¹˜í¬ê¸° ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê°ì²´\n",
    "* Create Model:\n",
    "    * Pretrained Language Model: ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ì „ì— í›ˆë ¨ëœ ëª¨ë¸ \n",
    "$$\\underset{\\theta}{\\arg \\max} P(x_{mask} \\vert x_{1:t})$$\n",
    "    * Fine-tuninig Layer: Downstream Taskì— ë§ì¶°ì„œ í•™ìŠµí•œë‹¤.       \n",
    "$$\\underset{\\theta}{\\arg \\max}P(y\\vert x_{1:t})$$\n",
    "        ì˜ˆë¥¼ ë“¤ì–´, ì˜í™” ê¸ì •/ë¶€ì • ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš°\n",
    "$$\\underset{\\theta}{\\arg \\max} P(y=\\text{ê¸ì •/ë¶€ì •} \\vert x_{1:t})$$\n",
    "* Train Model: ëª¨ë¸ í›ˆë ¨\n",
    "* Eval Model: ëª¨ë¸ í‰ê°€\n",
    "* Inference: ëª¨ë¸ ì„œë¹„ìŠ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsQKHSPlMovO"
   },
   "source": [
    "## Processor & Data Silo\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1XCc0AJpPBMFcC81NW0A6w0mpswZ2KU7h\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **Processor**ëŠ” file í˜¹ì€ requestë¥¼ PyTorch Datsetë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” ì—­í• ì´ë‹¤. ìì„¸í•œ ì¸ìê°’ì€ ë‹¤ìŒ ì½”ë“œ ë¸”ë¡ì—ì„œ ì„¤ëª…í•œë‹¤.\n",
    "* **Data Silo**ëŠ” train, dev, test setsë¥¼ ê´€ë¦¬í•˜ê³ , Processorì˜ functionë“¤ ì´ìš©í•´ ê° setë¥¼ DataLoaderë¡œ ë³€í™˜í•œë‹¤.\n",
    "* **Processor**ëŠ” ê° ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ, **Samples**, **SampleBasket**ì— ë‹´ê²Œ ë˜ëŠ”ë°, ì´ë“¤ì€ raw documentë¥¼ ê´€ë¦¬í•˜ëŠ” ê°ì²´ì´ë©° tokenized, featuresë“± ë°ì´í„°ì™€ ê° ìƒ˜í”Œì„ ê´€ë¦¬í•˜ëŠ” idë¥¼ ì €ì¥í•˜ê³  ìˆë‹¤. ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” í•˜ë‚˜ì˜ ì†ŒìŠ¤ í…ìŠ¤íŠ¸(raw text)ì—ì„œ ì—¬ëŸ¬ê°œì˜ ìƒ˜í”Œì„ ìƒì„±í•  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\n",
    "\n",
    "    ì—¬ë‹´ì´ì§€ë§Œ huggingfaceì˜ SquadProcessorëŠ” 512ê°œ í† í°ì´ ë„˜ì–´ê°€ë©´, ë’¤ì—ì„œ ë¶€í„° 512í† í°ì„ ì„¸ì„œ í•˜ë‚˜ì˜ ë°ì´í„°ë¥¼ ë‘ ê°œì˜ ìƒ˜í”Œë¡œ ë§Œë“ ë‹¤. \n",
    "    ```python\n",
    "    def dataset_from_dicts(self, ...)\n",
    "        # ...\n",
    "        for dictionary, input_ids, segment_ids, padding_mask, tokens in zip(\n",
    "                dicts, input_ids_batch, segment_ids_batch, padding_masks_batch, tokens_batch\n",
    "        ):\n",
    "            # ...\n",
    "            # Add Basket to self.baskets\n",
    "            curr_sample = Sample(\n",
    "                id=None,\n",
    "                clear_text=dictionary,\n",
    "                tokenized=tokenized,\n",
    "                features=[feat_dict]\n",
    "            )\n",
    "            curr_basket = SampleBasket(\n",
    "                id_internal=None,\n",
    "                raw=dictionary,\n",
    "                id_external=None,\n",
    "                samples=[curr_sample]\n",
    "               )\n",
    "            self.baskets.append(curr_basket)\n",
    "\n",
    "        # ...\n",
    "    ```\n",
    "\n",
    "ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuQEtIvNTI5V",
    "outputId": "814ce9ea-87cb-446e-96a9-c397b68e8d41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:29:29 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/30/2021 15:29:30 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "03/30/2021 15:29:30 - INFO - farm.data_handler.data_silo -   Loading train set from: nsmc/train.tsv \n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 149539 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -   /w\\\n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -   /'\\\n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/train.tsv:   0%|          | 0/149539 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/30/2021 15:29:32 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/30/2021 15:29:32 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1580-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: ì»¨ì €ë§, ë°ëª¨ë‹‰ì˜ í¥í–‰ì„ ì—…ê³  í•œë²ˆ í•´ë³´ë ¤ í–ˆë˜ ê²ƒ ê°™ìœ¼ë‚˜, ì „ ì‹œë¦¬ì¦ˆì™€ëŠ” ë¹„êµë„ í•  ìˆ˜ ì—†ëŠ” OOOê¸°\n",
      "Tokenized: \n",
      " \ttokens: ['ì»¨', '##ì €', '##ë§', ',', 'ë°ëª¨', '##ë‹‰', '##ì˜', 'í¥', '##í–‰ì„', 'ì—…ê³ ', 'í•œë²ˆ', 'í•´ë³´', '##ë ¤', 'í–ˆë˜', 'ê²ƒ', 'ê°™', '##ìœ¼ë‚˜', ',', 'ì „', 'ì‹œ', '##ë¦¬', '##ì¦ˆ', '##ì™€ëŠ”', 'ë¹„êµ', '##ë„', 'í• ', 'ìˆ˜', 'ì—†ëŠ”', 'OOO', '##ê¸°']\n",
      " \toffsets: [0, 1, 2, 3, 5, 7, 8, 10, 11, 14, 17, 20, 22, 24, 27, 29, 30, 32, 34, 36, 37, 38, 39, 42, 44, 46, 48, 50, 53, 56]\n",
      " \tstart_of_word: [True, False, False, False, True, False, False, True, False, True, True, True, False, True, True, True, False, False, True, True, False, False, False, True, False, True, True, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 3012, 4488, 5190, 15, 11047, 5623, 4042, 3524, 12857, 25563, 8295, 11660, 4135, 10129, 258, 217, 10410, 15, 2525, 2002, 4038, 4146, 15542, 8898, 4029, 3358, 1931, 8106, 8128, 4184, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/30/2021 15:29:32 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 448-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: í‰ì ì´ ë„ˆë¬´ ë†’ì€ê±° ê°™ì•„ ë‚®ì¶”ëŸ¬ ì™”ì”ë‹ˆë‹¤~ 4.35ë‚˜ ë˜ë‹¤ë‹ˆ\n",
      "Tokenized: \n",
      " \ttokens: ['í‰', '##ì ì´', 'ë„ˆë¬´', 'ë†’ì€', '##ê±°', 'ê°™ì•„', 'ë‚®ì¶”', '##ëŸ¬', 'ì™”', '##ì”', '##ë‹ˆë‹¤', '~', '4', '.', '35', '##ë‚˜', 'ë˜', '##ë‹¤ë‹ˆ']\n",
      " \toffsets: [0, 1, 4, 7, 9, 11, 14, 16, 18, 19, 20, 22, 24, 25, 26, 28, 30, 31]\n",
      " \tstart_of_word: [True, False, True, True, False, True, True, False, True, False, False, False, True, False, False, False, True, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 3288, 11283, 8069, 10974, 4014, 10891, 21716, 4053, 2327, 5267, 7969, 95, 23, 17, 14603, 4136, 900, 8273, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/train.tsv:   1%|â–         | 2000/149539 [00:01<02:02, 1206.29 Dicts/s]03/30/2021 15:29:33 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:   4%|â–         | 6000/149539 [00:04<01:51, 1285.64 Dicts/s]03/30/2021 15:29:36 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:   5%|â–Œ         | 8000/149539 [00:06<01:48, 1299.31 Dicts/s]03/30/2021 15:29:37 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:29:38 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  15%|â–ˆâ–        | 22000/149539 [00:16<01:30, 1412.42 Dicts/s]03/30/2021 15:29:49 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  16%|â–ˆâ–Œ        | 24000/149539 [00:17<01:33, 1348.19 Dicts/s]03/30/2021 15:29:50 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  19%|â–ˆâ–Š        | 28000/149539 [00:20<01:28, 1374.91 Dicts/s]03/30/2021 15:29:53 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  20%|â–ˆâ–ˆ        | 30000/149539 [00:22<01:29, 1338.03 Dicts/s]03/30/2021 15:29:54 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  21%|â–ˆâ–ˆâ–       | 32000/149539 [00:23<01:30, 1305.97 Dicts/s]03/30/2021 15:29:56 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  23%|â–ˆâ–ˆâ–       | 34000/149539 [00:25<01:24, 1370.28 Dicts/s]03/30/2021 15:29:57 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  25%|â–ˆâ–ˆâ–Œ       | 38000/149539 [00:28<01:22, 1352.03 Dicts/s]03/30/2021 15:30:00 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:30:00 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  35%|â–ˆâ–ˆâ–ˆâ–      | 52000/149539 [00:38<01:13, 1335.52 Dicts/s]03/30/2021 15:30:10 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 56000/149539 [00:41<01:10, 1320.73 Dicts/s]03/30/2021 15:30:13 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60000/149539 [00:44<01:06, 1352.65 Dicts/s]03/30/2021 15:30:16 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 62000/149539 [00:46<01:05, 1343.71 Dicts/s]03/30/2021 15:30:18 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 80000/149539 [00:59<00:50, 1387.15 Dicts/s]03/30/2021 15:30:31 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:30:31 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:30:32 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84000/149539 [01:02<00:50, 1303.80 Dicts/s]03/30/2021 15:30:34 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92000/149539 [01:08<00:43, 1322.72 Dicts/s]03/30/2021 15:30:41 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 94000/149539 [01:10<00:42, 1308.10 Dicts/s]03/30/2021 15:30:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:30:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106000/149539 [01:19<00:31, 1364.89 Dicts/s]03/30/2021 15:30:51 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132000/149539 [01:38<00:13, 1346.16 Dicts/s]03/30/2021 15:31:10 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138000/149539 [01:43<00:08, 1286.17 Dicts/s]03/30/2021 15:31:14 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 144000/149539 [01:47<00:03, 1390.82 Dicts/s]03/30/2021 15:31:19 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149539/149539 [01:51<00:00, 1342.08 Dicts/s]\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   Took 15537 samples out of train set to create dev set (dev split is roughly 0.1)\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   Loading test set from: nsmc/test.tsv\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 50000 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   /w\\\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   / \\\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/test.tsv:   0%|          | 0/50000 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/30/2021 15:31:24 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/30/2021 15:31:24 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1345-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: ì—”ë„ ìŠˆì‚¬ì¿ ê°€ ì“´ 'ì™•ë¹„ ë§ˆë¦¬ ì•™íˆ¬ì•„ë„¤íŠ¸' ì±… ì½ê³  ì´ ì˜í™” ë³´ë‹ˆ ì´ê±´ ê·¸ëƒ¥ í”„ë‘ìŠ¤ ë“œë ˆìŠ¤ ì…ê³  í•˜ì´í‹´ë¬¼ ì°ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ë„¤ìš”-_-\n",
      "Tokenized: \n",
      " \ttokens: ['ì—”', '##ë„', 'ìŠˆ', '##ì‚¬', '##ì¿ ', '##ê°€', 'ì“´', \"'\", 'ì™•', '##ë¹„', 'ë§ˆ', '##ë¦¬', 'ì•™', '##íˆ¬', '##ì•„ë„¤', '##íŠ¸', \"'\", 'ì±…', 'ì½ê³ ', 'ì´', 'ì˜í™”', 'ë³´ë‹ˆ', 'ì´ê±´', 'ê·¸ëƒ¥', 'í”„ë‘ìŠ¤', 'ë“œ', '##ë ˆìŠ¤', 'ì…ê³ ', 'í•˜ì´', '##í‹´', '##ë¬¼', 'ì°ëŠ”', 'ê²ƒì²˜ëŸ¼', 'ë³´ì´ë„¤ìš”', '-', '_', '-']\n",
      " \toffsets: [0, 1, 3, 4, 5, 6, 8, 10, 11, 12, 14, 15, 17, 18, 19, 21, 22, 24, 26, 29, 31, 34, 37, 40, 43, 47, 48, 51, 54, 56, 57, 59, 62, 66, 70, 71, 72]\n",
      " \tstart_of_word: [True, False, True, False, False, False, True, True, False, False, True, False, True, False, False, False, False, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, True, True, False, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 2257, 4029, 1975, 4107, 5047, 4009, 2141, 10, 2328, 4167, 1293, 4038, 2188, 4466, 27889, 4104, 10, 2856, 13985, 2451, 9376, 8844, 8259, 8019, 12818, 947, 10770, 12841, 15830, 4713, 4336, 17841, 12710, 21533, 16, 64, 16, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/30/2021 15:31:24 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 123-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: good\n",
      " \ttext: í›Œë¥­í•œ ì‘ê°€ì™€ ë›°ì–´ë‚œ ì—°ì¶œì§„ì´ ë§Œë‚˜, í›Œë¥­í•˜ê³  ë›°ì–´ë‚œ í•œ í¸ì˜ ë“œë¼ë§ˆê°€ ë§Œë“¤ì–´ì¡Œë‹¤.\n",
      "Tokenized: \n",
      " \ttokens: ['í›Œë¥­í•œ', 'ì‘ê°€', '##ì™€', 'ë›°ì–´ë‚œ', 'ì—°ì¶œ', '##ì§„ì´', 'ë§Œë‚˜', ',', 'í›Œë¥­', '##í•˜ê³ ', 'ë›°ì–´ë‚œ', 'í•œ', 'í¸ì˜', 'ë“œë¼ë§ˆ', '##ê°€', 'ë§Œë“¤ì–´', '##ì¡Œë‹¤', '.']\n",
      " \toffsets: [0, 4, 6, 8, 12, 14, 17, 19, 21, 23, 26, 30, 32, 35, 38, 40, 43, 45]\n",
      " \tstart_of_word: [True, True, False, True, True, False, True, False, True, False, True, True, True, True, False, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 13659, 16290, 4196, 25287, 23889, 12194, 10448, 15, 11133, 7968, 25287, 3354, 13165, 11913, 4009, 8251, 10127, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/test.tsv:   8%|â–Š         | 4000/50000 [00:03<00:37, 1230.28 Dicts/s]03/30/2021 15:31:27 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  12%|â–ˆâ–        | 6000/50000 [00:04<00:34, 1280.04 Dicts/s]03/30/2021 15:31:28 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:31:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:31:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  24%|â–ˆâ–ˆâ–       | 12000/50000 [00:09<00:28, 1319.83 Dicts/s]03/30/2021 15:31:33 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24000/50000 [00:18<00:19, 1319.35 Dicts/s]03/30/2021 15:31:41 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:31:41 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [00:19<00:18, 1319.22 Dicts/s]03/30/2021 15:31:43 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [00:24<00:13, 1320.87 Dicts/s]03/30/2021 15:31:48 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40000/50000 [00:29<00:07, 1369.33 Dicts/s]03/30/2021 15:31:53 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46000/50000 [00:34<00:03, 1315.14 Dicts/s]03/30/2021 15:31:58 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:37<00:00, 1331.56 Dicts/s]\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Cached the datasets at cache/data_silo/ac8b9ede46e0a9423d4adc9c41a611b6\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Examples in train: 133975\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Examples in dev  : 15537\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Examples in test : 49989\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   \n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     150\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 19.548318716178393\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Proportion clipped:      4.4784474715431986e-05\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = repo_path / \"nsmc\"\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"beomi/kcbert-base\"  # Reference: https://github.com/Beomi/KcBERT\n",
    "MAX_LENGTH = 150\n",
    "LABEL_LIST = [\"bad\", \"good\"]\n",
    "TRAIN_FILE = \"train.tsv\"\n",
    "TEST_FILE = \"test.tsv\"\n",
    "TASK_TYPE = \"text_classification\"\n",
    "\n",
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "\n",
    "processor = TextClassificationProcessor(\n",
    "    tokenizer=tokenizer,  # tokenizer \n",
    "    train_filename=TRAIN_FILE,  # training data íŒŒì¼ëª…\n",
    "    dev_filename=None,  # development data íŒŒì¼ëª…, ì—†ìœ¼ë©´, dev_split ë¹„ìœ¨ë§Œí¼ training dataì—ì„œ ìë¥¸ë‹¤ \n",
    "    test_filename=TEST_FILE,  # test data íŒŒì¼ëª…\n",
    "    dev_split=0.1,  # development dataë¡œ ì„¤ì •í•  ë¹„ìœ¨\n",
    "    header=0,  # csv, tsv, excel ë“± tabularí˜•íƒœ ë°ì´í„°ì—ì„œ ì²«í–‰(ë³´í†µì€ ì»¬ëŸ¼ëª…)ì˜ ìœ„ì¹˜\n",
    "    max_seq_len=MAX_LENGTH,  # ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´\n",
    "    data_dir=str(DATA_PATH),  # ë°ì´í„°ì˜ ë””ë ‰í† ë¦¬\n",
    "    label_list=LABEL_LIST,  # ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸(string í•„ìš”)\n",
    "    metric=\"acc\",  # í‰ê°€ì§€í‘œ\n",
    "    label_column_name=\"label\",  # tabularí˜•íƒœ ë°ì´í„°ì—ì„œ ë ˆì´ë¸”ì˜ ì»¬ëŸ¼ëª…\n",
    "    text_column_name=\"document\",  # tabularí˜•íƒœ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ì˜ ì»¬ëŸ¼ëª…\n",
    "    delimiter=\"\\t\"\n",
    ")\n",
    "\n",
    "\n",
    "data_silo = DataSilo(\n",
    "    processor=processor,\n",
    "    batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBqy3S68OIX9"
   },
   "source": [
    "ì½”ë“œ ì‹¤í–‰ í›„, ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ tokenization ë˜ë©°, sample ê°ì²´ì— ì €ì¥ëœë‹¤. \n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1DVPT_Rjv_SI4ggJZzqfPh0MgsMa1Q9El\" alt=\"Fine-tuning\" width=\"100%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "```plaintext\n",
    "03/28/2021 22:12:15 - INFO - farm.data_handler.processor -   \n",
    "\n",
    "      .--.        _____                       _      \n",
    "    .'_\\/_'.     / ____|                     | |     \n",
    "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
    "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
    "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
    "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
    "   (/\\||/                             |_|           \n",
    "______\\||/___________________________________________                     \n",
    "\n",
    "ID: 437-0\n",
    "Clear Text: \n",
    " \ttext_classification_label: good\n",
    " \ttext: ì´ ì˜í™”ë¥¼ ë³´ê³  ë‘í†µì´ ë‚˜ì•˜ìŠµë‹ˆë‹¤. ã…  ã… \n",
    "Tokenized: \n",
    " \ttokens: ['ì´', 'ì˜í™”ë¥¼', 'ë³´ê³ ', 'ë‘', '##í†µì´', 'ë‚˜', '##ì•˜ìŠµë‹ˆë‹¤', '.', '[UNK]', '[UNK]']\n",
    " \toffsets: [0, 2, 6, 9, 10, 13, 14, 18, 20, 22]\n",
    " \tstart_of_word: [True, True, True, True, False, True, False, False, True, True]\n",
    "Features: \n",
    " \tinput_ids: [2, 2451, 25833, 8198, 917, 11765, 587, 21809, 17, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \ttext_classification_label_ids: [1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ_tuLHRTgGb"
   },
   "source": [
    "## Modeling Layers: AdaptiveModel = LanguageModel + PredictionHead\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1OLWdr8rh7ucpF9t55gzVeMawMBJbRiEC\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **LanguageModel**ì€ pretrained language models(BERT, XLNet ...)ì˜ í‘œì¤€ í´ë˜ìŠ¤ \n",
    "* **PredictionHead**ëŠ” ëª¨ë“  down-stream tasks(NER, Text classification, QA ...)ë¥¼ í‘œì¤€ í´ë˜ìŠ¤\n",
    "* **AdaptiveModel**ì€ ìœ„ ë‘ ê°€ì§€ ëª¨ë“¤ì˜ ê²°í•©, í•˜ë‚˜ì˜ LanguageModelê³¼ ì—¬ëŸ¬ ê°œì˜ PredictionHeadë¥¼ ê²°í•©í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60M4IEtWTitY",
    "outputId": "75f046a8-1684-4821-ab5d-52d2b6b89f14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:32:47 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "03/30/2021 15:32:47 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.9974761 1.0025367]\n"
     ]
    }
   ],
   "source": [
    "# LanguageModel: Build pretrained language model\n",
    "EMBEDS_DROPOUT_PROB = 0.1\n",
    "TASK_NAME = \"text_classification\"\n",
    "\n",
    "language_model = LanguageModel.load(PRETRAINED_MODEL_NAME_OR_PATH, language=\"korean\")\n",
    "# PredictionHead: Build predictor layer\n",
    "prediction_head = TextClassificationHead(\n",
    "    num_labels=len(LABEL_LIST), \n",
    "    class_weights=data_silo.calculate_class_weights(\n",
    "        task_name=TASK_NAME\n",
    "    )\n",
    ")\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[prediction_head],\n",
    "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
    "    lm_output_types=[\"per_sequence\"],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeu-iW36mhhF"
   },
   "source": [
    "ì‹¤ì œ ëª¨ë¸ì˜ êµ¬ì„±ì„ ì‚´í´ë³´ë©´ classificationì„ ìœ„í•œ bertì™€ ìœ ì‚¬í•˜ê²Œ `PredictionHead`ì—ì„œëŠ” `pooler`ì—ì„œ ë‚˜ì˜¨ `pooled_output`ì„ `dropout`ì¸µì„ í†µê³¼í•œ í›„ì— `FeedForwardBlock`ìœ¼ë¡œ ë³´ë‚´ì„œ ìµœì¢… logitsì„ ìƒì„±í•œë‹¤. `AdaptiveModel` classì—ì„œ `embeds_dropout_prob`ë¥¼ ë°”ê¾¸ë©´, dropout í™•ë¥ ì„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhVOJTGugWlT",
    "outputId": "8eb7db6a-b858-4c9b-c26f-298b34bbcf61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <class 'farm.modeling.adaptive_model.AdaptiveModel'>\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: embeddings\n",
      "--------------------------------------------------------\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(300, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: encoder\n",
      "--------------------------------------------------------\n",
      "Showing last layer\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: feed_forward\n",
      "--------------------------------------------------------\n",
      "FeedForwardBlock(\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: loss_fct\n",
      "--------------------------------------------------------\n",
      "CrossEntropyLoss()\n",
      "Last Dropout Layer\n",
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {type(model)}\")\n",
    "for k, v in model.named_children():\n",
    "    for k1, v1 in v.named_children():\n",
    "        \n",
    "        for k2, v2 in v1.named_children():\n",
    "            print(\"----------------------------\"*2)\n",
    "            print(f\"Module: {k} | Layer: {k2}\")\n",
    "            print(\"----------------------------\"*2)\n",
    "            if k2 == \"encoder\":\n",
    "                print(\"Showing last layer\")\n",
    "                print(list(v2.children())[0][-1])\n",
    "                break\n",
    "            else:\n",
    "                print(v2)\n",
    "\n",
    "print(\"Last Dropout Layer\")\n",
    "print(model.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07u1dn-vc64p"
   },
   "source": [
    "ì‹¤ì œë¡œ transformers íŒ¨í‚¤ì§€ì™€ ë¹„êµí•´ë³´ë©´ ë¹„ìŠ·í•˜ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmMLVGnpTqxB",
    "outputId": "5b99cb1e-a812-4ca6-a839-4f4315365330"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Module: dropout\n",
      "--------------------------------------------------------\n",
      "Dropout(p=0.1, inplace=False)\n",
      "--------------------------------------------------------\n",
      "Module: classifer\n",
      "--------------------------------------------------------\n",
      "Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "bert = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: dropout\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.dropout)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: classifer\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbvskPXaTlGp"
   },
   "source": [
    "## Train & Eval & Inference\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1bD54igqAn7T96gDCFZ2uxzFHpZIL5GOh\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4JEzzR4nZej"
   },
   "source": [
    "### Train & Eval\n",
    "\n",
    "> ğŸ’¡ **TIP:** ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ì§€ë§Œ í™•ì¸í•˜ê³  ë©ˆì¶”ì„¸ìš”! ì•„ë˜ì—ì„œ ë¯¸ë¦¬ í›ˆë ¨ëœ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IuqGruw3mzm2",
    "outputId": "d3c89964-46e0-4344-f379-82c4125ac6d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:33:12 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 2e-05}'\n",
      "03/30/2021 15:33:13 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "03/30/2021 15:33:13 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 1674.7, 'num_training_steps': 16747}'\n",
      "03/30/2021 15:33:14 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.0000):   0%|          | 0/16747 [00:00<?, ?it/s]03/30/2021 15:33:15 - INFO - farm.train -   Saving a train checkpoint ...\n",
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "03/30/2021 15:33:22 - INFO - farm.train -   Saved a training checkpoint after epoch_0_step_0\n",
      "Train epoch 0/0 (Cur. train loss: 0.7284):   0%|          | 20/16747 [00:12<1:08:06,  4.09it/s]\n",
      "Evaluating:   0%|          | 0/1943 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   8%|â–Š         | 146/1943 [00:10<02:03, 14.58it/s]\u001b[A\n",
      "Evaluating:  15%|â–ˆâ–Œ        | 292/1943 [00:20<01:53, 14.51it/s]\u001b[A\n",
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 436/1943 [00:30<01:44, 14.40it/s]\u001b[A\n",
      "Evaluating:  30%|â–ˆâ–ˆâ–‰       | 578/1943 [00:40<01:35, 14.26it/s]\u001b[A\n",
      "Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 718/1943 [00:50<01:26, 14.09it/s]\u001b[A\n",
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 856/1943 [01:00<01:18, 13.90it/s]\u001b[A\n",
      "Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 991/1943 [01:11<01:09, 13.70it/s]\u001b[A\n",
      "Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1124/1943 [01:21<01:00, 13.49it/s]\u001b[A\n",
      "Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1256/1943 [01:31<00:51, 13.39it/s]\u001b[A\n",
      "Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1390/1943 [01:41<00:41, 13.39it/s]\u001b[A\n",
      "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1525/1943 [01:51<00:31, 13.41it/s]\u001b[A\n",
      "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1661/1943 [02:01<00:20, 13.44it/s]\u001b[A\n",
      "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1797/1943 [02:11<00:10, 13.46it/s]\u001b[A\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1943/1943 [02:22<00:00, 13.65it/s]\n",
      "03/30/2021 15:35:50 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 20 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "03/30/2021 15:35:50 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "03/30/2021 15:35:50 - INFO - farm.eval -   loss: 0.7042724797185066\n",
      "03/30/2021 15:35:50 - INFO - farm.eval -   task_name: text_classification\n",
      "03/30/2021 15:35:51 - INFO - farm.eval -   acc: 0.48574370856664734\n",
      "03/30/2021 15:35:51 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bad     0.4857    0.4411    0.4623      7788\n",
      "        good     0.4858    0.5306    0.5072      7749\n",
      "\n",
      "    accuracy                         0.4857     15537\n",
      "   macro avg     0.4857    0.4859    0.4848     15537\n",
      "weighted avg     0.4857    0.4857    0.4847     15537\n",
      "\n",
      "03/30/2021 15:35:51 - INFO - farm.train -   Saving current best model to ckpt/NSMC, eval=0.48574370856664734\n",
      "Train epoch 0/0 (Cur. train loss: 0.6686):   0%|          | 40/16747 [02:43<1:22:27,  3.38it/s]\n",
      "Evaluating:   0%|          | 0/1943 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c4f2cdf89f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# now train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m                         )\n\u001b[1;32m    317\u001b[0m                         \u001b[0mevalnr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                         \u001b[0mevaluator_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dev\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, model, return_preds_and_labels)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mlosses_per_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_to_loss_per_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_to_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/modeling/adaptive_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;31m# Run forward pass of language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# Run forward pass of (multiple) prediction heads using the output from above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/modeling/adaptive_model.py\u001b[0m in \u001b[0;36mforward_lm\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;31m# Run forward pass of language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextraction_layer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# get output from an earlier layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/modeling/language_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, segment_ids, padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         )\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_hidden_states\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m                 )\n\u001b[1;32m    484\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     \u001b[0mtens_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_overrides.py\u001b[0m in \u001b[0;36mhas_torch_function\u001b[0;34m(relevant_args)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0mimplementations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \"\"\"\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__torch_function__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_overridable_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "N_EPOCHS = 1\n",
    "N_GPU = 1\n",
    "checkpoint_path = Path(\"./ckpt/NSMC\")\n",
    "\n",
    "# Initialize Optimizer\n",
    "model, optimizer, lr_schedule = initialize_optimizer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_batches=len(data_silo.loaders[\"train\"]),\n",
    "    n_epochs=N_EPOCHS\n",
    ")\n",
    "# EarlyStopping\n",
    "earlymetric = \"f1\" if TASK_NAME == \"question_answering\" else \"acc\" \n",
    "mode = \"max\" if TASK_NAME in [\"text_classification\", \"question_answering\"] else \"min\"\n",
    "earlystop = EarlyStopping(\n",
    "    save_dir=checkpoint_path,\n",
    "    metric=earlymetric,\n",
    "    mode=mode,\n",
    "    patience=3,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_schedule=lr_schedule,\n",
    "    data_silo=data_silo,\n",
    "    early_stopping=earlystop,\n",
    "    evaluate_every=20,\n",
    "    checkpoints_to_keep=3,\n",
    "    checkpoint_root_dir=checkpoint_path,\n",
    "    checkpoint_every=200,\n",
    "    epochs=N_EPOCHS,\n",
    "    n_gpu=N_GPU,\n",
    "    device=device, \n",
    ")\n",
    "# now train!\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vaa778Dsc64q"
   },
   "source": [
    "í›ˆë ¨ ê³¼ì •ì— ê³„ì† Logê°€ ì°íˆê³ , Processorë‹¨ê³„ì—ì„œ ì…ë ¥í•´ë‘” `test_filename`ë¡œ ìµœì¢…í‰ê°€ë„ í•´ì¤€ë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì€ 430ê°œì˜ ë°°ì¹˜ ë°ì´í„°(batch_size=256)ë¥¼ ëŒë ¸ì„ ë•Œ earlystoppingí•œ ê²°ê³¼ë‹¤.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1m1K9CjBNulC4dzSxC1vKjLb94p9BQu26\" alt=\"Fine-tuning\" width=\"80%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vr574Y5nWEu"
   },
   "source": [
    "### Inference\n",
    "\n",
    "ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ì‹œ, í˜„ì¬ëŠ” ë‘ ê°€ì§€ í˜•íƒœë¡œ ì§€ì›í•˜ê³  ìˆë‹¤. \n",
    "\n",
    "1. **QA (FARM style)**: \n",
    "\n",
    "    ```\n",
    "    [{â€œquestionsâ€: [â€œWhat is X?â€], â€œtextâ€: â€œSome context containing the answerâ€}]\n",
    "    ```\n",
    "2. **Classification / NER / embeddings**: \n",
    "\n",
    "    ```\n",
    "    [{â€œtextâ€: â€œSome input textâ€}]\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mum_enpquaHH",
    "outputId": "8e3f627a-c843-4795-c3af-4bd125dada62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Na_JDIVkWdzYjjxK2AR5e1rJIego2-p4\n",
      "To: /content/ckpt/best_nsmc.tar\n",
      "\n",
      "\n",
      "0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "8.91MB [00:00, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "25.7MB [00:00, 63.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "36.7MB [00:00, 72.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "50.9MB [00:00, 79.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "66.6MB [00:00, 92.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "82.8MB [00:00, 106MB/s] \u001b[A\u001b[A\n",
      "\n",
      "94.9MB [00:01, 65.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "115MB [00:01, 81.7MB/s] \u001b[A\u001b[A\n",
      "\n",
      "127MB [00:01, 90.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "146MB [00:01, 107MB/s] \u001b[A\u001b[A\n",
      "\n",
      "167MB [00:01, 124MB/s]\u001b[A\u001b[A\n",
      "\n",
      "187MB [00:01, 141MB/s]\u001b[A\u001b[A\n",
      "\n",
      "212MB [00:01, 162MB/s]\u001b[A\u001b[A\n",
      "\n",
      "232MB [00:01, 171MB/s]\u001b[A\u001b[A\n",
      "\n",
      "254MB [00:01, 184MB/s]\u001b[A\u001b[A\n",
      "\n",
      "275MB [00:02, 138MB/s]\u001b[A\u001b[A\n",
      "\n",
      "300MB [00:02, 159MB/s]\u001b[A\u001b[A\n",
      "\n",
      "319MB [00:02, 143MB/s]\u001b[A\u001b[A\n",
      "\n",
      "337MB [00:02, 141MB/s]\u001b[A\u001b[A\n",
      "\n",
      "353MB [00:02, 143MB/s]\u001b[A\u001b[A\n",
      "\n",
      "371MB [00:02, 152MB/s]\u001b[A\u001b[A\n",
      "\n",
      "393MB [00:02, 166MB/s]\u001b[A\u001b[A\n",
      "\n",
      "411MB [00:03, 123MB/s]\u001b[A\u001b[A\n",
      "\n",
      "436MB [00:03, 136MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./best_nsmc/\n",
      "./best_nsmc/processor_config.json\n",
      "./best_nsmc/language_model.bin\n",
      "./best_nsmc/prediction_head_0.bin\n",
      "./best_nsmc/vocab.txt\n",
      "./best_nsmc/language_model_config.json\n",
      "./best_nsmc/tokenizer_config.json\n",
      "./best_nsmc/special_tokens_map.json\n",
      "./best_nsmc/prediction_head_0_config.json\n"
     ]
    }
   ],
   "source": [
    "# if you not trained the model please use it\n",
    "from pathlib import Path\n",
    "import gdown\n",
    "url = \"https://drive.google.com/uc?id=1Na_JDIVkWdzYjjxK2AR5e1rJIego2-p4\"\n",
    "ckpt_path = Path(\".\") / \"ckpt\"\n",
    "if not ckpt_path.exists():\n",
    "    ckpt_path.mkdir()\n",
    "gdown.download(url, str(ckpt_path / \"best_nsmc.tar\"), quiet=False)\n",
    "!tar -xvf ./ckpt/best_nsmc.tar -C ./ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeafZ3uBvdJA"
   },
   "source": [
    "ì‹¤ì œ ë„¤ì´ë²„ ì˜í™” ë‘ ê³³ì—ì„œ ê°ê¸° ë‹¤ë¥¸ í‰ì ì„ ê°€ì ¸ì™€ì„œ í…ŒìŠ¤íŠ¸ í•´ë³´ì•˜ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nyAQNWsnlUK",
    "outputId": "6c927c4b-35c9-4e8a-8c08-68feafe65628"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:37:25 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/30/2021 15:37:27 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "03/30/2021 15:37:27 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "03/30/2021 15:37:27 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "03/30/2021 15:37:27 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.9976914525032043, 1.0023192167282104]\n",
      "03/30/2021 15:37:27 - INFO - farm.modeling.prediction_head -   Loading prediction head from ckpt/best_nsmc/prediction_head_0.bin\n",
      "03/30/2021 15:37:28 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='lm_name' was already logged with value='beomi/kcbert-base' for run ID='a31a58378fda4212ae1286ea00778a7e. Attempted logging new value './ckpt/best_nsmc'.\n",
      "03/30/2021 15:37:28 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/30/2021 15:37:28 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "03/30/2021 15:37:28 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/30/2021 15:37:28 - INFO - farm.infer -   Got ya 1 parallel workers to do inference ...\n",
      "03/30/2021 15:37:28 - INFO - farm.infer -    0 \n",
      "03/30/2021 15:37:28 - INFO - farm.infer -   /w\\\n",
      "03/30/2021 15:37:28 - INFO - farm.infer -   /'\\\n",
      "03/30/2021 15:37:28 - INFO - farm.infer -   \n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/30/2021 15:37:28 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/30/2021 15:37:28 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 3-0\n",
      "Clear Text: \n",
      " \ttext: ì ˆë ˆì ˆë ˆ ëˆì£¼ê³  ë³´ì§€ë§ˆì…ˆã…‹ã…‹ã…‹ã…‹\n",
      "Tokenized: \n",
      " \ttokens: ['ì ˆ', '##ë ˆ', '##ì ˆ', '##ë ˆ', 'ëˆì£¼ê³ ', 'ë³´ì§€', '##ë§ˆ', '##ì…ˆ', '##ã…‹ã…‹ã…‹', '##ã…‹']\n",
      " \toffsets: [0, 1, 2, 3, 5, 9, 11, 12, 13, 16]\n",
      " \tstart_of_word: [True, False, False, False, True, True, False, False, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 2529, 4306, 4094, 4306, 13900, 11708, 4168, 4602, 8418, 4157, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "03/30/2021 15:37:28 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: ë¹„ì— ì –ì§€ ì•ŠëŠ” ê³ ê¸‰ ì¥ë‚œê° í…íŠ¸ì™€, ë¹„ì— ì –ë‹¤ ëª»í•´ ì ê²¨ë²„ë¦¬ëŠ” ë°˜ì§€í•˜ ê°€êµ¬\n",
      "Tokenized: \n",
      " \ttokens: ['ë¹„', '##ì—', 'ì –', '##ì§€', 'ì•ŠëŠ”', 'ê³ ê¸‰', 'ì¥ë‚œ', '##ê°', 'í…íŠ¸', '##ì™€', ',', 'ë¹„', '##ì—', 'ì –', '##ë‹¤', 'ëª»í•´', 'ì ', '##ê²¨', '##ë²„ë¦¬ëŠ”', 'ë°˜', '##ì§€', '##í•˜', 'ê°€êµ¬']\n",
      " \toffsets: [0, 1, 3, 4, 6, 9, 12, 14, 16, 18, 19, 21, 22, 24, 25, 27, 30, 31, 32, 36, 37, 38, 40]\n",
      " \tstart_of_word: [True, False, True, False, True, True, True, False, True, False, False, True, False, True, False, True, True, False, False, True, False, False, True]\n",
      "Features: \n",
      " \tinput_ids: [2, 1664, 4113, 2540, 4102, 8743, 15794, 8977, 4114, 22279, 4196, 15, 1664, 4113, 2540, 4020, 10305, 2487, 4349, 14622, 1483, 4102, 4159, 22560, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "\n",
      "\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Inferencing Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.87s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from farm.infer import Inferencer\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "# https://movie.naver.com/movie/bi/mi/basic.nhn?code=161967\n",
    "# https://movie.naver.com/movie/bi/mi/point.nhn?code=196051\n",
    "\n",
    "basic_texts = [\n",
    "    {\"text\": \"ë¹„ì— ì –ì§€ ì•ŠëŠ” ê³ ê¸‰ ì¥ë‚œê° í…íŠ¸ì™€, ë¹„ì— ì –ë‹¤ ëª»í•´ ì ê²¨ë²„ë¦¬ëŠ” ë°˜ì§€í•˜ ê°€êµ¬\"},  # 161967 / í‰ì  10\n",
    "    {\"text\": \"\"\"ë‚¨ë“¤ì´ ë‚œí•´í•˜ë‹¨ê±° ë³´ê³  í˜¼ì ì´í•´í–ˆë‹¤ë©° ì‹¬ì˜¤í•œì²™ í•˜ê³ í”Œë•Œë‚˜ ë³´ë©´ ë”±ì¸ ì˜í™”. í†µì°°ë„ ì‹œì‚¬ì ë„ ì¬ë¯¸ë„ ì˜ë¯¸ë„ ê°ë™ë„ ì—†ëŠ”... \n",
    "ì§„ì •í•œ í‚¬ë§íƒ€ì„. ê°€ë‚œí•œ ì‚¬ëŒë“¤ ë‹¤ ê¸°ìƒì¶©ì— ë¹„ìœ í•œê±°ì•¼? ê·¸ë ‡ë‹¤ë©´ ê°ë… ê°œë˜¥ì² í•™ ì™„ì „ê½ì´ê³ ...\"\"\"},  # 161967 / í‰ì  1\n",
    "    {\"text\": \"ì™€ ì´ê±° ì•ˆë³´ë©´ ì¸ìƒ ì ˆë°˜ í›„íšŒí•œê²ë‹ˆë‹¤ ì—¬ëŸ¬ë¶„\"},  # 196051 / í‰ì  10\n",
    "    {\"text\": \"ì ˆë ˆì ˆë ˆ ëˆì£¼ê³  ë³´ì§€ë§ˆì…ˆã…‹ã…‹ã…‹ã…‹\"}  # 196051 / í‰ì  1\n",
    "]\n",
    "\n",
    "infer_model = Inferencer.load(\n",
    "    model_name_or_path=\"./ckpt/best_nsmc\",\n",
    "    task_type=\"text_classification\"\n",
    ")\n",
    "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4olYJtIrryF",
    "outputId": "1eef1642-4b4c-4358-e3cd-cad6f5081e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[Movie: 161967] Context:\u001b[0m\n",
      "ë¹„ì— ì –ì§€ ì•ŠëŠ” ê³ ê¸‰ ì¥ë‚œê° í…íŠ¸ì™€, ë¹„ì— ì –ë‹¤ ëª»í•´ ì ê²¨ë²„ë¦¬ëŠ” ë°˜ì§€í•˜ ê°€êµ¬\n",
      "Probability 76.74% | Predict: \u001b[1m\u001b[31mbad\u001b[0m | Real Star: \u001b[1m\u001b[34m10\u001b[0m\n",
      "\n",
      "\u001b[1m[Movie: 161967] Context:\u001b[0m\n",
      "ë‚¨ë“¤ì´ ë‚œí•´í•˜ë‹¨ê±° ë³´ê³  í˜¼ì ì´í•´í–ˆë‹¤ë©° ì‹¬ì˜¤í•œì²™ í•˜ê³ í”Œë•Œë‚˜ ë³´ë©´ ë”±ì¸ ì˜í™”. í†µì°°ë„ ì‹œì‚¬ì ë„ ì¬ë¯¸ë„ ì˜ë¯¸ë„ ê°ë™ë„ ì—†ëŠ”... \n",
      "ì§„ì •í•œ í‚¬ë§íƒ€ì„. ê°€ë‚œí•œ ì‚¬ëŒë“¤ ë‹¤ ê¸°ìƒì¶©ì— ë¹„ìœ í•œê±°ì•¼? ê·¸ë ‡ë‹¤ë©´ ê°ë… ê°œë˜¥ì² í•™ ì™„ì „ê½ì´ê³ ...\n",
      "Probability 99.58% | Predict: \u001b[1m\u001b[31mbad\u001b[0m | Real Star: \u001b[1m\u001b[34m1\u001b[0m\n",
      "\n",
      "\u001b[1m[Movie: 196051] Context:\u001b[0m\n",
      "ì™€ ì´ê±° ì•ˆë³´ë©´ ì¸ìƒ ì ˆë°˜ í›„íšŒí•œê²ë‹ˆë‹¤ ì—¬ëŸ¬ë¶„\n",
      "Probability 77.20% | Predict: \u001b[1m\u001b[32mgood\u001b[0m | Real Star: \u001b[1m\u001b[34m10\u001b[0m\n",
      "\n",
      "\u001b[1m[Movie: 196051] Context:\u001b[0m\n",
      "ì ˆë ˆì ˆë ˆ ëˆì£¼ê³  ë³´ì§€ë§ˆì…ˆã…‹ã…‹ã…‹ã…‹\n",
      "Probability 97.12% | Predict: \u001b[1m\u001b[31mbad\u001b[0m | Real Star: \u001b[1m\u001b[34m1\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import termcolor\n",
    "\n",
    "for p, mid, star in zip(result[0][\"predictions\"], [161967, 161967, 196051, 196051], [10, 1, 10, 1]):\n",
    "    context = p[\"context\"]\n",
    "    label = p[\"label\"]\n",
    "    probability = p[\"probability\"]\n",
    "    star = termcolor.colored(str(star), \"blue\", attrs=[\"bold\"])\n",
    "    if label == \"bad\":\n",
    "        label = termcolor.colored(label, \"red\", attrs=[\"bold\"])\n",
    "    else:\n",
    "        label = termcolor.colored(label, \"green\", attrs=[\"bold\"])\n",
    "    print(termcolor.colored(f\"[Movie: {mid}] Context:\", attrs=[\"bold\"]))\n",
    "    print(context)\n",
    "    print(f\"Probability {probability*100:.2f}% | Predict: {label} | Real Star: {star}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‘ ì˜í™”ëŠ” ë´‰ì¤€í˜¸ ê°ë…ë‹˜ì˜ 'ê¸°ìƒì¶©(id=161967)', ìµœê·¼ ì¸ê¸° ê°€ë„ë¥¼ ë‹¬ë¦¬ê³  ìˆëŠ” 'ê·¹ì¥íŒ ê·€ë©¸ì˜ ì¹¼ë‚ : ë¬´í•œì—´ì°¨í¸(id=196051)'ë¥¼ ì„ ì •í–ˆë‹¤. í•˜ë‚˜ë¥¼ ì œì™¸í•˜ê³  ì˜ ë§ì¶˜ ëª¨ìŠµì„ ë³´ì—¬ì¤¬ëŠ”ë°, ì²«ë²ˆì§¸ ìƒ˜í”Œì˜ ê²½ìš° ì‚¬ì‹¤ ì˜í™”ì˜ ì¥ë©´ì„ ë¬˜ì‚¬í•œ ê²ƒìœ¼ë¡œ, ê·¸ë§Œí¼ ì¸ìƒê¹Šì—ˆë˜ ì¥ë©´ë“¤ì„ ë‹¬ë©´ì„œ í‰ì ì€ 10ì ìœ¼ë¡œ ë‹¬ì•˜ë‹¤. ì‚¬ëŒìœ¼ë¡œì¨ ì´ ì˜í™”ì„ ë³¸ ê´€ê°ì´ë¼ë©´ ì´ í‰ê°€ê°€ 10ì ì— ì•Œë§ëŠ” í‰ì (í˜¹ì€ ê¸ì •)ì´ì§€ë§Œ, ê¸°ê³„ì—ê²ŒëŠ” ì•„ì§ ì–´ë ¤ìš´ ì  ì¤‘ì— í•˜ë‚˜ë¼ê³  ìƒê°í•œë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "667ZCa2Hc64r"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WlcEEWGyVEZ"
   },
   "source": [
    "# MLflow\n",
    "\n",
    "MLflowë¥¼ ì´ìš©í•˜ ë¹ ë¥´ê³  ì‰½ê²Œ ì‹¤í—˜ì„ ê´€ë¦¬í•˜ê³ , ê´€ë ¨ í‰ê°€ì§€í‘œë„ í•¨ê»˜ ë³¼ ìˆ˜ ìˆë‹¤. ë‹¤ìŒ ê·¸ë¦¼ë“¤ì€ TITAN RTX 4ëŒ€ì—ì„œ ë°°ì¹˜í¬ê¸°ë¥¼ 256ìœ¼ë¡œ í›ˆë ¨ ì‹œí‚¨ ê²°ê³¼ë‹¤(440 batches ì—ì„œ Early Stoppingí–ˆë‹¤.).\n",
    "\n",
    "* public mlflow([ë§í¬](https://public-mlflow.deepset.ai/#/experiments/313/runs/05e7e3d4945642f9ab3e296637d57c26))ì—ì„œ í™•ì¸í•˜ê¸°\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=13Cg8eziHBgA3JLwZJ3Bo8YzeySWPRmiP\" alt=\"Fine-tuning\" width=\"30%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Trainê³¼ Dev ì„¸íŠ¸ì˜ lossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1cpFWVvjkSqshvN0hS_CuPk4RjyEyM0AV\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Dev ì„¸íŠ¸ì˜ ì •í™•ë„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1VPso9Gx60V8_dgE4as054n7kymCoQ9w5\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA3QoKnvTz17"
   },
   "source": [
    "# TASK Supported\n",
    "\n",
    "|Task|BERT|RoBERTa*|XLNet|ALBERT|DistilBERT|XLMRoBERTa|ELECTRA|MiniLM|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|Text classification|x|x|x|x|x|x|x|x|\n",
    "|NER|x|x|x|x|x|x|x|x|\n",
    "|Question Answering|x|x|x|x|x|x|x|x|\n",
    "|Language Model Fine-tuning|x||||||||\n",
    "|Text Regression|x|x|x|x|x|x|x|x|\n",
    "|Multilabel Text classif.|x|x|x|x|x|x|x|x|\n",
    "|Extracting embeddings|x|x|x|x|x|x|x|x|\n",
    "|LM from scratch|x||||||||\n",
    "|Text Pair Classification|x|x|x|x|x|x|x|x|\n",
    "|Passage Ranking|x|x|x|x|x|x|x|x|\n",
    "|Document retrieval (DPR)|x|x||x|x|x|x|x|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhTAYSY1t4Sr"
   },
   "source": [
    "# Compare to others\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1TZoRpza8-o4wSTr0s16f8hHQRroLQg30\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµí•´ë³´ë©´ FARMì€ ì¡°ê¸ˆ ë” huggingfaceì™€ pytorch-lightningì˜ í•©ë³¸ ì¶•ì•½ ë²„ì „ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. ë§ˆì¹˜ Tensorflow v1ê³¼ kerasì˜ ì°¨ì´ ëŠë‚Œì´ë‹¤.\n",
    "\n",
    "## FARM ì¥ë‹¨ì \n",
    "\n",
    "ì¥ì :\n",
    "\n",
    "* ë°ì´í„° ì„¸íŠ¸ë§Œ ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë©´, ë‹¤ë¥¸ íŒ¨í‚¤ì§€ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì„¤ì • í•  ê²ƒì´ ì ìŒ\n",
    "* í›ˆë ¨ ì†ë„ê°€ ë¹ ë¥´ê³ , ì‹¤í—˜ ê¸°ë¡ ë° ê´€ë¦¬ì´ í¸ë¦¬í•´ì„œ ë¹ ë¥´ê²Œ ì‹¤í—˜í•´ ë³¼ ìˆ˜ ìˆìŒ(í…ì„œë³´ë“œ ëŒ€ì‹  mlflow ì‚¬ìš© ê°€ëŠ¥)\n",
    "* ë©€í‹° GPU ì„¤ì •ì„ í•´ì¤„ í•„ìš”ê°€ ì—†ìŒ\n",
    "\n",
    "ë‹¨ì : \n",
    "\n",
    "* customizationì´ ìƒëŒ€ì ìœ¼ë¡œ í˜ë“¦\n",
    "* ì•„ì§ ë°œì „ ì¤‘ì´ë¼ ë¶ˆì•ˆì •í•˜ê³  documentatonì´ ì˜ ì•ˆë˜ì–´ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FARM_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
