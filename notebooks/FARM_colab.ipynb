{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need to run this cell\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/ColabNotebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Colab User \n",
    "\n",
    "* set `runtime=GPU` then start to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWSR3a5vxSGL"
   },
   "outputs": [],
   "source": [
    "# For Colab: Install FARM\n",
    "!pip install torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install farm==0.5.0\n",
    "!pip install -U -q emoji soynlp\n",
    "!git clone https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iM4H-pbDL_XC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "def read_data(path:str, header=None):\n",
    "    return pd.read_csv(path, sep='\\t', header=header)\n",
    "\n",
    "def clean(x):\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "    url_pattern = re.compile(\n",
    "        r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    \n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x\n",
    "\n",
    "def preprocess_dataframe(df:pd.DataFrame):\n",
    "    r\"\"\"\n",
    "    Changed the code\n",
    "    source from: https://colab.research.google.com/drive/1IPkZo1Wd-DghIOK6gJpcb0Dv4_Gv2kXB\n",
    "    \"\"\"\n",
    "\n",
    "    label_dict = {0:\"bad\", 1:\"good\"}\n",
    "    df['document'] = df['document'].apply(lambda x: clean(str(x)))\n",
    "    df['label'] = df['label'].apply(label_dict.get)\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_dataframe(read_data(\"./nsmc/ratings_train.txt\", header=0))\n",
    "df_test = preprocess_dataframe(read_data(\"./nsmc/ratings_test.txt\", header=0))\n",
    "df_train.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/train.tsv\", sep=\"\\t\", index=False)\n",
    "df_test.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/test.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-PcIhBaB3-E",
    "outputId": "158cb1c2-8b2e-42b3-9c83-257c607fa318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\t\t  ratings_train.txt  raw\tsynopses.json  train.tsv\n",
      "ratings_test.txt  ratings.txt\t     README.md\ttest.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeTtCfRkyxVJ",
    "outputId": "8fb8c67c-7c93-45ec-85f9-e7e60ace7a90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 09:45:11 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __          __  _                            _        \n",
      " \\ \\        / / | |                          | |       \n",
      "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
      "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
      "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
      "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
      "  ______      _____  __  __  \n",
      " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
      " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
      " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
      " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
      " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
      "                                     |    |==|==|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      " \n",
      "Devices available: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import TextClassificationHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.train import Trainer\n",
    "from farm.utils import MLFlowLogger\n",
    "\n",
    "repo_path = Path() # Path().absolute().parent\n",
    "sys.path.append(str(repo_path))\n",
    "\n",
    "# Change to your experiment name and run name\n",
    "EXP_NAME = \"FARM_tutorial\"\n",
    "RUN_NAME = \"NSMC_colab\"\n",
    "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
    "ml_logger.init_experiment(experiment_name=EXP_NAME, run_name=RUN_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Devices available: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS3WaV8YMJmh"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1hbtUClFoXg45IbViZoFRLnnDGVlr9Dlb\" alt=\"Fine-tuning\" width=\"30%\" height=\"30%\"></center>\n",
    "\n",
    "# FARM\n",
    "\n",
    "> Framework for Adapting Representation Models\n",
    "\n",
    "이 패키지를 한 마디로 요약하면 Fine-tuning에 최적화된 도구다.\n",
    "\n",
    "최근의 자연어처리 분야는 Transformer와 그 변형의 등장으로 인해, 보통 2단계로 나눠서 학습이 진행된다. \n",
    "1. **Pretrained Language Modeling**\n",
    "\n",
    "   대량의 텍스트 데이터를 이용해 비지도학습(unsupervised learning)으로 언어 모델링은 진행한다. 언어 모델링이란 인간의 언어를 컴퓨터로 모델링하는 과정이다. 쉽게 말하면, 모델에게 단어들을 입력했을 때, 제일 말이 되는 단어(토큰)을 뱉어내게 하는 것이다. 과거에는 단어(토큰)의 순서가 중요했었다. 즉, 일정 단어들의 시퀀스 $x_{1:t-1}$가 주어지면, $t$번째 단어인 $x_t$를 잘 학습시키는 것이었다. 이를 Auto Regressive Modeling이라고도 한다. 그러나, Masked Language Modeling 방법이 등장했는데, 이는 랜덤으로 맞춰야할 단어를 가린 다음에 가려진 단어 $x_{mask}$가 포함된 시퀀스 $x_{1:t}$ 를 모델에게 입력하여 맞추는 학습 방법이다. 이러한 방법이 좋은 성과를 거두면서, 최근에는 모든 \n",
    "2. **Fine-tuning**\n",
    "\n",
    "    PLM(Pretrained Language Model)을 만들고 나면, 각기 다른 downstream task에 따라서 fine-tuning을 하게 된다. Downstream task은 구체적으로 풀고 싶은 문제를 말하며, 주로 다음과 같은 문제들이다.\n",
    "    * 텍스트 분류 Text Classification - 예시: 영화 댓글 긍정/부정 분류하기\n",
    "    * 개체명인식 NER(Named Entity Recognition) - 예시: 특정 기관명, 인명 및 시간 날짜 등 토큰에 알맞는 태그로 분류하기\n",
    "    * 질의응답 Question and Answering - 예시: 특정 지문과 질의(query)가 주어지면 대답하기\n",
    "\n",
    "오늘 소개할 FARM 패키지는 2번째 단계인 Fine-tuning을 보다 손쉽게 만들어놓은 패키지다. Github에서 Colab tutorial과 함께 보면 좋다.\n",
    "\n",
    "- **Tutorial github:** https://github.com/simonjisu/FARM_tutorial\n",
    "- **Colab Tutorial:** [링크](https://colab.research.google.com/github/simonjisu/FARM_tutorial/blob/main/notebooks/FARM_colab.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PkdHkjZoR4A"
   },
   "source": [
    "## Core Features\n",
    "\n",
    "- **Easy fine-tuning of language models** to your task and domain language\n",
    "- **Speed**: AMP(Automatic Mixed Precision) optimizers (~35% faster) and parallel preprocessing (16 CPU cores => ~16x faster)\n",
    "- **Modular design** of language models and prediction heads\n",
    "- Switch between heads or combine them for **multitask learning**\n",
    "- **Full Compatibility** with HuggingFace Transformers' models and model hub\n",
    "- **Smooth upgrading** to newer language models\n",
    "- Integration of **custom datasets** via Processor class\n",
    "- Powerful **experiment tracking** & execution\n",
    "- **Checkpointing & Caching** to resume training and reduce costs with spot instances\n",
    "- Simple **deployment** and **visualization** to showcase your model\n",
    "\n",
    "<details>\n",
    "<summary> 👉 what is AMP? </summary>\n",
    "\n",
    "**Reference**\n",
    "- https://github.com/NVIDIA/apex\n",
    "- https://forums.fast.ai/t/mixed-precision-training/20720\n",
    "\n",
    "**mixed precision training이란**\n",
    "- 처리 속도를 높이기 위한 FP16(16bit floating point)연산과 정확도 유지를 위한 FP32 연산을 섞어 학습하는 방법\n",
    "- Tensor Core를 활용한 FP16연산을 이용하면 FP32연산 대비 절반의 메모리 사용량과 8배의 연산 처리량 & 2배의 메모리 처리량 효과가 있다\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjzUlWjDobxD"
   },
   "source": [
    "# NSMC 데이터로 FARM 알아보기\n",
    "\n",
    "## NSMC 데이터\n",
    "\n",
    "NSMC(Naver Sentiment Movie Corpus)는 한국어로 된 영화 댓글 데이터 세트다. 해당 Task는 타겟 값이 긍정(1)/부정(0)이 되는 Binary Text Classification 문제로 볼 수 있다. https://github.com/e9t/nsmc 에서 받을 수 있다.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1FIGIBtZxtuKD5Prps5vPOPldBb0xHwzH\" alt=\"Fine-tuning\" width=\"50%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP4UcaeKoVM8"
   },
   "source": [
    "## Fine-tuning Process\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1j9pn8Lpg7sy6S8Ubvq3E7JLWf28KvRt4\" alt=\"Fine-tuning\" width=\"50%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Fine-tuning Process는 위 그림과 같이 진행된다.\n",
    "\n",
    "* Load Data: 데이터를 알맞는 형식(json, csv 등)으로 불러온다.\n",
    "* Create Dataset: 데이터세트(Dataset) 만들기\n",
    "    * Tokenization: 텍스트를 토큰으로 나누고, 단어장(vocab)을 생성한다.\n",
    "    * ToTensor: vocab에 해당하는 단어를 수치화하는 과정 (`input_ids` in transformers)\n",
    "    * Attention Mask: 패딩계산을 피하기 위해 Attention 해야할 토큰만 masking(`attention_mask` in transformers)\n",
    "* Create Dataloader: 훈련, 평가시 배치크기 단위로 데이터를 불러오는 객체\n",
    "* Create Model:\n",
    "    * Pretrained Language Model: 대량의 텍스트 데이터로 사전에 훈련된 모델 \n",
    "$$\\underset{\\theta}{\\arg \\max} P(x_{mask} \\vert x_{1:t})$$\n",
    "    * Fine-tuninig Layer: Downstream Task에 맞춰서 학습한다.       \n",
    "$$\\underset{\\theta}{\\arg \\max}P(y\\vert x_{1:t})$$\n",
    "        예를 들어, 영화 긍정/부정 분류 문제의 경우\n",
    "$$\\underset{\\theta}{\\arg \\max} P(y=\\text{긍정/부정} \\vert x_{1:t})$$\n",
    "* Train Model: 모델 훈련\n",
    "* Eval Model: 모델 평가\n",
    "* Inference: 모델 서비스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsQKHSPlMovO"
   },
   "source": [
    "## Processor & Data Silo\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1XCc0AJpPBMFcC81NW0A6w0mpswZ2KU7h\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **Processor**는 file 혹은 request를 PyTorch Datset로 만들어 주는 역할이다. 자세한 인자값은 다음 코드 블록에서 설명한다.\n",
    "* **Data Silo**는 train, dev, test sets를 관리하고, Processor의 function들 이용해 각 set를 DataLoader로 변환한다.\n",
    "* **Processor**는 각 데이터를 처리할 때, **Samples**, **SampleBasket**에 담게 되는데, 이들은 raw document를 관리하는 객체이며 tokenized, features등 데이터를 저장하고 있다. 이렇게 하는 이유는 하나의 소스 텍스트(raw text)에서 여러개의 샘플을 생성할 수도 있기 때문이다(e.g. QA task)\n",
    "    ```python\n",
    "    def dataset_from_dicts(self, ...)\n",
    "        # ...\n",
    "        for dictionary, input_ids, segment_ids, padding_mask, tokens in zip(\n",
    "                dicts, input_ids_batch, segment_ids_batch, padding_masks_batch, tokens_batch\n",
    "        ):\n",
    "            # ...\n",
    "            # Add Basket to self.baskets\n",
    "            curr_sample = Sample(\n",
    "                id=None,\n",
    "                clear_text=dictionary,\n",
    "                tokenized=tokenized,\n",
    "                features=[feat_dict]\n",
    "            )\n",
    "            curr_basket = SampleBasket(\n",
    "                id_internal=None,\n",
    "                raw=dictionary,\n",
    "                id_external=None,\n",
    "                samples=[curr_sample]\n",
    "               )\n",
    "            self.baskets.append(curr_basket)\n",
    "\n",
    "        # ...\n",
    "    ```\n",
    "\n",
    "사용하는 방법은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuQEtIvNTI5V",
    "outputId": "57da6862-931f-4490-b86a-e7700da2b698"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 09:45:12 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/29/2021 09:45:14 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "03/29/2021 09:45:14 - INFO - farm.data_handler.data_silo -   Loading train set from: nsmc/train.tsv \n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 149539 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   /|\\\n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   /'\\\n",
      "03/29/2021 09:45:15 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/train.tsv:   0%|          | 0/149539 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/29/2021 09:45:17 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/29/2021 09:45:17 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1213-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: 그림체는 좋다. 그렇다 그뿐이다 내용 전개도 억지스럽고 설명 부족에 뿌린 떡밥은 많으나 치워지지 않는 떡밥 사실 이 평점 1점도 이 글을 쓰기위함이지 사실 점수가 된다면 -무한대를 주고 싶을만큼 졸작이자 쓰래기이다. 개인적인 의견이지만..\n",
      "Tokenized: \n",
      " \ttokens: ['그림', '##체', '##는', '좋다', '.', '그렇다', '그', '##뿐이다', '내용', '전', '##개도', '억지', '##스럽고', '설명', '부족', '##에', '뿌린', '떡', '##밥', '##은', '많으', '##나', '치워', '##지지', '않는', '떡', '##밥', '사실', '이', '평', '##점', '1', '##점도', '이', '글을', '쓰기', '##위', '##함이', '##지', '사실', '점수', '##가', '된다면', '-', '무한', '##대를', '주고', '싶을', '##만큼', '졸', '##작', '##이자', '쓰래기', '##이다', '.', '개인적인', '의견이', '##지만', '.', '.']\n",
      " \toffsets: [0, 2, 3, 5, 7, 9, 13, 14, 18, 21, 22, 25, 27, 31, 34, 36, 38, 41, 42, 43, 45, 47, 49, 51, 54, 57, 58, 60, 63, 65, 66, 68, 69, 72, 74, 77, 79, 80, 82, 84, 87, 89, 91, 95, 96, 98, 101, 104, 106, 109, 110, 111, 114, 117, 119, 121, 126, 129, 131, 132]\n",
      " \tstart_of_word: [True, False, False, True, False, True, True, False, True, True, False, True, False, True, True, False, True, True, False, False, True, False, True, False, True, True, False, True, True, True, False, True, False, True, True, True, False, False, False, True, True, False, True, True, False, False, True, True, False, True, False, False, True, False, False, True, True, False, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 13447, 4230, 4008, 10350, 17, 12120, 391, 12678, 9132, 2525, 25289, 12676, 13937, 10415, 9633, 4113, 19542, 1023, 4578, 4057, 20847, 4136, 19129, 8840, 8743, 1023, 4578, 8220, 2451, 3288, 4213, 20, 24896, 2451, 12342, 25912, 4069, 11467, 4102, 8220, 25899, 4009, 17524, 16, 14244, 10633, 9568, 22519, 8801, 2579, 4240, 10575, 15893, 7976, 17, 20485, 21100, 8015, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/29/2021 09:45:17 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 214-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: good\n",
      " \ttext: 강부자 여동생 대사와 연기에 넋을 잃고 봤네\n",
      "Tokenized: \n",
      " \ttokens: ['강', '##부자', '여', '##동생', '대사', '##와', '연기', '##에', '넋', '##을', '잃고', '봤', '##네']\n",
      " \toffsets: [0, 1, 4, 5, 8, 10, 12, 14, 16, 17, 19, 22, 23]\n",
      " \tstart_of_word: [True, False, True, False, True, False, True, False, True, False, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 213, 27729, 2269, 16247, 23805, 4196, 11219, 4113, 636, 4027, 18629, 1594, 4011, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/train.tsv:   5%|▌         | 8000/149539 [00:07<02:20, 1009.24 Dicts/s]03/29/2021 09:45:23 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  11%|█         | 16000/149539 [00:15<02:07, 1048.69 Dicts/s]03/29/2021 09:45:32 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:45:32 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  17%|█▋        | 26000/149539 [00:25<01:59, 1035.47 Dicts/s]03/29/2021 09:45:41 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:45:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:45:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  19%|█▊        | 28000/149539 [00:27<01:58, 1026.38 Dicts/s]03/29/2021 09:45:44 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  21%|██▏       | 32000/149539 [00:31<01:54, 1030.38 Dicts/s]03/29/2021 09:45:47 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  28%|██▊       | 42000/149539 [00:40<01:45, 1019.58 Dicts/s]03/29/2021 09:45:57 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  32%|███▏      | 48000/149539 [00:46<01:36, 1051.39 Dicts/s]03/29/2021 09:46:04 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  35%|███▍      | 52000/149539 [00:50<01:33, 1046.30 Dicts/s]03/29/2021 09:46:06 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  45%|████▌     | 68000/149539 [01:05<01:19, 1029.91 Dicts/s]03/29/2021 09:46:21 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  47%|████▋     | 70000/149539 [01:07<01:16, 1035.14 Dicts/s]03/29/2021 09:46:24 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  49%|████▉     | 74000/149539 [01:11<01:13, 1031.48 Dicts/s]03/29/2021 09:46:27 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:46:28 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:46:28 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  51%|█████     | 76000/149539 [01:13<01:10, 1045.74 Dicts/s]03/29/2021 09:46:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  55%|█████▍    | 82000/149539 [01:19<01:05, 1035.73 Dicts/s]03/29/2021 09:46:35 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  68%|██████▊   | 102000/149539 [01:38<00:44, 1058.87 Dicts/s]03/29/2021 09:46:54 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:46:54 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  70%|██████▉   | 104000/149539 [01:40<00:43, 1051.16 Dicts/s]03/29/2021 09:46:56 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  75%|███████▍  | 112000/149539 [01:47<00:35, 1053.11 Dicts/s]03/29/2021 09:47:04 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:47:05 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  86%|████████▌ | 128000/149539 [02:03<00:20, 1034.29 Dicts/s]03/29/2021 09:47:20 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  91%|█████████ | 136000/149539 [02:11<00:13, 1036.30 Dicts/s]03/29/2021 09:47:27 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  92%|█████████▏| 138000/149539 [02:13<00:11, 1013.23 Dicts/s]03/29/2021 09:47:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  96%|█████████▋| 144000/149539 [02:18<00:05, 1027.34 Dicts/s]03/29/2021 09:47:36 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv: 100%|██████████| 149539/149539 [02:23<00:00, 1039.25 Dicts/s]\n",
      "03/29/2021 09:47:39 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
      "03/29/2021 09:47:39 - INFO - farm.data_handler.data_silo -   Took 15536 samples out of train set to create dev set (dev split is roughly 0.1)\n",
      "03/29/2021 09:47:39 - INFO - farm.data_handler.data_silo -   Loading test set from: nsmc/test.tsv\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 50000 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   /w\\\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   / \\\n",
      "03/29/2021 09:47:40 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/test.tsv:   0%|          | 0/50000 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/29/2021 09:47:42 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/29/2021 09:47:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 71-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: 미친놈들 집합소네 연출력 빵점, 스토리 빵점. 구성빵점\n",
      "Tokenized: \n",
      " \ttokens: ['미친', '##놈들', '집합소', '##네', '연출', '##력', '빵', '##점', ',', '스토', '##리', '빵', '##점', '.', '구성', '##빵', '##점']\n",
      " \toffsets: [0, 2, 5, 8, 10, 12, 14, 15, 16, 18, 20, 22, 23, 24, 26, 28, 29]\n",
      " \tstart_of_word: [True, False, True, False, True, False, True, False, False, True, False, True, False, False, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 8257, 8423, 20933, 4011, 23889, 4286, 1692, 4213, 15, 17319, 4038, 1692, 4213, 17, 17474, 4555, 4213, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/29/2021 09:47:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 767-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: 그냥 답답하다. 영화보는 내내 우울한 분위기 ...\n",
      "Tokenized: \n",
      " \ttokens: ['그냥', '답답하다', '.', '영화', '##보는', '내내', '우울', '##한', '분위기', '.', '.', '.']\n",
      " \toffsets: [0, 3, 7, 9, 11, 14, 17, 19, 21, 25, 26, 27]\n",
      " \tstart_of_word: [True, True, False, True, False, True, True, False, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 8019, 13234, 17, 9376, 9136, 16714, 15343, 4047, 12655, 17, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/test.tsv:   8%|▊         | 4000/50000 [00:04<00:47, 969.59 Dicts/s]03/29/2021 09:47:45 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  12%|█▏        | 6000/50000 [00:05<00:44, 994.10 Dicts/s]03/29/2021 09:47:46 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:47:47 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:47:48 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  24%|██▍       | 12000/50000 [00:11<00:36, 1044.20 Dicts/s]03/29/2021 09:47:52 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  48%|████▊     | 24000/50000 [00:23<00:25, 1029.36 Dicts/s]03/29/2021 09:48:03 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/29/2021 09:48:04 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  52%|█████▏    | 26000/50000 [00:25<00:23, 1026.97 Dicts/s]03/29/2021 09:48:06 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  64%|██████▍   | 32000/50000 [00:30<00:17, 1030.19 Dicts/s]03/29/2021 09:48:12 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  80%|████████  | 40000/50000 [00:38<00:09, 1049.44 Dicts/s]03/29/2021 09:48:19 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  92%|█████████▏| 46000/50000 [00:44<00:03, 1020.58 Dicts/s]03/29/2021 09:48:25 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv: 100%|██████████| 50000/50000 [00:48<00:00, 1031.86 Dicts/s]\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Cached the datasets at cache/data_silo/ac8b9ede46e0a9423d4adc9c41a611b6\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Examples in train: 133976\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Examples in dev  : 15536\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Examples in test : 49989\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   \n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     150\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 19.57721532214725\n",
      "03/29/2021 09:48:30 - INFO - farm.data_handler.data_silo -   Proportion clipped:      4.478414044306443e-05\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = repo_path / \"nsmc\"\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"beomi/kcbert-base\"  # Reference: https://github.com/Beomi/KcBERT\n",
    "MAX_LENGTH = 150\n",
    "LABEL_LIST = [\"bad\", \"good\"]\n",
    "TRAIN_FILE = \"train.tsv\"\n",
    "TEST_FILE = \"test.tsv\"\n",
    "TASK_TYPE = \"text_classification\"\n",
    "\n",
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "\n",
    "processor = TextClassificationProcessor(\n",
    "    tokenizer=tokenizer,  # tokenizer \n",
    "    train_filename=TRAIN_FILE,  # training data 파일명\n",
    "    dev_filename=None,  # development data 파일명, 없으면, dev_split 비율만큼 training data에서 자른다 \n",
    "    test_filename=TEST_FILE,  # test data 파일명\n",
    "    dev_split=0.1,  # development data로 설정할 비율\n",
    "    header=0,  # csv, tsv, excel 등 tabular형태 데이터에서 첫행(보통은 컬럼명)의 위치\n",
    "    max_seq_len=MAX_LENGTH,  # 문장의 최대 길이\n",
    "    data_dir=str(DATA_PATH),  # 데이터의 디렉토리\n",
    "    label_list=LABEL_LIST,  # 레이블 리스트(string 필요)\n",
    "    metric=\"acc\",  # 평가지표\n",
    "    label_column_name=\"label\",  # tabular형태 데이터에서 레이블의 컬럼명\n",
    "    text_column_name=\"document\",  # tabular형태 데이터에서 텍스트의 컬럼명\n",
    "    delimiter=\"\\t\"\n",
    ")\n",
    "\n",
    "\n",
    "data_silo = DataSilo(\n",
    "    processor=processor,\n",
    "    batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBqy3S68OIX9"
   },
   "source": [
    "데이터는 다음과 같이 tokenization 되며, sample 객체에 저장된다. \n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1DVPT_Rjv_SI4ggJZzqfPh0MgsMa1Q9El\" alt=\"Fine-tuning\" width=\"100%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "```plaintext\n",
    "03/28/2021 22:12:15 - INFO - farm.data_handler.processor -   \n",
    "\n",
    "      .--.        _____                       _      \n",
    "    .'_\\/_'.     / ____|                     | |     \n",
    "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
    "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
    "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
    "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
    "   (/\\||/                             |_|           \n",
    "______\\||/___________________________________________                     \n",
    "\n",
    "ID: 437-0\n",
    "Clear Text: \n",
    " \ttext_classification_label: good\n",
    " \ttext: 이 영화를 보고 두통이 나았습니다. ㅠ ㅠ\n",
    "Tokenized: \n",
    " \ttokens: ['이', '영화를', '보고', '두', '##통이', '나', '##았습니다', '.', '[UNK]', '[UNK]']\n",
    " \toffsets: [0, 2, 6, 9, 10, 13, 14, 18, 20, 22]\n",
    " \tstart_of_word: [True, True, True, True, False, True, False, False, True, True]\n",
    "Features: \n",
    " \tinput_ids: [2, 2451, 25833, 8198, 917, 11765, 587, 21809, 17, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \ttext_classification_label_ids: [1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ_tuLHRTgGb"
   },
   "source": [
    "## Modeling Layers: AdaptiveModel = LanguageModel + PredictionHead\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1OLWdr8rh7ucpF9t55gzVeMawMBJbRiEC\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **LanguageModel**은 pretrained language models(BERT, XLNet ...)의 표준 클래스 \n",
    "* **PredictionHead**는 모든 down-stream tasks(NER, Text classification, QA ...)를 표준 클래스\n",
    "* **AdaptiveModel**은 위 두 가지 모들의 결합, 하나의 LanguageModel과 여러 개의 PredictionHead를 결합할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60M4IEtWTitY",
    "outputId": "f56fa874-aff8-45f2-de57-a1605444b9bf"
   },
   "outputs": [],
   "source": [
    "# LanguageModel: Build pretrained language model\n",
    "EMBEDS_DROPOUT_PROB = 0.1\n",
    "TASK_NAME = \"text_classification\"\n",
    "\n",
    "language_model = LanguageModel.load(PRETRAINED_MODEL_NAME_OR_PATH, language=\"korean\")\n",
    "# PredictionHead: Build predictor layer\n",
    "prediction_head = TextClassificationHead(\n",
    "    num_labels=len(LABEL_LIST), \n",
    "    class_weights=data_silo.calculate_class_weights(\n",
    "        task_name=TASK_NAME\n",
    "    )\n",
    ")\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[prediction_head],\n",
    "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
    "    lm_output_types=[\"per_sequence\"],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhVOJTGugWlT",
    "outputId": "ead457e2-251a-4768-c9d4-a37d719acdaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <class 'farm.modeling.adaptive_model.AdaptiveModel'>\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: embeddings\n",
      "--------------------------------------------------------\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(300, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: encoder\n",
      "--------------------------------------------------------\n",
      "Showing last layer\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: feed_forward\n",
      "--------------------------------------------------------\n",
      "FeedForwardBlock(\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: loss_fct\n",
      "--------------------------------------------------------\n",
      "CrossEntropyLoss()\n",
      "Last Dropout Layer\n",
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {type(model)}\")\n",
    "for k, v in model.named_children():\n",
    "    for k1, v1 in v.named_children():\n",
    "        \n",
    "        for k2, v2 in v1.named_children():\n",
    "            print(\"----------------------------\"*2)\n",
    "            print(f\"Module: {k} | Layer: {k2}\")\n",
    "            print(\"----------------------------\"*2)\n",
    "            if k2 == \"encoder\":\n",
    "                print(\"Showing last layer\")\n",
    "                print(list(v2.children())[0][-1])\n",
    "                break\n",
    "            else:\n",
    "                print(v2)\n",
    "\n",
    "print(\"Last Dropout Layer\")\n",
    "print(model.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 transformers 패키지와 비교해보면 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmMLVGnpTqxB",
    "outputId": "e1f6a8c7-3010-4cbf-e8f9-5f3c960f25a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Module: classifer\n",
      "--------------------------------------------------------\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "--------------------------------------------------------\n",
      "Module: dropout\n",
      "--------------------------------------------------------\n",
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "bert = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: classifer\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.classifier)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: dropout\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbvskPXaTlGp"
   },
   "source": [
    "## Train & Eval & Inference\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1bD54igqAn7T96gDCFZ2uxzFHpZIL5GOh\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4JEzzR4nZej"
   },
   "source": [
    "### Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "id": "IuqGruw3mzm2",
    "outputId": "2762b7d5-54c1-49b0-cb69-39b531b52cfa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 10:52:16 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 2e-05}'\n",
      "03/29/2021 10:52:16 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "03/29/2021 10:52:16 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 1674.7, 'num_training_steps': 16747}'\n",
      "03/29/2021 10:52:16 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.6181):   0%|          | 28/16747 [00:14<2:15:31,  2.06it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-aa4abf55ebca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# now train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mper_sample_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_to_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_sample_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;31m# Perform  evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/train.py\u001b[0m in \u001b[0;36mbackward_propagate\u001b[0;34m(self, loss, step)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_acc_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "N_EPOCHS = 1\n",
    "N_GPU = 1\n",
    "checkpoint_path = \"./ckpt/NSMC\"\n",
    "\n",
    "# Initialize Optimizer\n",
    "model, optimizer, lr_schedule = initialize_optimizer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_batches=len(data_silo.loaders[\"train\"]),\n",
    "    n_epochs=N_EPOCHS\n",
    ")\n",
    "# EarlyStopping\n",
    "earlymetric = \"f1\" if args.task_name == \"question_answering\" else \"acc\" \n",
    "mode = \"max\" if args.task_name in [\"text_classification\", \"question_answering\"] else \"min\"\n",
    "earlystop = EarlyStopping(\n",
    "    save_dir=checkpoint_path,\n",
    "    metric=earlymetric,\n",
    "    mode=mode,\n",
    "    patience=3,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_schedule=lr_schedule,\n",
    "    data_silo=data_silo,\n",
    "    early_stopping=earlystop,\n",
    "    evaluate_every=20,\n",
    "    checkpoints_to_keep=3,\n",
    "    checkpoint_root_dir=checkpoint_path,\n",
    "    checkpoint_every=200,\n",
    "    epochs=N_EPOCHS,\n",
    "    n_gpu=N_GPU,\n",
    "    device=device, \n",
    ")\n",
    "# now train!\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 과정에 계속 Log가 찍히고, Processor단계에서 입력해둔 `test_filename`로 평가도 해준다. 다음 그림은 430개의 배치 데이터(batch_size=256)를 돌렸을 때 earlystopping한 결과다.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1m1K9CjBNulC4dzSxC1vKjLb94p9BQu26\" alt=\"Fine-tuning\" width=\"80%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vr574Y5nWEu"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mhWTc4FstDU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/ColabNotebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nyAQNWsnlUK",
    "outputId": "22a8b301-fe24-4877-e7e1-32bac5e9ec57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2021 11:39:09 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "03/29/2021 11:39:11 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.9966925978660583, 1.0033293962478638]\n",
      "03/29/2021 11:39:11 - INFO - farm.modeling.prediction_head -   Loading prediction head from ckpt/NSMC/prediction_head_0.bin\n",
      "03/29/2021 11:39:12 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "03/29/2021 11:39:12 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   Got ya 1 parallel workers to do inference ...\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -    0 \n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   /w\\\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   /'\\\n",
      "03/29/2021 11:39:12 - INFO - farm.infer -   \n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1-0\n",
      "Clear Text: \n",
      " \ttext: 황정민 나오는 영화는 다 볼만한듯.\n",
      "Tokenized: \n",
      " \ttokens: ['황', '##정', '##민', '나오는', '영화', '##는', '다', '볼만', '##한듯', '.']\n",
      " \toffsets: [0, 1, 2, 4, 8, 10, 12, 14, 16, 18]\n",
      " \tstart_of_word: [True, False, False, True, True, False, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 3461, 4036, 4141, 9592, 9376, 4008, 786, 20421, 15510, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "03/29/2021 11:39:12 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: 기생충,,, 이 영화 정말 재밌네요.\n",
      "Tokenized: \n",
      " \ttokens: ['기생충', ',', ',', ',', '이', '영화', '정말', '재밌', '##네요', '.']\n",
      " \toffsets: [0, 3, 4, 5, 7, 9, 12, 15, 17, 19]\n",
      " \tstart_of_word: [True, False, False, False, True, True, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 9409, 15, 15, 15, 2451, 9376, 8050, 13365, 8025, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.30s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from farm.infer import Inferencer\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "basic_texts = [\n",
    "    {\"text\": \"기생충... 이 영화 정말 올해의 대작임.\"},\n",
    "    {\"text\": \"황정민 나오는 영화는 다 볼만한듯.\"},\n",
    "]\n",
    "\n",
    "infer_model = Inferencer.load(\n",
    "    model_name_or_path=\"./ckpt/best_nsmc\",\n",
    "    task_type=\"text_classification\"\n",
    ")\n",
    "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-mk4CwptABQ",
    "outputId": "5d33ee9d-cdd1-4da8-c638-5a398e875728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'predictions': [{'context': '기생충,,, 이 영화 정말 재밌네요.',\n",
      "                   'end': None,\n",
      "                   'label': 'good',\n",
      "                   'probability': 0.83329886,\n",
      "                   'start': None},\n",
      "                  {'context': '황정민 나오는 영화는 다 볼만한듯.',\n",
      "                   'end': None,\n",
      "                   'label': 'good',\n",
      "                   'probability': 0.7448745,\n",
      "                   'start': None}],\n",
      "  'task': 'text_classification'}]\n"
     ]
    }
   ],
   "source": [
    "PrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WlcEEWGyVEZ"
   },
   "source": [
    "# MLflow\n",
    "\n",
    "MLflow를 이용하 빠르고 쉽게 실험을 관리하고, 관련 평가지표도 함께 볼 수 있다. public mlflow([링크](https://public-mlflow.deepset.ai/#/experiments/313/runs/05e7e3d4945642f9ab3e296637d57c26))에서 확인하기\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=13Cg8eziHBgA3JLwZJ3Bo8YzeySWPRmiP\" alt=\"Fine-tuning\" width=\"30%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Train과 Dev 세트의 loss는 다음과 같다.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1cpFWVvjkSqshvN0hS_CuPk4RjyEyM0AV\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Dev 세트의 정확도는 다음과 같다.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1VPso9Gx60V8_dgE4as054n7kymCoQ9w5\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA3QoKnvTz17"
   },
   "source": [
    "# TASK Supported\n",
    "\n",
    "|Task|BERT|RoBERTa*|XLNet|ALBERT|DistilBERT|XLMRoBERTa|ELECTRA|MiniLM|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|Text classification|x|x|x|x|x|x|x|x|\n",
    "|NER|x|x|x|x|x|x|x|x|\n",
    "|Question Answering|x|x|x|x|x|x|x|x|\n",
    "|Language Model Fine-tuning|x||||||||\n",
    "|Text Regression|x|x|x|x|x|x|x|x|\n",
    "|Multilabel Text classif.|x|x|x|x|x|x|x|x|\n",
    "|Extracting embeddings|x|x|x|x|x|x|x|x|\n",
    "|LM from scratch|x||||||||\n",
    "|Text Pair Classification|x|x|x|x|x|x|x|x|\n",
    "|Passage Ranking|x|x|x|x|x|x|x|x|\n",
    "|Document retrieval (DPR)|x|x||x|x|x|x|x|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhTAYSY1t4Sr"
   },
   "source": [
    "# Compare to others\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1TZoRpza8-o4wSTr0s16f8hHQRroLQg30\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "다른 모델과 비교해보면 FARM은 조금 더 huggingface와 pytorch-lightning의 합본 축약 버전이라고 생각할 수 있다. 마치 Tensorflow v1과 keras의 차이 느낌이다.\n",
    "\n",
    "## FARM 장단점\n",
    "\n",
    "장점:\n",
    "\n",
    "* 데이터 세트만 준비되어 있으면, 다른 패키지에 비해 상대적으로 설정 할 것이 적음\n",
    "* 훈련 속도가 빠르고, 실험 기록 및 관리이 편리해서 빠르게 실험해 볼 수 있다.\n",
    "* 텐서보드 대신 mlflow 사용 가능\n",
    "\n",
    "단점: \n",
    "\n",
    "* customization이 상대적으로 힘듦\n",
    "* 아직 발전 중이라 불안정하고 documentaton이 잘 안되어 있다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FARM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
