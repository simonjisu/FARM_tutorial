{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4dmCNpOc64i"
   },
   "source": [
    "# For Colab User \n",
    "\n",
    "* set `runtime=GPU` then start to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWSR3a5vxSGL",
    "outputId": "e77e8eb2-2cdf-4ab0-cbe1-645dc90f8ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.6.0+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
      "Requirement already satisfied: farm==0.5.0 in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.3.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (2.23.0)\n",
      "Requirement already satisfied: torch<1.7,>1.5 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.6.0+cu101)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (54.1.2)\n",
      "Requirement already satisfied: flask-cors in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (3.0.10)\n",
      "Requirement already satisfied: flask-restplus in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.13.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.17.40)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.36.2)\n",
      "Requirement already satisfied: dotmap==1.3.0 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.3.0)\n",
      "Requirement already satisfied: Werkzeug==0.16.1 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.16.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.4.1)\n",
      "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (0.0.12)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (4.41.1)\n",
      "Requirement already satisfied: transformers==3.3.1 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (3.3.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (5.4.8)\n",
      "Requirement already satisfied: mlflow==1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm==0.5.0) (1.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.5.0) (1.24.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<1.7,>1.5->farm==0.5.0) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<1.7,>1.5->farm==0.5.0) (1.19.5)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (7.1.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.5.0) (2.11.3)\n",
      "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors->farm==0.5.0) (1.15.0)\n",
      "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (9.0.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (2018.9)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.5.0) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->farm==0.5.0) (0.22.2.post1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (0.3.6)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.40 in /usr/local/lib/python3.7/dist-packages (from boto3->farm==0.5.0) (1.20.40)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->farm==0.5.0) (2.4.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.0.43)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.8.1rc2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (2019.12.20)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (0.1.95)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->farm==0.5.0) (20.9)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.12.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.1.5)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.1.14)\n",
      "Requirement already satisfied: databricks-cli>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.14.3)\n",
      "Requirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.4.1)\n",
      "Requirement already satisfied: gunicorn in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (20.1.0)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.3.23)\n",
      "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.2.4)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.5.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.13)\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (3.17.2)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (1.3.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (0.3)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (2.8.1)\n",
      "Requirement already satisfied: docker>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.5.0) (4.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->farm==0.5.0) (1.1.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->farm==0.5.0) (1.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->farm==0.5.0) (2.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1->farm==0.5.0) (2.4.7)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (4.0.7)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.5.0) (0.8.9)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (1.1.4)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.5.0) (1.0.4)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from docker>=3.6.0->mlflow==1.0.0->farm==0.5.0) (0.58.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0) (4.0.0)\n",
      "Cloning into 'nsmc'...\n",
      "remote: Enumerating objects: 14763, done.\u001b[K\n",
      "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
      "Receiving objects: 100% (14763/14763), 56.19 MiB | 21.48 MiB/s, done.\n",
      "Resolving deltas: 100% (1749/1749), done.\n",
      "Checking out files: 100% (14737/14737), done.\n"
     ]
    }
   ],
   "source": [
    "# For Colab: Install FARM\n",
    "!pip install torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install farm==0.5.0\n",
    "!pip install -U -q emoji soynlp\n",
    "!git clone https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxcBddyhc64j"
   },
   "source": [
    "전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iM4H-pbDL_XC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "def read_data(path:str, header=None):\n",
    "    return pd.read_csv(path, sep='\\t', header=header)\n",
    "\n",
    "def clean(x):\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "    url_pattern = re.compile(\n",
    "        r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    \n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x\n",
    "\n",
    "def preprocess_dataframe(df:pd.DataFrame):\n",
    "    r\"\"\"\n",
    "    Changed the code\n",
    "    source from: https://colab.research.google.com/drive/1IPkZo1Wd-DghIOK6gJpcb0Dv4_Gv2kXB\n",
    "    \"\"\"\n",
    "\n",
    "    label_dict = {0:\"bad\", 1:\"good\"}\n",
    "    df['document'] = df['document'].apply(lambda x: clean(str(x)))\n",
    "    df['label'] = df['label'].apply(label_dict.get)\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_dataframe(read_data(\"./nsmc/ratings_train.txt\", header=0))\n",
    "df_test = preprocess_dataframe(read_data(\"./nsmc/ratings_test.txt\", header=0))\n",
    "df_train.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/train.tsv\", sep=\"\\t\", index=False)\n",
    "df_test.loc[:, [\"label\", \"document\"]].to_csv(\"./nsmc/test.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-PcIhBaB3-E",
    "outputId": "d85d0f31-16e2-41c2-e783-214bc035b744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\t\t  ratings_train.txt  raw\tsynopses.json  train.tsv\n",
      "ratings_test.txt  ratings.txt\t     README.md\ttest.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeTtCfRkyxVJ",
    "outputId": "6d8a2731-f944-420f-cd3b-277508cfdfb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:29:20 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __          __  _                            _        \n",
      " \\ \\        / / | |                          | |       \n",
      "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
      "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
      "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
      "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
      "  ______      _____  __  __  \n",
      " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
      " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
      " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
      " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
      " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
      "                                     |    |==|==|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      " \n",
      "Devices available: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import TextClassificationHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.train import Trainer, EarlyStopping\n",
    "from farm.utils import MLFlowLogger\n",
    "\n",
    "repo_path = Path() # Path().absolute().parent\n",
    "sys.path.append(str(repo_path))\n",
    "\n",
    "# Change to your experiment name and run name\n",
    "EXP_NAME = \"FARM_tutorial\"\n",
    "RUN_NAME = \"NSMC_colab\"\n",
    "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
    "ml_logger.init_experiment(experiment_name=EXP_NAME, run_name=RUN_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Devices available: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS3WaV8YMJmh"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1hbtUClFoXg45IbViZoFRLnnDGVlr9Dlb\" alt=\"Fine-tuning\" width=\"30%\" height=\"30%\"></center>\n",
    "\n",
    "# FARM\n",
    "\n",
    "> Framework for Adapting Representation Models\n",
    "\n",
    "이 패키지를 한 마디로 요약하면 Fine-tuning에 최적화된 도구다.\n",
    "\n",
    "최근의 자연어처리 분야는 Transformer와 그 변형의 등장으로 인해, 보통 2단계로 나눠서 학습이 진행된다. \n",
    "1. **Pretrained Language Modeling**\n",
    "\n",
    "   대량의 텍스트 데이터를 이용해 비지도학습(unsupervised learning)으로 언어 모델링은 진행한다. 언어 모델링이란 인간의 언어를 컴퓨터로 모델링하는 과정이다. 쉽게 말하면, 모델에게 단어들을 입력했을 때, 제일 말이 되는 단어(토큰)을 뱉어내게 하는 것이다. 과거에는 단어(토큰)의 순서가 중요했었다. 즉, 일정 단어들의 시퀀스 $x_{1:t-1}$가 주어지면, $t$번째 단어인 $x_t$를 잘 학습시키는 것이었다. 이를 Auto Regressive Modeling이라고도 한다. 그러나, Masked Language Modeling 방법이 등장했는데, 이는 랜덤으로 맞춰야할 단어를 가린 다음에 가려진 단어 $x_{mask}$가 포함된 시퀀스 $x_{1:t}$ 를 모델에게 입력하여 맞추는 학습 방법이다. 이러한 방법이 좋은 성과를 거두면서, 최근에는 모든 언어모델링 기법들이 MLM을 기반으로 하고 있다.\n",
    "2. **Fine-tuning**\n",
    "\n",
    "    PLM(Pretrained Language Model)을 만들고 나면, 각기 다른 downstream task에 따라서 fine-tuning을 하게 된다. Downstream task은 구체적으로 풀고 싶은 문제를 말하며, 주로 다음과 같은 문제들이다.\n",
    "    * 텍스트 분류 Text Classification - 예시: 영화 댓글 긍정/부정 분류하기\n",
    "    * 개체명인식 NER(Named Entity Recognition) - 예시: 특정 기관명, 인명 및 시간 날짜 등 토큰에 알맞는 태그로 분류하기\n",
    "    * 질의응답 Question and Answering - 예시: 특정 지문과 질의(query)가 주어지면 대답하기\n",
    "\n",
    "오늘 소개할 FARM 패키지는 2번째 단계인 Fine-tuning을 보다 손쉽게 만들어놓은 패키지다. Github에서 Colab tutorial과 함께 보면 좋다.\n",
    "\n",
    "- **코드 없는 버전 - Blog:** [링크](https://simonjisu.github.io/nlp/2021/03/30/farm.html)\n",
    "- **Tutorial github:** [링크](https://github.com/simonjisu/FARM_tutorial)\n",
    "- **Colab Tutorial:** [링크](https://colab.research.google.com/github/simonjisu/FARM_tutorial/blob/main/notebooks/FARM_colab.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PkdHkjZoR4A"
   },
   "source": [
    "## Core Features\n",
    "\n",
    "- **Easy fine-tuning of language models** to your task and domain language\n",
    "- **Speed**: AMP(Automatic Mixed Precision) optimizers (~35% faster) and parallel preprocessing (16 CPU cores => ~16x faster)\n",
    "- **Modular design** of language models and prediction heads\n",
    "- Switch between heads or combine them for **multitask learning**\n",
    "- **Full Compatibility** with HuggingFace Transformers' models and model hub\n",
    "- **Smooth upgrading** to newer language models\n",
    "- Integration of **custom datasets** via Processor class\n",
    "- Powerful **experiment tracking** & execution\n",
    "- **Checkpointing & Caching** to resume training and reduce costs with spot instances\n",
    "- Simple **deployment** and **visualization** to showcase your model\n",
    "\n",
    "<details>\n",
    "<summary> 👉 what is AMP? </summary>\n",
    "\n",
    "**Reference**\n",
    "- https://github.com/NVIDIA/apex\n",
    "- https://forums.fast.ai/t/mixed-precision-training/20720\n",
    "\n",
    "**mixed precision training이란**\n",
    "- 처리 속도를 높이기 위한 FP16(16bit floating point)연산과 정확도 유지를 위한 FP32 연산을 섞어 학습하는 방법\n",
    "- Tensor Core를 활용한 FP16연산을 이용하면 FP32연산 대비 절반의 메모리 사용량과 8배의 연산 처리량 & 2배의 메모리 처리량 효과가 있다\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enUPk-Nac64m"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjzUlWjDobxD"
   },
   "source": [
    "# NSMC 데이터로 FARM 알아보기\n",
    "\n",
    "## NSMC 데이터\n",
    "\n",
    "NSMC(Naver Sentiment Movie Corpus)는 한국어로 된 영화 댓글 데이터 세트다. 해당 Task는 타겟 값이 긍정(1)/부정(0)이 되는 Binary Text Classification 문제로 볼 수 있다. https://github.com/e9t/nsmc 에서 받을 수 있다(아래 그림은 label을 bad와 good으로 처리해놓은 상태).\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1FIGIBtZxtuKD5Prps5vPOPldBb0xHwzH\" alt=\"Fine-tuning\" width=\"50%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP4UcaeKoVM8"
   },
   "source": [
    "## Fine-tuning Process\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1j9pn8Lpg7sy6S8Ubvq3E7JLWf28KvRt4\" alt=\"Fine-tuning\" width=\"50%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Fine-tuning Process는 위 그림과 같이 진행된다.\n",
    "\n",
    "* Load Data: 데이터를 알맞는 형식(json, csv 등)으로 불러온다.\n",
    "* Create Dataset: 데이터세트(Dataset) 만들기\n",
    "    * Tokenization: 텍스트를 토큰으로 나누고, 단어장(vocab)을 생성한다.\n",
    "    * ToTensor: vocab에 해당하는 단어를 수치화하는 과정 (`input_ids` in transformers)\n",
    "    * Attention Mask: 패딩계산을 피하기 위해 Attention 해야할 토큰만 masking(`attention_mask` in transformers)\n",
    "* Create Dataloader: 훈련, 평가시 배치크기 단위로 데이터를 불러오는 객체\n",
    "* Create Model:\n",
    "    * Pretrained Language Model: 대량의 텍스트 데이터로 사전에 훈련된 모델 \n",
    "$$\\underset{\\theta}{\\arg \\max} P(x_{mask} \\vert x_{1:t})$$\n",
    "    * Fine-tuninig Layer: Downstream Task에 맞춰서 학습한다.       \n",
    "$$\\underset{\\theta}{\\arg \\max}P(y\\vert x_{1:t})$$\n",
    "        예를 들어, 영화 긍정/부정 분류 문제의 경우\n",
    "$$\\underset{\\theta}{\\arg \\max} P(y=\\text{긍정/부정} \\vert x_{1:t})$$\n",
    "* Train Model: 모델 훈련\n",
    "* Eval Model: 모델 평가\n",
    "* Inference: 모델 서비스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsQKHSPlMovO"
   },
   "source": [
    "## Processor & Data Silo\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1XCc0AJpPBMFcC81NW0A6w0mpswZ2KU7h\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **Processor**는 file 혹은 request를 PyTorch Datset로 만들어 주는 역할이다. 자세한 인자값은 다음 코드 블록에서 설명한다.\n",
    "* **Data Silo**는 train, dev, test sets를 관리하고, Processor의 function들 이용해 각 set를 DataLoader로 변환한다.\n",
    "* **Processor**는 각 데이터를 처리할 때, **Samples**, **SampleBasket**에 담게 되는데, 이들은 raw document를 관리하는 객체이며 tokenized, features등 데이터와 각 샘플을 관리하는 id를 저장하고 있다. 이렇게 하는 이유는 하나의 소스 텍스트(raw text)에서 여러개의 샘플을 생성할 수도 있기 때문이다.\n",
    "\n",
    "    여담이지만 huggingface의 SquadProcessor는 512개 토큰이 넘어가면, 뒤에서 부터 512토큰을 세서 하나의 데이터를 두 개의 샘플로 만든다. \n",
    "    ```python\n",
    "    def dataset_from_dicts(self, ...)\n",
    "        # ...\n",
    "        for dictionary, input_ids, segment_ids, padding_mask, tokens in zip(\n",
    "                dicts, input_ids_batch, segment_ids_batch, padding_masks_batch, tokens_batch\n",
    "        ):\n",
    "            # ...\n",
    "            # Add Basket to self.baskets\n",
    "            curr_sample = Sample(\n",
    "                id=None,\n",
    "                clear_text=dictionary,\n",
    "                tokenized=tokenized,\n",
    "                features=[feat_dict]\n",
    "            )\n",
    "            curr_basket = SampleBasket(\n",
    "                id_internal=None,\n",
    "                raw=dictionary,\n",
    "                id_external=None,\n",
    "                samples=[curr_sample]\n",
    "               )\n",
    "            self.baskets.append(curr_basket)\n",
    "\n",
    "        # ...\n",
    "    ```\n",
    "\n",
    "사용하는 방법은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuQEtIvNTI5V",
    "outputId": "814ce9ea-87cb-446e-96a9-c397b68e8d41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:29:29 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/30/2021 15:29:30 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "03/30/2021 15:29:30 - INFO - farm.data_handler.data_silo -   Loading train set from: nsmc/train.tsv \n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 149539 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -   /w\\\n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -   /'\\\n",
      "03/30/2021 15:29:31 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/train.tsv:   0%|          | 0/149539 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/30/2021 15:29:32 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/30/2021 15:29:32 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1580-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: 컨저링, 데모닉의 흥행을 업고 한번 해보려 했던 것 같으나, 전 시리즈와는 비교도 할 수 없는 OOO기\n",
      "Tokenized: \n",
      " \ttokens: ['컨', '##저', '##링', ',', '데모', '##닉', '##의', '흥', '##행을', '업고', '한번', '해보', '##려', '했던', '것', '같', '##으나', ',', '전', '시', '##리', '##즈', '##와는', '비교', '##도', '할', '수', '없는', 'OOO', '##기']\n",
      " \toffsets: [0, 1, 2, 3, 5, 7, 8, 10, 11, 14, 17, 20, 22, 24, 27, 29, 30, 32, 34, 36, 37, 38, 39, 42, 44, 46, 48, 50, 53, 56]\n",
      " \tstart_of_word: [True, False, False, False, True, False, False, True, False, True, True, True, False, True, True, True, False, False, True, True, False, False, False, True, False, True, True, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 3012, 4488, 5190, 15, 11047, 5623, 4042, 3524, 12857, 25563, 8295, 11660, 4135, 10129, 258, 217, 10410, 15, 2525, 2002, 4038, 4146, 15542, 8898, 4029, 3358, 1931, 8106, 8128, 4184, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/30/2021 15:29:32 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 448-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: 평점이 너무 높은거 같아 낮추러 왔씁니다~ 4.35나 되다니\n",
      "Tokenized: \n",
      " \ttokens: ['평', '##점이', '너무', '높은', '##거', '같아', '낮추', '##러', '왔', '##씁', '##니다', '~', '4', '.', '35', '##나', '되', '##다니']\n",
      " \toffsets: [0, 1, 4, 7, 9, 11, 14, 16, 18, 19, 20, 22, 24, 25, 26, 28, 30, 31]\n",
      " \tstart_of_word: [True, False, True, True, False, True, True, False, True, False, False, False, True, False, False, False, True, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 3288, 11283, 8069, 10974, 4014, 10891, 21716, 4053, 2327, 5267, 7969, 95, 23, 17, 14603, 4136, 900, 8273, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/train.tsv:   1%|▏         | 2000/149539 [00:01<02:02, 1206.29 Dicts/s]03/30/2021 15:29:33 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:   4%|▍         | 6000/149539 [00:04<01:51, 1285.64 Dicts/s]03/30/2021 15:29:36 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:   5%|▌         | 8000/149539 [00:06<01:48, 1299.31 Dicts/s]03/30/2021 15:29:37 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:29:38 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  15%|█▍        | 22000/149539 [00:16<01:30, 1412.42 Dicts/s]03/30/2021 15:29:49 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  16%|█▌        | 24000/149539 [00:17<01:33, 1348.19 Dicts/s]03/30/2021 15:29:50 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  19%|█▊        | 28000/149539 [00:20<01:28, 1374.91 Dicts/s]03/30/2021 15:29:53 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  20%|██        | 30000/149539 [00:22<01:29, 1338.03 Dicts/s]03/30/2021 15:29:54 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  21%|██▏       | 32000/149539 [00:23<01:30, 1305.97 Dicts/s]03/30/2021 15:29:56 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  23%|██▎       | 34000/149539 [00:25<01:24, 1370.28 Dicts/s]03/30/2021 15:29:57 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  25%|██▌       | 38000/149539 [00:28<01:22, 1352.03 Dicts/s]03/30/2021 15:30:00 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:30:00 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  35%|███▍      | 52000/149539 [00:38<01:13, 1335.52 Dicts/s]03/30/2021 15:30:10 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  37%|███▋      | 56000/149539 [00:41<01:10, 1320.73 Dicts/s]03/30/2021 15:30:13 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  40%|████      | 60000/149539 [00:44<01:06, 1352.65 Dicts/s]03/30/2021 15:30:16 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  41%|████▏     | 62000/149539 [00:46<01:05, 1343.71 Dicts/s]03/30/2021 15:30:18 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  53%|█████▎    | 80000/149539 [00:59<00:50, 1387.15 Dicts/s]03/30/2021 15:30:31 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:30:31 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:30:32 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  56%|█████▌    | 84000/149539 [01:02<00:50, 1303.80 Dicts/s]03/30/2021 15:30:34 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  62%|██████▏   | 92000/149539 [01:08<00:43, 1322.72 Dicts/s]03/30/2021 15:30:41 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  63%|██████▎   | 94000/149539 [01:10<00:42, 1308.10 Dicts/s]03/30/2021 15:30:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:30:42 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  71%|███████   | 106000/149539 [01:19<00:31, 1364.89 Dicts/s]03/30/2021 15:30:51 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  88%|████████▊ | 132000/149539 [01:38<00:13, 1346.16 Dicts/s]03/30/2021 15:31:10 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  92%|█████████▏| 138000/149539 [01:43<00:08, 1286.17 Dicts/s]03/30/2021 15:31:14 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv:  96%|█████████▋| 144000/149539 [01:47<00:03, 1390.82 Dicts/s]03/30/2021 15:31:19 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/train.tsv: 100%|██████████| 149539/149539 [01:51<00:00, 1342.08 Dicts/s]\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   Took 15537 samples out of train set to create dev set (dev split is roughly 0.1)\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   Loading test set from: nsmc/test.tsv\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 50000 dictionaries to pytorch datasets (chunksize = 2000)...\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -    0 \n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   /w\\\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   / \\\n",
      "03/30/2021 15:31:23 - INFO - farm.data_handler.data_silo -   \n",
      "Preprocessing Dataset nsmc/test.tsv:   0%|          | 0/50000 [00:00<?, ? Dicts/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/30/2021 15:31:24 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/30/2021 15:31:24 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1345-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: bad\n",
      " \ttext: 엔도 슈사쿠가 쓴 '왕비 마리 앙투아네트' 책 읽고 이 영화 보니 이건 그냥 프랑스 드레스 입고 하이틴물 찍는 것처럼 보이네요-_-\n",
      "Tokenized: \n",
      " \ttokens: ['엔', '##도', '슈', '##사', '##쿠', '##가', '쓴', \"'\", '왕', '##비', '마', '##리', '앙', '##투', '##아네', '##트', \"'\", '책', '읽고', '이', '영화', '보니', '이건', '그냥', '프랑스', '드', '##레스', '입고', '하이', '##틴', '##물', '찍는', '것처럼', '보이네요', '-', '_', '-']\n",
      " \toffsets: [0, 1, 3, 4, 5, 6, 8, 10, 11, 12, 14, 15, 17, 18, 19, 21, 22, 24, 26, 29, 31, 34, 37, 40, 43, 47, 48, 51, 54, 56, 57, 59, 62, 66, 70, 71, 72]\n",
      " \tstart_of_word: [True, False, True, False, False, False, True, True, False, False, True, False, True, False, False, False, False, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, True, True, False, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 2257, 4029, 1975, 4107, 5047, 4009, 2141, 10, 2328, 4167, 1293, 4038, 2188, 4466, 27889, 4104, 10, 2856, 13985, 2451, 9376, 8844, 8259, 8019, 12818, 947, 10770, 12841, 15830, 4713, 4336, 17841, 12710, 21533, 16, 64, 16, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "03/30/2021 15:31:24 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 123-0\n",
      "Clear Text: \n",
      " \ttext_classification_label: good\n",
      " \ttext: 훌륭한 작가와 뛰어난 연출진이 만나, 훌륭하고 뛰어난 한 편의 드라마가 만들어졌다.\n",
      "Tokenized: \n",
      " \ttokens: ['훌륭한', '작가', '##와', '뛰어난', '연출', '##진이', '만나', ',', '훌륭', '##하고', '뛰어난', '한', '편의', '드라마', '##가', '만들어', '##졌다', '.']\n",
      " \toffsets: [0, 4, 6, 8, 12, 14, 17, 19, 21, 23, 26, 30, 32, 35, 38, 40, 43, 45]\n",
      " \tstart_of_word: [True, True, False, True, True, False, True, False, True, False, True, True, True, True, False, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 13659, 16290, 4196, 25287, 23889, 12194, 10448, 15, 11133, 7968, 25287, 3354, 13165, 11913, 4009, 8251, 10127, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset nsmc/test.tsv:   8%|▊         | 4000/50000 [00:03<00:37, 1230.28 Dicts/s]03/30/2021 15:31:27 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  12%|█▏        | 6000/50000 [00:04<00:34, 1280.04 Dicts/s]03/30/2021 15:31:28 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:31:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:31:29 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  24%|██▍       | 12000/50000 [00:09<00:28, 1319.83 Dicts/s]03/30/2021 15:31:33 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  48%|████▊     | 24000/50000 [00:18<00:19, 1319.35 Dicts/s]03/30/2021 15:31:41 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "03/30/2021 15:31:41 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  52%|█████▏    | 26000/50000 [00:19<00:18, 1319.22 Dicts/s]03/30/2021 15:31:43 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  64%|██████▍   | 32000/50000 [00:24<00:13, 1320.87 Dicts/s]03/30/2021 15:31:48 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  80%|████████  | 40000/50000 [00:29<00:07, 1369.33 Dicts/s]03/30/2021 15:31:53 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv:  92%|█████████▏| 46000/50000 [00:34<00:03, 1315.14 Dicts/s]03/30/2021 15:31:58 - WARNING - farm.data_handler.processor -   The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: \n",
      "Preprocessing Dataset nsmc/test.tsv: 100%|██████████| 50000/50000 [00:37<00:00, 1331.56 Dicts/s]\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Cached the datasets at cache/data_silo/ac8b9ede46e0a9423d4adc9c41a611b6\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Examples in train: 133975\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Examples in dev  : 15537\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Examples in test : 49989\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   \n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     150\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 19.548318716178393\n",
      "03/30/2021 15:32:02 - INFO - farm.data_handler.data_silo -   Proportion clipped:      4.4784474715431986e-05\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = repo_path / \"nsmc\"\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"beomi/kcbert-base\"  # Reference: https://github.com/Beomi/KcBERT\n",
    "MAX_LENGTH = 150\n",
    "LABEL_LIST = [\"bad\", \"good\"]\n",
    "TRAIN_FILE = \"train.tsv\"\n",
    "TEST_FILE = \"test.tsv\"\n",
    "TASK_TYPE = \"text_classification\"\n",
    "\n",
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "\n",
    "processor = TextClassificationProcessor(\n",
    "    tokenizer=tokenizer,  # tokenizer \n",
    "    train_filename=TRAIN_FILE,  # training data 파일명\n",
    "    dev_filename=None,  # development data 파일명, 없으면, dev_split 비율만큼 training data에서 자른다 \n",
    "    test_filename=TEST_FILE,  # test data 파일명\n",
    "    dev_split=0.1,  # development data로 설정할 비율\n",
    "    header=0,  # csv, tsv, excel 등 tabular형태 데이터에서 첫행(보통은 컬럼명)의 위치\n",
    "    max_seq_len=MAX_LENGTH,  # 문장의 최대 길이\n",
    "    data_dir=str(DATA_PATH),  # 데이터의 디렉토리\n",
    "    label_list=LABEL_LIST,  # 레이블 리스트(string 필요)\n",
    "    metric=\"acc\",  # 평가지표\n",
    "    label_column_name=\"label\",  # tabular형태 데이터에서 레이블의 컬럼명\n",
    "    text_column_name=\"document\",  # tabular형태 데이터에서 텍스트의 컬럼명\n",
    "    delimiter=\"\\t\"\n",
    ")\n",
    "\n",
    "\n",
    "data_silo = DataSilo(\n",
    "    processor=processor,\n",
    "    batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBqy3S68OIX9"
   },
   "source": [
    "코드 실행 후, 데이터는 다음과 같이 tokenization 되며, sample 객체에 저장된다. \n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1DVPT_Rjv_SI4ggJZzqfPh0MgsMa1Q9El\" alt=\"Fine-tuning\" width=\"100%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "```plaintext\n",
    "03/28/2021 22:12:15 - INFO - farm.data_handler.processor -   \n",
    "\n",
    "      .--.        _____                       _      \n",
    "    .'_\\/_'.     / ____|                     | |     \n",
    "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
    "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
    "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
    "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
    "   (/\\||/                             |_|           \n",
    "______\\||/___________________________________________                     \n",
    "\n",
    "ID: 437-0\n",
    "Clear Text: \n",
    " \ttext_classification_label: good\n",
    " \ttext: 이 영화를 보고 두통이 나았습니다. ㅠ ㅠ\n",
    "Tokenized: \n",
    " \ttokens: ['이', '영화를', '보고', '두', '##통이', '나', '##았습니다', '.', '[UNK]', '[UNK]']\n",
    " \toffsets: [0, 2, 6, 9, 10, 13, 14, 18, 20, 22]\n",
    " \tstart_of_word: [True, True, True, True, False, True, False, False, True, True]\n",
    "Features: \n",
    " \tinput_ids: [2, 2451, 25833, 8198, 917, 11765, 587, 21809, 17, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \ttext_classification_label_ids: [1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ_tuLHRTgGb"
   },
   "source": [
    "## Modeling Layers: AdaptiveModel = LanguageModel + PredictionHead\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1OLWdr8rh7ucpF9t55gzVeMawMBJbRiEC\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "* **LanguageModel**은 pretrained language models(BERT, XLNet ...)의 표준 클래스 \n",
    "* **PredictionHead**는 모든 down-stream tasks(NER, Text classification, QA ...)를 표준 클래스\n",
    "* **AdaptiveModel**은 위 두 가지 모들의 결합, 하나의 LanguageModel과 여러 개의 PredictionHead를 결합할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60M4IEtWTitY",
    "outputId": "75f046a8-1684-4821-ab5d-52d2b6b89f14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:32:47 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "03/30/2021 15:32:47 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.9974761 1.0025367]\n"
     ]
    }
   ],
   "source": [
    "# LanguageModel: Build pretrained language model\n",
    "EMBEDS_DROPOUT_PROB = 0.1\n",
    "TASK_NAME = \"text_classification\"\n",
    "\n",
    "language_model = LanguageModel.load(PRETRAINED_MODEL_NAME_OR_PATH, language=\"korean\")\n",
    "# PredictionHead: Build predictor layer\n",
    "prediction_head = TextClassificationHead(\n",
    "    num_labels=len(LABEL_LIST), \n",
    "    class_weights=data_silo.calculate_class_weights(\n",
    "        task_name=TASK_NAME\n",
    "    )\n",
    ")\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[prediction_head],\n",
    "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
    "    lm_output_types=[\"per_sequence\"],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeu-iW36mhhF"
   },
   "source": [
    "실제 모델의 구성을 살펴보면 classification을 위한 bert와 유사하게 `PredictionHead`에서는 `pooler`에서 나온 `pooled_output`을 `dropout`층을 통과한 후에 `FeedForwardBlock`으로 보내서 최종 logits을 생성한다. `AdaptiveModel` class에서 `embeds_dropout_prob`를 바꾸면, dropout 확률을 조절할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhVOJTGugWlT",
    "outputId": "8eb7db6a-b858-4c9b-c26f-298b34bbcf61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <class 'farm.modeling.adaptive_model.AdaptiveModel'>\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: embeddings\n",
      "--------------------------------------------------------\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(300, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: language_model | Layer: encoder\n",
      "--------------------------------------------------------\n",
      "Showing last layer\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: feed_forward\n",
      "--------------------------------------------------------\n",
      "FeedForwardBlock(\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------------\n",
      "Module: prediction_heads | Layer: loss_fct\n",
      "--------------------------------------------------------\n",
      "CrossEntropyLoss()\n",
      "Last Dropout Layer\n",
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {type(model)}\")\n",
    "for k, v in model.named_children():\n",
    "    for k1, v1 in v.named_children():\n",
    "        \n",
    "        for k2, v2 in v1.named_children():\n",
    "            print(\"----------------------------\"*2)\n",
    "            print(f\"Module: {k} | Layer: {k2}\")\n",
    "            print(\"----------------------------\"*2)\n",
    "            if k2 == \"encoder\":\n",
    "                print(\"Showing last layer\")\n",
    "                print(list(v2.children())[0][-1])\n",
    "                break\n",
    "            else:\n",
    "                print(v2)\n",
    "\n",
    "print(\"Last Dropout Layer\")\n",
    "print(model.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07u1dn-vc64p"
   },
   "source": [
    "실제로 transformers 패키지와 비교해보면 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmMLVGnpTqxB",
    "outputId": "5b99cb1e-a812-4ca6-a839-4f4315365330"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Module: dropout\n",
      "--------------------------------------------------------\n",
      "Dropout(p=0.1, inplace=False)\n",
      "--------------------------------------------------------\n",
      "Module: classifer\n",
      "--------------------------------------------------------\n",
      "Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "bert = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: dropout\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.dropout)\n",
    "print(\"----------------------------\"*2)\n",
    "print(f\"Module: classifer\")\n",
    "print(\"----------------------------\"*2)\n",
    "print(bert.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbvskPXaTlGp"
   },
   "source": [
    "## Train & Eval & Inference\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1bD54igqAn7T96gDCFZ2uxzFHpZIL5GOh\" alt=\"Fine-tuning\" width=\"60%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4JEzzR4nZej"
   },
   "source": [
    "### Train & Eval\n",
    "\n",
    "> 💡 **TIP:** 실제로 작동하는 지만 확인하고 멈추세요! 아래에서 미리 훈련된 모델을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IuqGruw3mzm2",
    "outputId": "d3c89964-46e0-4344-f379-82c4125ac6d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:33:12 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 2e-05}'\n",
      "03/30/2021 15:33:13 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "03/30/2021 15:33:13 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 1674.7, 'num_training_steps': 16747}'\n",
      "03/30/2021 15:33:14 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.0000):   0%|          | 0/16747 [00:00<?, ?it/s]03/30/2021 15:33:15 - INFO - farm.train -   Saving a train checkpoint ...\n",
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "03/30/2021 15:33:22 - INFO - farm.train -   Saved a training checkpoint after epoch_0_step_0\n",
      "Train epoch 0/0 (Cur. train loss: 0.7284):   0%|          | 20/16747 [00:12<1:08:06,  4.09it/s]\n",
      "Evaluating:   0%|          | 0/1943 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 146/1943 [00:10<02:03, 14.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 292/1943 [00:20<01:53, 14.51it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 436/1943 [00:30<01:44, 14.40it/s]\u001b[A\n",
      "Evaluating:  30%|██▉       | 578/1943 [00:40<01:35, 14.26it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 718/1943 [00:50<01:26, 14.09it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 856/1943 [01:00<01:18, 13.90it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 991/1943 [01:11<01:09, 13.70it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 1124/1943 [01:21<01:00, 13.49it/s]\u001b[A\n",
      "Evaluating:  65%|██████▍   | 1256/1943 [01:31<00:51, 13.39it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 1390/1943 [01:41<00:41, 13.39it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1525/1943 [01:51<00:31, 13.41it/s]\u001b[A\n",
      "Evaluating:  85%|████████▌ | 1661/1943 [02:01<00:20, 13.44it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1797/1943 [02:11<00:10, 13.46it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 1943/1943 [02:22<00:00, 13.65it/s]\n",
      "03/30/2021 15:35:50 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 20 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "03/30/2021 15:35:50 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "03/30/2021 15:35:50 - INFO - farm.eval -   loss: 0.7042724797185066\n",
      "03/30/2021 15:35:50 - INFO - farm.eval -   task_name: text_classification\n",
      "03/30/2021 15:35:51 - INFO - farm.eval -   acc: 0.48574370856664734\n",
      "03/30/2021 15:35:51 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bad     0.4857    0.4411    0.4623      7788\n",
      "        good     0.4858    0.5306    0.5072      7749\n",
      "\n",
      "    accuracy                         0.4857     15537\n",
      "   macro avg     0.4857    0.4859    0.4848     15537\n",
      "weighted avg     0.4857    0.4857    0.4847     15537\n",
      "\n",
      "03/30/2021 15:35:51 - INFO - farm.train -   Saving current best model to ckpt/NSMC, eval=0.48574370856664734\n",
      "Train epoch 0/0 (Cur. train loss: 0.6686):   0%|          | 40/16747 [02:43<1:22:27,  3.38it/s]\n",
      "Evaluating:   0%|          | 0/1943 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c4f2cdf89f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# now train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m                         )\n\u001b[1;32m    317\u001b[0m                         \u001b[0mevalnr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                         \u001b[0mevaluator_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dev\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, model, return_preds_and_labels)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mlosses_per_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_to_loss_per_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_to_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/modeling/adaptive_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;31m# Run forward pass of language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# Run forward pass of (multiple) prediction heads using the output from above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/modeling/adaptive_model.py\u001b[0m in \u001b[0;36mforward_lm\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;31m# Run forward pass of language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextraction_layer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# get output from an earlier layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/modeling/language_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, segment_ids, padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         )\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_hidden_states\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m                 )\n\u001b[1;32m    484\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     \u001b[0mtens_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_overrides.py\u001b[0m in \u001b[0;36mhas_torch_function\u001b[0;34m(relevant_args)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0mimplementations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \"\"\"\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__torch_function__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_overridable_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "N_EPOCHS = 1\n",
    "N_GPU = 1\n",
    "checkpoint_path = Path(\"./ckpt/NSMC\")\n",
    "\n",
    "# Initialize Optimizer\n",
    "model, optimizer, lr_schedule = initialize_optimizer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_batches=len(data_silo.loaders[\"train\"]),\n",
    "    n_epochs=N_EPOCHS\n",
    ")\n",
    "# EarlyStopping\n",
    "earlymetric = \"f1\" if TASK_NAME == \"question_answering\" else \"acc\" \n",
    "mode = \"max\" if TASK_NAME in [\"text_classification\", \"question_answering\"] else \"min\"\n",
    "earlystop = EarlyStopping(\n",
    "    save_dir=checkpoint_path,\n",
    "    metric=earlymetric,\n",
    "    mode=mode,\n",
    "    patience=3,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_schedule=lr_schedule,\n",
    "    data_silo=data_silo,\n",
    "    early_stopping=earlystop,\n",
    "    evaluate_every=20,\n",
    "    checkpoints_to_keep=3,\n",
    "    checkpoint_root_dir=checkpoint_path,\n",
    "    checkpoint_every=200,\n",
    "    epochs=N_EPOCHS,\n",
    "    n_gpu=N_GPU,\n",
    "    device=device, \n",
    ")\n",
    "# now train!\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vaa778Dsc64q"
   },
   "source": [
    "훈련 과정에 계속 Log가 찍히고, Processor단계에서 입력해둔 `test_filename`로 최종평가도 해준다. 다음 그림은 430개의 배치 데이터(batch_size=256)를 돌렸을 때 earlystopping한 결과다.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1m1K9CjBNulC4dzSxC1vKjLb94p9BQu26\" alt=\"Fine-tuning\" width=\"80%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vr574Y5nWEu"
   },
   "source": [
    "### Inference\n",
    "\n",
    "모델을 사용하여 추론 시, 현재는 두 가지 형태로 지원하고 있다. \n",
    "\n",
    "1. **QA (FARM style)**: \n",
    "\n",
    "    ```\n",
    "    [{“questions”: [“What is X?”], “text”: “Some context containing the answer”}]\n",
    "    ```\n",
    "2. **Classification / NER / embeddings**: \n",
    "\n",
    "    ```\n",
    "    [{“text”: “Some input text”}]\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mum_enpquaHH",
    "outputId": "8e3f627a-c843-4795-c3af-4bd125dada62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Na_JDIVkWdzYjjxK2AR5e1rJIego2-p4\n",
      "To: /content/ckpt/best_nsmc.tar\n",
      "\n",
      "\n",
      "0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "8.91MB [00:00, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "25.7MB [00:00, 63.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "36.7MB [00:00, 72.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "50.9MB [00:00, 79.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "66.6MB [00:00, 92.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "82.8MB [00:00, 106MB/s] \u001b[A\u001b[A\n",
      "\n",
      "94.9MB [00:01, 65.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "115MB [00:01, 81.7MB/s] \u001b[A\u001b[A\n",
      "\n",
      "127MB [00:01, 90.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "146MB [00:01, 107MB/s] \u001b[A\u001b[A\n",
      "\n",
      "167MB [00:01, 124MB/s]\u001b[A\u001b[A\n",
      "\n",
      "187MB [00:01, 141MB/s]\u001b[A\u001b[A\n",
      "\n",
      "212MB [00:01, 162MB/s]\u001b[A\u001b[A\n",
      "\n",
      "232MB [00:01, 171MB/s]\u001b[A\u001b[A\n",
      "\n",
      "254MB [00:01, 184MB/s]\u001b[A\u001b[A\n",
      "\n",
      "275MB [00:02, 138MB/s]\u001b[A\u001b[A\n",
      "\n",
      "300MB [00:02, 159MB/s]\u001b[A\u001b[A\n",
      "\n",
      "319MB [00:02, 143MB/s]\u001b[A\u001b[A\n",
      "\n",
      "337MB [00:02, 141MB/s]\u001b[A\u001b[A\n",
      "\n",
      "353MB [00:02, 143MB/s]\u001b[A\u001b[A\n",
      "\n",
      "371MB [00:02, 152MB/s]\u001b[A\u001b[A\n",
      "\n",
      "393MB [00:02, 166MB/s]\u001b[A\u001b[A\n",
      "\n",
      "411MB [00:03, 123MB/s]\u001b[A\u001b[A\n",
      "\n",
      "436MB [00:03, 136MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./best_nsmc/\n",
      "./best_nsmc/processor_config.json\n",
      "./best_nsmc/language_model.bin\n",
      "./best_nsmc/prediction_head_0.bin\n",
      "./best_nsmc/vocab.txt\n",
      "./best_nsmc/language_model_config.json\n",
      "./best_nsmc/tokenizer_config.json\n",
      "./best_nsmc/special_tokens_map.json\n",
      "./best_nsmc/prediction_head_0_config.json\n"
     ]
    }
   ],
   "source": [
    "# if you not trained the model please use it\n",
    "from pathlib import Path\n",
    "import gdown\n",
    "url = \"https://drive.google.com/uc?id=1Na_JDIVkWdzYjjxK2AR5e1rJIego2-p4\"\n",
    "ckpt_path = Path(\".\") / \"ckpt\"\n",
    "if not ckpt_path.exists():\n",
    "    ckpt_path.mkdir()\n",
    "gdown.download(url, str(ckpt_path / \"best_nsmc.tar\"), quiet=False)\n",
    "!tar -xvf ./ckpt/best_nsmc.tar -C ./ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeafZ3uBvdJA"
   },
   "source": [
    "실제 네이버 영화 두 곳에서 각기 다른 평점을 가져와서 테스트 해보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nyAQNWsnlUK",
    "outputId": "6c927c4b-35c9-4e8a-8c08-68feafe65628"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 15:37:25 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/30/2021 15:37:27 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "03/30/2021 15:37:27 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "03/30/2021 15:37:27 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "03/30/2021 15:37:27 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [0.9976914525032043, 1.0023192167282104]\n",
      "03/30/2021 15:37:27 - INFO - farm.modeling.prediction_head -   Loading prediction head from ckpt/best_nsmc/prediction_head_0.bin\n",
      "03/30/2021 15:37:28 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='lm_name' was already logged with value='beomi/kcbert-base' for run ID='a31a58378fda4212ae1286ea00778a7e. Attempted logging new value './ckpt/best_nsmc'.\n",
      "03/30/2021 15:37:28 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "03/30/2021 15:37:28 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "03/30/2021 15:37:28 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "03/30/2021 15:37:28 - INFO - farm.infer -   Got ya 1 parallel workers to do inference ...\n",
      "03/30/2021 15:37:28 - INFO - farm.infer -    0 \n",
      "03/30/2021 15:37:28 - INFO - farm.infer -   /w\\\n",
      "03/30/2021 15:37:28 - INFO - farm.infer -   /'\\\n",
      "03/30/2021 15:37:28 - INFO - farm.infer -   \n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  FutureWarning,\n",
      "03/30/2021 15:37:28 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "03/30/2021 15:37:28 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 3-0\n",
      "Clear Text: \n",
      " \ttext: 절레절레 돈주고 보지마셈ㅋㅋㅋㅋ\n",
      "Tokenized: \n",
      " \ttokens: ['절', '##레', '##절', '##레', '돈주고', '보지', '##마', '##셈', '##ㅋㅋㅋ', '##ㅋ']\n",
      " \toffsets: [0, 1, 2, 3, 5, 9, 11, 12, 13, 16]\n",
      " \tstart_of_word: [True, False, False, False, True, True, False, False, False, False]\n",
      "Features: \n",
      " \tinput_ids: [2, 2529, 4306, 4094, 4306, 13900, 11708, 4168, 4602, 8418, 4157, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "03/30/2021 15:37:28 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: 비에 젖지 않는 고급 장난감 텐트와, 비에 젖다 못해 잠겨버리는 반지하 가구\n",
      "Tokenized: \n",
      " \ttokens: ['비', '##에', '젖', '##지', '않는', '고급', '장난', '##감', '텐트', '##와', ',', '비', '##에', '젖', '##다', '못해', '잠', '##겨', '##버리는', '반', '##지', '##하', '가구']\n",
      " \toffsets: [0, 1, 3, 4, 6, 9, 12, 14, 16, 18, 19, 21, 22, 24, 25, 27, 30, 31, 32, 36, 37, 38, 40]\n",
      " \tstart_of_word: [True, False, True, False, True, True, True, False, True, False, False, True, False, True, False, True, True, False, False, True, False, False, True]\n",
      "Features: \n",
      " \tinput_ids: [2, 1664, 4113, 2540, 4102, 8743, 15794, 8977, 4114, 22279, 4196, 15, 1664, 4113, 2540, 4020, 10305, 2487, 4349, 14622, 1483, 4102, 4159, 22560, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "\n",
      "\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.87s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from farm.infer import Inferencer\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "# https://movie.naver.com/movie/bi/mi/basic.nhn?code=161967\n",
    "# https://movie.naver.com/movie/bi/mi/point.nhn?code=196051\n",
    "\n",
    "basic_texts = [\n",
    "    {\"text\": \"비에 젖지 않는 고급 장난감 텐트와, 비에 젖다 못해 잠겨버리는 반지하 가구\"},  # 161967 / 평점 10\n",
    "    {\"text\": \"\"\"남들이 난해하단거 보고 혼자 이해했다며 심오한척 하고플때나 보면 딱인 영화. 통찰도 시사점도 재미도 의미도 감동도 없는... \n",
    "진정한 킬링타임. 가난한 사람들 다 기생충에 비유한거야? 그렇다면 감독 개똥철학 완전꽝이고...\"\"\"},  # 161967 / 평점 1\n",
    "    {\"text\": \"와 이거 안보면 인생 절반 후회한겁니다 여러분\"},  # 196051 / 평점 10\n",
    "    {\"text\": \"절레절레 돈주고 보지마셈ㅋㅋㅋㅋ\"}  # 196051 / 평점 1\n",
    "]\n",
    "\n",
    "infer_model = Inferencer.load(\n",
    "    model_name_or_path=\"./ckpt/best_nsmc\",\n",
    "    task_type=\"text_classification\"\n",
    ")\n",
    "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4olYJtIrryF",
    "outputId": "1eef1642-4b4c-4358-e3cd-cad6f5081e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[Movie: 161967] Context:\u001b[0m\n",
      "비에 젖지 않는 고급 장난감 텐트와, 비에 젖다 못해 잠겨버리는 반지하 가구\n",
      "Probability 76.74% | Predict: \u001b[1m\u001b[31mbad\u001b[0m | Real Star: \u001b[1m\u001b[34m10\u001b[0m\n",
      "\n",
      "\u001b[1m[Movie: 161967] Context:\u001b[0m\n",
      "남들이 난해하단거 보고 혼자 이해했다며 심오한척 하고플때나 보면 딱인 영화. 통찰도 시사점도 재미도 의미도 감동도 없는... \n",
      "진정한 킬링타임. 가난한 사람들 다 기생충에 비유한거야? 그렇다면 감독 개똥철학 완전꽝이고...\n",
      "Probability 99.58% | Predict: \u001b[1m\u001b[31mbad\u001b[0m | Real Star: \u001b[1m\u001b[34m1\u001b[0m\n",
      "\n",
      "\u001b[1m[Movie: 196051] Context:\u001b[0m\n",
      "와 이거 안보면 인생 절반 후회한겁니다 여러분\n",
      "Probability 77.20% | Predict: \u001b[1m\u001b[32mgood\u001b[0m | Real Star: \u001b[1m\u001b[34m10\u001b[0m\n",
      "\n",
      "\u001b[1m[Movie: 196051] Context:\u001b[0m\n",
      "절레절레 돈주고 보지마셈ㅋㅋㅋㅋ\n",
      "Probability 97.12% | Predict: \u001b[1m\u001b[31mbad\u001b[0m | Real Star: \u001b[1m\u001b[34m1\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import termcolor\n",
    "\n",
    "for p, mid, star in zip(result[0][\"predictions\"], [161967, 161967, 196051, 196051], [10, 1, 10, 1]):\n",
    "    context = p[\"context\"]\n",
    "    label = p[\"label\"]\n",
    "    probability = p[\"probability\"]\n",
    "    star = termcolor.colored(str(star), \"blue\", attrs=[\"bold\"])\n",
    "    if label == \"bad\":\n",
    "        label = termcolor.colored(label, \"red\", attrs=[\"bold\"])\n",
    "    else:\n",
    "        label = termcolor.colored(label, \"green\", attrs=[\"bold\"])\n",
    "    print(termcolor.colored(f\"[Movie: {mid}] Context:\", attrs=[\"bold\"]))\n",
    "    print(context)\n",
    "    print(f\"Probability {probability*100:.2f}% | Predict: {label} | Real Star: {star}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 영화는 봉준호 감독님의 '기생충(id=161967)', 최근 인기 가도를 달리고 있는 '극장판 귀멸의 칼날: 무한열차편(id=196051)'를 선정했다. 하나를 제외하고 잘 맞춘 모습을 보여줬는데, 첫번째 샘플의 경우 사실 영화의 장면을 묘사한 것으로, 그만큼 인상깊었던 장면들을 달면서 평점은 10점으로 달았다. 사람으로써 이 영화을 본 관객이라면 이 평가가 10점에 알맞는 평점(혹은 긍정)이지만, 기계에게는 아직 어려운 점 중에 하나라고 생각한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "667ZCa2Hc64r"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WlcEEWGyVEZ"
   },
   "source": [
    "# MLflow\n",
    "\n",
    "MLflow를 이용하 빠르고 쉽게 실험을 관리하고, 관련 평가지표도 함께 볼 수 있다. 다음 그림들은 TITAN RTX 4대에서 배치크기를 256으로 훈련 시킨 결과다(440 batches 에서 Early Stopping했다.).\n",
    "\n",
    "* public mlflow([링크](https://public-mlflow.deepset.ai/#/experiments/313/runs/05e7e3d4945642f9ab3e296637d57c26))에서 확인하기\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=13Cg8eziHBgA3JLwZJ3Bo8YzeySWPRmiP\" alt=\"Fine-tuning\" width=\"30%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Train과 Dev 세트의 loss는 다음과 같다.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1cpFWVvjkSqshvN0hS_CuPk4RjyEyM0AV\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "Dev 세트의 정확도는 다음과 같다.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1VPso9Gx60V8_dgE4as054n7kymCoQ9w5\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA3QoKnvTz17"
   },
   "source": [
    "# TASK Supported\n",
    "\n",
    "|Task|BERT|RoBERTa*|XLNet|ALBERT|DistilBERT|XLMRoBERTa|ELECTRA|MiniLM|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|Text classification|x|x|x|x|x|x|x|x|\n",
    "|NER|x|x|x|x|x|x|x|x|\n",
    "|Question Answering|x|x|x|x|x|x|x|x|\n",
    "|Language Model Fine-tuning|x||||||||\n",
    "|Text Regression|x|x|x|x|x|x|x|x|\n",
    "|Multilabel Text classif.|x|x|x|x|x|x|x|x|\n",
    "|Extracting embeddings|x|x|x|x|x|x|x|x|\n",
    "|LM from scratch|x||||||||\n",
    "|Text Pair Classification|x|x|x|x|x|x|x|x|\n",
    "|Passage Ranking|x|x|x|x|x|x|x|x|\n",
    "|Document retrieval (DPR)|x|x||x|x|x|x|x|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhTAYSY1t4Sr"
   },
   "source": [
    "# Compare to others\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1TZoRpza8-o4wSTr0s16f8hHQRroLQg30\" alt=\"Fine-tuning\" width=\"90%\" height=\"50%\" align=\"center\"></center>\n",
    "\n",
    "다른 모델과 비교해보면 FARM은 조금 더 huggingface와 pytorch-lightning의 합본 축약 버전이라고 생각할 수 있다. 마치 Tensorflow v1과 keras의 차이 느낌이다.\n",
    "\n",
    "## FARM 장단점\n",
    "\n",
    "장점:\n",
    "\n",
    "* 데이터 세트만 준비되어 있으면, 다른 패키지에 비해 상대적으로 설정 할 것이 적음\n",
    "* 훈련 속도가 빠르고, 실험 기록 및 관리이 편리해서 빠르게 실험해 볼 수 있음(텐서보드 대신 mlflow 사용 가능)\n",
    "* 멀티 GPU 설정을 해줄 필요가 없음\n",
    "\n",
    "단점: \n",
    "\n",
    "* customization이 상대적으로 힘듦\n",
    "* 아직 발전 중이라 불안정하고 documentaton이 잘 안되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FARM_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
